{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HOW TO: Introducing New Task\n",
    "\n",
    "Welcome and thanks for joining our journey! \n",
    "\n",
    "\n",
    "This tutorial will serve as a walkthrough guide to create a custom task on the MAMMAL framework to fine-tune our base model [**biomed.omics.bl.sm.ma-ted-458m**](https://huggingface.co/ibm/biomed.omics.bl.sm.ma-ted-458m). \\\n",
    "As a case study we will use our [protein solubility prediction](https://github.com/BiomedSciAI/biomed-multi-alignment/tree/main?tab=readme-ov-file#protein-solubility-prediction) task. We will break down the main components so you will be able to create / modify them with your own task and data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Installation\n",
    "Firstly, make sure you have mammal package install. Please follow the [installation guide](https://github.com/BiomedSciAI/biomed-multi-alignment/blob/main/README.md#installation) in the main README file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Big Picture\n",
    "In order to implement a downstream task for the MAMMAL framework, one should implement a `MammalTask` ([source](https://github.com/BiomedSciAI/biomed-multi-alignment/blob/e56a03e0e9f69e42f919a96def739b78e50a47e5/mammal/task.py#L15)). A `MammalTask` consist of three main components:\n",
    "1. Data Module - A Lightning data module class where we load and process the data for the task.\n",
    "2. `data_preprocessing()` function. Which responsible for formatting the input prompt.\n",
    "3. `process_model_output()` function. Which takes the raw output of the model and translate it into a human-level answers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Data Module\n",
    "We need to create a custom Lightning data module in order to load and prepare the input data for the model. \\\n",
    "In the datamodule we will need to:\n",
    "1. Load the raw dataset from our source (in this example we use TDC's API) - will be done in `load_datasets()` .\n",
    "2. Process the raw data to match the model's prompt structure - will be done in `data_preprocessing()` which we cover after in the next section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import wget\n",
    "import shutil\n",
    "import pandas as pd\n",
    "\n",
    "from fuse.data.ops.ops_read import OpReadDataframe\n",
    "from fuse.data.datasets.dataset_default import DatasetDefault\n",
    "from fuse.data.pipelines.pipeline_default import PipelineDefault\n",
    "\n",
    "\n",
    "_SOLUBILITY_URL = \"https://zenodo.org/api/records/1162886/files-archive\"\n",
    "\n",
    "\n",
    "def load_datasets(data_path: str) -> dict[str, DatasetDefault]:\n",
    "    \"\"\"\n",
    "    Automatically downloads the data and create dataset iterator for \"train\", \"val\" and \"test\".\n",
    "    paper: https://academic.oup.com/bioinformatics/article/34/15/2605/4938490\n",
    "    Data retrieved from: https://zenodo.org/records/1162886\n",
    "    The benchmark requires classifying protein sequences into binary labels - Soluble or Insoluble (1 or 0).\n",
    "    :param data_path: path to a directory to store the raw data\n",
    "    :return: dictionary that maps fold name \"train\", \"val\" and \"test\" to a dataset iterator\n",
    "    \"\"\"\n",
    "\n",
    "    if not os.path.exists(data_path):\n",
    "        os.makedirs(data_path)\n",
    "\n",
    "    raw_data_path = os.path.join(data_path, \"sameerkhurana10-DSOL_rv0.2-20562ad/data\")\n",
    "    if not os.path.exists(raw_data_path):\n",
    "        wget.download(_SOLUBILITY_URL, data_path)\n",
    "        file_path = os.path.join(data_path, \"1162886.zip\")\n",
    "        shutil.unpack_archive(file_path, extract_dir=data_path)\n",
    "        inner_file_path = os.path.join(\n",
    "            data_path, \"sameerkhurana10\", \"DSOL_rv0.2-v0.3.zip\"\n",
    "        )\n",
    "        shutil.unpack_archive(inner_file_path, extract_dir=data_path)\n",
    "        assert os.path.exists(\n",
    "            raw_data_path\n",
    "        ), f\"Error: download complete but {raw_data_path} doesn't exist\"\n",
    "\n",
    "    # read files\n",
    "    df_dict = {}\n",
    "    for set_name in [\"train\", \"val\", \"test\"]:\n",
    "        input_df = pd.read_csv(\n",
    "            os.path.join(raw_data_path, f\"{set_name}_src\"), names=[\"data.protein\"]\n",
    "        )\n",
    "        labels_df = pd.read_csv(\n",
    "            os.path.join(raw_data_path, f\"{set_name}_tgt\"), names=[\"data.label\"]\n",
    "        )\n",
    "        df_dict[set_name] = (input_df, labels_df)\n",
    "\n",
    "    ds_dict = {}\n",
    "    for set_name in [\"train\", \"val\", \"test\"]:\n",
    "        input_df, labels_df = df_dict[set_name]\n",
    "        size = len(labels_df)\n",
    "        print(f\"{set_name} set size is {size}\")\n",
    "        dynamic_pipeline = PipelineDefault(\n",
    "            \"solubility\",\n",
    "            [\n",
    "                (OpReadDataframe(input_df, key_column=None), dict()),\n",
    "                (OpReadDataframe(labels_df, key_column=None), dict()),\n",
    "            ],\n",
    "        )\n",
    "\n",
    "        ds = DatasetDefault(sample_ids=size, dynamic_pipeline=dynamic_pipeline)\n",
    "        ds.create()\n",
    "        ds_dict[set_name] = ds\n",
    "\n",
    "    return ds_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-1 / unknowntrain set size is 62478\n",
      "val set size is 6942\n",
      "test set size is 1999\n",
      "Visualize sample in a tree-fashion\n",
      "--- data\n",
      "------ protein -> GMILKTNLFGHTYQFKSITDVLAKANEEKSGDRLAGVAAESAEERVAAKVVLSKMTLGDLRNNPVVPYETDEVTRIIQDQVNDRIHDSIKNWTVEELREWILDHKTTDADIKRVARGLTSEIIAAVTKLMSNLDLIYGAKKIRVIAHANTTIGLPGTFSARLQPNHPTDDPDGILASLMEGLTYGIGDAVIGLNPVDDSTDSVVRLLNKFEEFRSKWDVPTQTCVLAHVKTQMEAMRRGAPTGLVFQSIAGSEKGNTAFGFDGATIEEARQLALQSGAATGPNVMYFETGQGSELSSDAHFGVDQVTMEARCYGFAKKFDPFLVNTVVGFIGPEYLYDSKQVIRAGLEDHFMGKLTGISMGCDVCYTNHMKADQNDVENLSVLLTAAGCNFIMGIPHGDDVMLNYQTTGYHETATLRELFGLKPIKEFDQWMEKMGFSENGKLTSRAGDASIFLK\n",
      "------ initial_sample_id -> 0\n",
      "------ label -> 1\n",
      "------ sample_id -> 0\n",
      "Visualize sample as a raw flat dictionary\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'data.initial_sample_id': 0, 'data.sample_id': 0, 'data.protein': 'GMILKTNLFGHTYQFKSITDVLAKANEEKSGDRLAGVAAESAEERVAAKVVLSKMTLGDLRNNPVVPYETDEVTRIIQDQVNDRIHDSIKNWTVEELREWILDHKTTDADIKRVARGLTSEIIAAVTKLMSNLDLIYGAKKIRVIAHANTTIGLPGTFSARLQPNHPTDDPDGILASLMEGLTYGIGDAVIGLNPVDDSTDSVVRLLNKFEEFRSKWDVPTQTCVLAHVKTQMEAMRRGAPTGLVFQSIAGSEKGNTAFGFDGATIEEARQLALQSGAATGPNVMYFETGQGSELSSDAHFGVDQVTMEARCYGFAKKFDPFLVNTVVGFIGPEYLYDSKQVIRAGLEDHFMGKLTGISMGCDVCYTNHMKADQNDVENLSVLLTAAGCNFIMGIPHGDDVMLNYQTTGYHETATLRELFGLKPIKEFDQWMEKMGFSENGKLTSRAGDASIFLK', 'data.label': 1}"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load dataset - train, validation and test splits\n",
    "ds_dict = load_datasets(\"./data\")\n",
    "\n",
    "# Retrieve and visualize a single sample\n",
    "sample_dict = ds_dict[\"train\"][0]\n",
    "\n",
    "print(\"Visualize sample in a tree-fashion\")\n",
    "sample_dict.print_tree(print_values=True)\n",
    "\n",
    "print(\"Visualize sample as a raw flat dictionary\")\n",
    "sample_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can complete our data module class.\n",
    "\n",
    "##### NOTE\n",
    "The `data_preprocessing` callable function will be implemented and explained in the next section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pytorch_lightning as pl\n",
    "from torch.utils.data.dataloader import DataLoader\n",
    "from fuse.data.utils.collates import CollateDefault\n",
    "from fuse.data.tokenizers.modular_tokenizer.op import ModularTokenizerOp\n",
    "\n",
    "\n",
    "class ProteinSolubilityDataModule(pl.LightningDataModule):\n",
    "    def __init__(\n",
    "        self,\n",
    "        *,\n",
    "        data_path: str,\n",
    "        batch_size: int,\n",
    "        tokenizer_op: ModularTokenizerOp,\n",
    "        train_dl_kwargs: dict,\n",
    "        valid_dl_kwargs: dict,\n",
    "        seed: int,\n",
    "        data_preprocessing: callable,\n",
    "        protein_max_seq_length: int,\n",
    "        encoder_input_max_seq_len: int,\n",
    "        labels_max_seq_len: int,\n",
    "    ) -> None:\n",
    "        \"\"\"_summary_\n",
    "        Args:\n",
    "            data_path (str): path to the raw data, if not exist, will download the data to the given path.\n",
    "            batch_size (int): batch size\n",
    "            tokenizer_op (ModularTokenizerOp): tokenizer op\n",
    "            encoder_inputs_max_seq_len: max tokenizer sequence length for the encoder inputs,\n",
    "            labels_max_seq_len: max tokenizer sequence length for the labels,\n",
    "            train_dl_kwargs (dict): train dataloader constructor parameters\n",
    "            valid_dl_kwargs (dict): validation dataloader constructor parameters\n",
    "            seed (int): random seed\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.data_path = data_path\n",
    "        self.tokenizer_op = tokenizer_op\n",
    "        self.protein_max_seq_length = protein_max_seq_length\n",
    "        self.encoder_input_max_seq_len = encoder_input_max_seq_len\n",
    "        self.labels_max_seq_len = labels_max_seq_len\n",
    "        self.batch_size = batch_size\n",
    "        self.train_dl_kwargs = train_dl_kwargs\n",
    "        self.valid_dl_kwargs = valid_dl_kwargs\n",
    "        self.seed = seed\n",
    "        self.data_preprocessing = data_preprocessing\n",
    "\n",
    "        self.pad_token_id = self.tokenizer_op.get_token_id(\"<PAD>\")\n",
    "\n",
    "    def setup(self, stage: str) -> None:\n",
    "        self.ds_dict = load_datasets(self.data_path)\n",
    "\n",
    "        task_pipeline = [\n",
    "            (\n",
    "                # Prepare the input string(s) in modular tokenizer input format\n",
    "                self.data_preprocessing,\n",
    "                dict(\n",
    "                    protein_sequence_key=\"data.protein\",\n",
    "                    solubility_label_key=\"data.label\",\n",
    "                    tokenizer_op=self.tokenizer_op,\n",
    "                    protein_max_seq_length=self.protein_max_seq_length,\n",
    "                    encoder_input_max_seq_len=self.encoder_input_max_seq_len,\n",
    "                    labels_max_seq_len=self.labels_max_seq_len,\n",
    "                ),\n",
    "            ),\n",
    "        ]\n",
    "\n",
    "        for ds in self.ds_dict.values():\n",
    "            ds.dynamic_pipeline.extend(task_pipeline)\n",
    "\n",
    "    def train_dataloader(self) -> DataLoader:\n",
    "        train_loader = DataLoader(\n",
    "            dataset=self.ds_dict[\"train\"],\n",
    "            batch_size=self.batch_size,\n",
    "            collate_fn=CollateDefault(),\n",
    "            shuffle=True,\n",
    "            **self.train_dl_kwargs,\n",
    "        )\n",
    "        return train_loader\n",
    "\n",
    "    def val_dataloader(self) -> DataLoader:\n",
    "        val_loader = DataLoader(\n",
    "            self.ds_dict[\"val\"],\n",
    "            batch_size=self.batch_size,\n",
    "            collate_fn=CollateDefault(),\n",
    "            **self.valid_dl_kwargs,\n",
    "        )\n",
    "\n",
    "        return val_loader\n",
    "\n",
    "    def test_dataloader(self) -> DataLoader:\n",
    "        test_loader = DataLoader(\n",
    "            self.ds_dict[\"test\"],\n",
    "            batch_size=self.batch_size,\n",
    "            collate_fn=CollateDefault(),\n",
    "            **self.valid_dl_kwargs,\n",
    "        )\n",
    "\n",
    "        return test_loader\n",
    "\n",
    "    def predict_dataloader(self) -> DataLoader:\n",
    "        return self.test_dataloader()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. `data_preprocessing()` function\n",
    "\n",
    "\n",
    "This method plays a crucial role in the task's workflow. Here we process the raw data the we load into a prompt that fits the model's pretraining prompt distribution (see paper for more details). \\\n",
    "Besides formatting the prompt as a string, we also tokenize it using our custom modular tokenizer operator.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "from mammal.keys import *  # Import all dictionary static keys\n",
    "\n",
    "\n",
    "def data_preprocessing(\n",
    "    sample_dict: dict,\n",
    "    *,\n",
    "    protein_sequence_key: str,\n",
    "    tokenizer_op: ModularTokenizerOp,\n",
    "    solubility_label_key: int | None = None,\n",
    "    protein_max_seq_length: int = 1250,\n",
    "    encoder_input_max_seq_len: int | None = 1260,\n",
    "    labels_max_seq_len: int | None = 4,\n",
    "    device: str | torch.device = \"cpu\",\n",
    ") -> dict:\n",
    "    \"\"\"\n",
    "    :param sample_dict: a dictionary with raw data\n",
    "    :param protein_sequence_key: sample_dict key which points to protein sequence\n",
    "    :param solubility_label_key: sample_dict key which points to label\n",
    "    :param protein_max_seq_length: max sequence length of a protein. Will be used to truncate the protein\n",
    "    :param encoder_input_max_seq_len: max sequence length of labels. Will be used to truncate/pad the encoder_input.\n",
    "    :param labels_max_seq_len: max sequence length of labels. Will be used to truncate/pad the labels.\n",
    "    :param tokenizer_op: tokenizer op\n",
    "\n",
    "    \"\"\"\n",
    "    protein_sequence = sample_dict[protein_sequence_key]\n",
    "    solubility_label = sample_dict.get(solubility_label_key, None)\n",
    "\n",
    "    sample_dict[ENCODER_INPUTS_STR] = (\n",
    "        f\"<@TOKENIZER-TYPE=AA><MOLECULAR_ENTITY><MOLECULAR_ENTITY_GENERAL_PROTEIN><SOLUBILITY><SENTINEL_ID_0><@TOKENIZER-TYPE=AA@MAX-LEN={protein_max_seq_length}><SEQUENCE_NATURAL_START>{protein_sequence}<SEQUENCE_NATURAL_END><EOS>\"\n",
    "    )\n",
    "    tokenizer_op(\n",
    "        sample_dict=sample_dict,\n",
    "        key_in=ENCODER_INPUTS_STR,\n",
    "        key_out_tokens_ids=ENCODER_INPUTS_TOKENS,\n",
    "        key_out_attention_mask=ENCODER_INPUTS_ATTENTION_MASK,\n",
    "        max_seq_len=encoder_input_max_seq_len,\n",
    "    )\n",
    "    sample_dict[ENCODER_INPUTS_TOKENS] = torch.tensor(\n",
    "        sample_dict[ENCODER_INPUTS_TOKENS], device=device\n",
    "    )\n",
    "    sample_dict[ENCODER_INPUTS_ATTENTION_MASK] = torch.tensor(\n",
    "        sample_dict[ENCODER_INPUTS_ATTENTION_MASK], device=device\n",
    "    )\n",
    "\n",
    "    if solubility_label is not None:\n",
    "        pad_id = tokenizer_op.get_token_id(\"<PAD>\")\n",
    "        ignore_token_value = -100\n",
    "        sample_dict[LABELS_STR] = (\n",
    "            f\"<@TOKENIZER-TYPE=AA><SENTINEL_ID_0><{solubility_label}><EOS>\"\n",
    "        )\n",
    "        tokenizer_op(\n",
    "            sample_dict=sample_dict,\n",
    "            key_in=LABELS_STR,\n",
    "            key_out_tokens_ids=LABELS_TOKENS,\n",
    "            key_out_attention_mask=LABELS_ATTENTION_MASK,\n",
    "            max_seq_len=labels_max_seq_len,\n",
    "        )\n",
    "        sample_dict[LABELS_TOKENS] = torch.tensor(\n",
    "            sample_dict[LABELS_TOKENS], device=device\n",
    "        )\n",
    "        sample_dict[LABELS_ATTENTION_MASK] = torch.tensor(\n",
    "            sample_dict[LABELS_ATTENTION_MASK], device=device\n",
    "        )\n",
    "        # replace pad_id with -100 to\n",
    "        pad_id_tns = torch.tensor(pad_id)\n",
    "        sample_dict[LABELS_TOKENS][\n",
    "            (sample_dict[LABELS_TOKENS][..., None] == pad_id_tns).any(-1).nonzero()\n",
    "        ] = ignore_token_value\n",
    "\n",
    "        sample_dict[DECODER_INPUTS_STR] = (\n",
    "            f\"<@TOKENIZER-TYPE=AA><DECODER_START><SENTINEL_ID_0><{solubility_label}><EOS>\"\n",
    "        )\n",
    "        tokenizer_op(\n",
    "            sample_dict=sample_dict,\n",
    "            key_in=DECODER_INPUTS_STR,\n",
    "            key_out_tokens_ids=DECODER_INPUTS_TOKENS,\n",
    "            key_out_attention_mask=DECODER_INPUTS_ATTENTION_MASK,\n",
    "            max_seq_len=labels_max_seq_len,\n",
    "        )\n",
    "        sample_dict[DECODER_INPUTS_TOKENS] = torch.tensor(\n",
    "            sample_dict[DECODER_INPUTS_TOKENS], device=device\n",
    "        )\n",
    "        sample_dict[DECODER_INPUTS_ATTENTION_MASK] = torch.tensor(\n",
    "            sample_dict[DECODER_INPUTS_ATTENTION_MASK], device=device\n",
    "        )\n",
    "\n",
    "    return sample_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1a09d4b47b6f4d0397fc71c82d4a1cd1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 5 files:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "# Loading the tokenizer\n",
    "tokenizer_op = ModularTokenizerOp.from_pretrained(\"ibm/biomed.omics.bl.sm.ma-ted-458m\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Visualize participating sample dicts before and after the data processing for the model:\n",
      "initial_sample_dict={'data.protein_sequence': 'AAA'}\n",
      "processed_sample_dict={'data.protein_sequence': 'AAA', 'data.query.encoder_input': '<@TOKENIZER-TYPE=AA><MOLECULAR_ENTITY><MOLECULAR_ENTITY_GENERAL_PROTEIN><SOLUBILITY><SENTINEL_ID_0><@TOKENIZER-TYPE=AA@MAX-LEN=1250><SEQUENCE_NATURAL_START>AAA<SEQUENCE_NATURAL_END><EOS>', 'data.query.encoder_input.with_placeholders': '<@TOKENIZER-TYPE=AA><MOLECULAR_ENTITY><MOLECULAR_ENTITY_GENERAL_PROTEIN><SOLUBILITY><SENTINEL_ID_0><@TOKENIZER-TYPE=AA@MAX-LEN=1250><SEQUENCE_NATURAL_START>AAA<SEQUENCE_NATURAL_END><EOS>', 'data.query.encoder_input.per_meta_part_encoding': [Encoding(num_tokens=4, attributes=[ids, type_ids, tokens, offsets, attention_mask, special_tokens_mask, overflowing]), Encoding(num_tokens=6, attributes=[ids, type_ids, tokens, offsets, attention_mask, special_tokens_mask, overflowing])], 'data.encoder_input_token_ids': tensor([  6, 274,  27,  ...,   1,   1,   1]), 'data.encoder_input_attention_mask': tensor([ True,  True,  True,  ..., False, False, False])}\n"
     ]
    }
   ],
   "source": [
    "import copy\n",
    "\n",
    "protein_sequence_key = \"data.protein_sequence\"\n",
    "\n",
    "# Define dummy initial sample dict\n",
    "initial_sample_dict = dict()\n",
    "initial_sample_dict[protein_sequence_key] = \"AAA\"\n",
    "\n",
    "processed_sample_dict = data_preprocessing(\n",
    "    copy.deepcopy(initial_sample_dict),\n",
    "    protein_sequence_key=protein_sequence_key,\n",
    "    tokenizer_op=tokenizer_op,\n",
    ")\n",
    "\n",
    "print(\n",
    "    \"Visualize participating sample dicts before and after the data processing for the model:\"\n",
    ")\n",
    "print(f\"{initial_sample_dict=}\")\n",
    "print(f\"{processed_sample_dict=}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. `process_model_output()` function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "def process_model_output(\n",
    "    tokenizer_op: ModularTokenizerOp,\n",
    "    decoder_output: np.ndarray,\n",
    "    decoder_output_scores: np.ndarray,\n",
    ") -> dict | None:\n",
    "    \"\"\"\n",
    "    Extract predicted solubility class and scores\n",
    "    expecting decoder output to be <SENTINEL_ID_0><0><EOS> or <SENTINEL_ID_0><1><EOS>\n",
    "    note - the normalized version will calculate the positive ('<1>') score divided by the sum of the scores for both '<0>' and '<1>'\n",
    "        BE CAREFUL as both negative and positive absolute scores can be drastically low, and normalized score could be very high.\n",
    "    outputs a dictionary containing:\n",
    "        dict(\n",
    "            predicted_token_str = #... e.g. '<1>'\n",
    "            not_normalized_score = #the score for the positive token... e.g.  0.01\n",
    "            normalized_score = #... (positive_token_score) / (positive_token_score+negative_token_score)\n",
    "        )\n",
    "        if there is any error in parsing the model output, None is returned.\n",
    "    \"\"\"\n",
    "\n",
    "    negative_token_id = tokenizer_op.get_token_id(\"<0>\")\n",
    "    positive_token_id = tokenizer_op.get_token_id(\"<1>\")\n",
    "    label_id_to_int = {\n",
    "        negative_token_id: 0,\n",
    "        positive_token_id: 1,\n",
    "    }\n",
    "    classification_position = 1\n",
    "\n",
    "    if decoder_output_scores is not None:\n",
    "        not_normalized_score = decoder_output_scores[\n",
    "            classification_position, positive_token_id\n",
    "        ]\n",
    "        normalized_score = not_normalized_score / (\n",
    "            not_normalized_score\n",
    "            + decoder_output_scores[classification_position, negative_token_id]\n",
    "            + 1e-10\n",
    "        )\n",
    "    ans = dict(\n",
    "        pred=label_id_to_int.get(int(decoder_output[classification_position]), -1),\n",
    "        not_normalized_scores=not_normalized_score,\n",
    "        normalized_scores=normalized_score,\n",
    "    )\n",
    "\n",
    "    return ans"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finally - Define Task Object\n",
    "\n",
    "Now we can combine all of the components together to form our `MammalTask` object that will define the new task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Any\n",
    "\n",
    "from mammal.metrics import classification_metrics\n",
    "from mammal.task import (\n",
    "    MammalTask,\n",
    "    MetricBase,\n",
    ")\n",
    "\n",
    "\n",
    "class ProteinSolubilityTask(MammalTask):\n",
    "    def __init__(\n",
    "        self,\n",
    "        *,\n",
    "        name: str,\n",
    "        tokenizer_op: ModularTokenizerOp,\n",
    "        data_module_kwargs: dict,\n",
    "        seed: int,\n",
    "        logger: Any | None = None,\n",
    "    ) -> None:\n",
    "        super().__init__(\n",
    "            name=name,\n",
    "            logger=logger,\n",
    "            tokenizer_op=tokenizer_op,\n",
    "        )\n",
    "        self._data_module_kwargs = data_module_kwargs\n",
    "        self._seed = seed\n",
    "\n",
    "        self.preds_key = CLS_PRED\n",
    "        self.scores_key = SCORES\n",
    "        self.labels_key = LABELS_TOKENS\n",
    "\n",
    "    def data_module(self) -> pl.LightningDataModule:\n",
    "        return ProteinSolubilityDataModule(\n",
    "            tokenizer_op=self._tokenizer_op,\n",
    "            seed=self._seed,\n",
    "            data_preprocessing=self.data_preprocessing,\n",
    "            **self._data_module_kwargs,\n",
    "        )\n",
    "\n",
    "    def train_metrics(self) -> dict[str, MetricBase]:\n",
    "        metrics = super().train_metrics()\n",
    "        metrics.update(\n",
    "            classification_metrics(\n",
    "                self.name(),\n",
    "                class_position=1,\n",
    "                tokenizer_op=self._tokenizer_op,\n",
    "                class_tokens=[\"<0>\", \"<1>\"],\n",
    "            )\n",
    "        )\n",
    "\n",
    "        return metrics\n",
    "\n",
    "    def validation_metrics(self) -> dict[str, MetricBase]:\n",
    "        validation_metrics = super().validation_metrics()\n",
    "        validation_metrics.update(\n",
    "            classification_metrics(\n",
    "                self.name(),\n",
    "                class_position=1,\n",
    "                tokenizer_op=self._tokenizer_op,\n",
    "                class_tokens=[\"<0>\", \"<1>\"],\n",
    "            )\n",
    "        )\n",
    "        return validation_metrics\n",
    "\n",
    "    @staticmethod\n",
    "    def data_preprocessing(\n",
    "        sample_dict: dict,\n",
    "        *,\n",
    "        protein_sequence_key: str,\n",
    "        tokenizer_op: ModularTokenizerOp,\n",
    "        solubility_label_key: int | None = None,\n",
    "        protein_max_seq_length: int = 1250,\n",
    "        encoder_input_max_seq_len: int | None = 1260,\n",
    "        labels_max_seq_len: int | None = 4,\n",
    "        device: str | torch.device = \"cpu\",\n",
    "    ) -> dict:\n",
    "\n",
    "        # We use the method we defined above, just to make it cleaner\n",
    "        sample_dict = data_preprocessing(\n",
    "            sample_dict=sample_dict,\n",
    "            tokenizer_op=tokenizer_op,\n",
    "            protein_sequence_key=protein_sequence_key,\n",
    "            solubility_label_key=solubility_label_key,\n",
    "            protein_max_seq_length=protein_max_seq_length,\n",
    "            encoder_input_max_seq_len=encoder_input_max_seq_len,\n",
    "            labels_max_seq_len=labels_max_seq_len,\n",
    "            device=device,\n",
    "        )\n",
    "        return sample_dict\n",
    "\n",
    "    @staticmethod\n",
    "    def process_model_output(\n",
    "        tokenizer_op: ModularTokenizerOp,\n",
    "        decoder_output: np.ndarray,\n",
    "        decoder_output_scores: np.ndarray,\n",
    "    ) -> dict | None:\n",
    "\n",
    "        # We use the method we defined above, just to make it cleaner\n",
    "        ans = process_model_output(\n",
    "            tokenizer_op=tokenizer_op,\n",
    "            decoder_output=decoder_output,\n",
    "            decoder_output_scores=decoder_output_scores,\n",
    "        )\n",
    "\n",
    "        return ans"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MORE\n",
    "\n",
    "Losses, Metrics..\n",
    "\n",
    "we have impl for reg and cls"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mammal_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
