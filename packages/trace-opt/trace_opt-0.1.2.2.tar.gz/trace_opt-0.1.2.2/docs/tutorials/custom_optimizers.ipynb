{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Building Custom Optimizer\n",
                "\n",
                "We give a tutorial on how to build custom optimizers in Trace. We will demonstrate how the classical back-propagation and gradient descent algorithms can be implemented in Trace as an optimizer. We will show two ways to do this. The first is through implementing the back-propagation algorithm within the Trace optimzier, which operates on Trace graph. The second is to overload the propagator to propagate gradeints directly in Trace, instead of Trace graph. This example shows the flexibilty of the Trace framework."
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Basic back-propagation and gradient descent with PyTorch\n",
                "\n",
                "To start, let's define a simple objective and run vanilla gradient descent to optimize the variable in pytorch. This code will be used as the reference of desired behaviors. We make the code below transparent for tutorial purppose, so we use the `torch.autograd.grad` api and write down the gradient descent update rule manually."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "outputs": [],
            "source": [
                "!pip install trace-opt\n",
                "!pip install torch"
            ],
            "metadata": {
                "collapsed": false
            }
        },
        {
            "cell_type": "code",
            "execution_count": 1,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Vanilla gradient descent implementation using PyTorch\n",
                        "  Loss at iter 0: 1.5\n",
                        "  Loss at iter 1: 1.1200000047683716\n",
                        "  Loss at iter 2: 0.8122000098228455\n",
                        "  Loss at iter 3: 0.5628820061683655\n",
                        "  Loss at iter 4: 0.36093443632125854\n",
                        "  Loss at iter 5: 0.19735687971115112\n",
                        "  Loss at iter 6: 0.0648590698838234\n",
                        "  Loss at iter 7: 0.04434824362397194\n",
                        "  Loss at iter 8: 0.06279093772172928\n",
                        "  Loss at iter 9: 0.046178679913282394\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "/home/chinganc/miniconda3/envs/trace-3.9/lib/python3.9/site-packages/torch/autograd/graph.py:744: UserWarning: CUDA initialization: The NVIDIA driver on your system is too old (found version 11030). Please update your GPU driver by downloading and installing a new version from the URL: http://www.nvidia.com/Download/index.aspx Alternatively, go to: https://pytorch.org to install a PyTorch version that has been compiled with your version of the CUDA driver. (Triggered internally at ../c10/cuda/CUDAFunctions.cpp:108.)\n",
                        "  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n"
                    ]
                }
            ],
            "source": [
                "import torch\n",
                "\n",
                "stepsize = 0.1\n",
                "print('Vanilla gradient descent implementation using PyTorch')\n",
                "param  = torch.tensor(1.0, requires_grad=True)  # this is the param we optimize\n",
                "def forward():\n",
                "    x = param\n",
                "    return torch.abs(x) + torch.square(x) * torch.tensor(0.5, requires_grad=True)\n",
                "for i in range(10):\n",
                "    y = forward()\n",
                "    g = torch.autograd.grad(y, [param], torch.tensor(1.0))\n",
                "    param = param - stepsize * g[0]\n",
                "    print(f'  Loss at iter {i}: {y.data}')\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Set up the objective in Trace\n",
                "\n",
                "After seeing how ideally basic gradient descent + back-propagation behaves, next we show how it can be implemented it in Trace. To this end, we need to turn each math ops used in the above loss as a `bundle`, and define the parameter as a `node`. In this way, Trace can create a computational graph (DAG) of the workflow of computing the objective. We visualize the DAG below."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 2,
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "image/svg+xml": [
                            "<?xml version=\"1.0\" encoding=\"UTF-8\" standalone=\"no\"?>\n",
                            "<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n",
                            " \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n",
                            "<!-- Generated by graphviz version 2.40.1 (20161225.0304)\n",
                            " -->\n",
                            "<!-- Title: %3 Pages: 1 -->\n",
                            "<svg width=\"933pt\" height=\"394pt\"\n",
                            " viewBox=\"0.00 0.00 932.80 393.86\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n",
                            "<g id=\"graph0\" class=\"graph\" transform=\"scale(1 1) rotate(0) translate(4 389.86)\">\n",
                            "<title>%3</title>\n",
                            "<polygon fill=\"#ffffff\" stroke=\"transparent\" points=\"-4,4 -4,-389.86 928.796,-389.86 928.796,4 -4,4\"/>\n",
                            "<!-- abs0 -->\n",
                            "<g id=\"node1\" class=\"node\">\n",
                            "<title>abs0</title>\n",
                            "<ellipse fill=\"#deebf6\" stroke=\"#5c9bd5\" stroke-width=\"1.2\" cx=\"160.5132\" cy=\"-148.43\" rx=\"160.5265\" ry=\"37.4533\"/>\n",
                            "<text text-anchor=\"middle\" x=\"160.5132\" y=\"-159.73\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">abs0</text>\n",
                            "<text text-anchor=\"middle\" x=\"160.5132\" y=\"-144.73\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">[abs] .</text>\n",
                            "<text text-anchor=\"middle\" x=\"160.5132\" y=\"-129.73\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">tensor(1., grad_fn=&lt;AbsBackward0&gt;)</text>\n",
                            "</g>\n",
                            "<!-- add0 -->\n",
                            "<g id=\"node2\" class=\"node\">\n",
                            "<title>add0</title>\n",
                            "<ellipse fill=\"#deebf6\" stroke=\"#5c9bd5\" stroke-width=\"1.2\" cx=\"352.5132\" cy=\"-37.4767\" rx=\"180.1247\" ry=\"37.4533\"/>\n",
                            "<text text-anchor=\"middle\" x=\"352.5132\" y=\"-48.7767\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">add0</text>\n",
                            "<text text-anchor=\"middle\" x=\"352.5132\" y=\"-33.7767\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">[add] This is an add operator of x and y. .</text>\n",
                            "<text text-anchor=\"middle\" x=\"352.5132\" y=\"-18.7767\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">tensor(1.5000, grad_fn=&lt;AddBackward0&gt;)</text>\n",
                            "</g>\n",
                            "<!-- abs0&#45;&gt;add0 -->\n",
                            "<g id=\"edge1\" class=\"edge\">\n",
                            "<title>abs0&#45;&gt;add0</title>\n",
                            "<path fill=\"none\" stroke=\"#000000\" d=\"M220.7364,-113.6281C240.2794,-102.3346 262.1306,-89.7071 282.3337,-78.0321\"/>\n",
                            "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"284.2746,-80.953 291.1817,-72.9191 280.7722,-74.8922 284.2746,-80.953\"/>\n",
                            "</g>\n",
                            "<!-- multiply0 -->\n",
                            "<g id=\"node3\" class=\"node\">\n",
                            "<title>multiply0</title>\n",
                            "<ellipse fill=\"#deebf6\" stroke=\"#5c9bd5\" stroke-width=\"1.2\" cx=\"545.5132\" cy=\"-148.43\" rx=\"206.4504\" ry=\"37.4533\"/>\n",
                            "<text text-anchor=\"middle\" x=\"545.5132\" y=\"-159.73\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">multiply0</text>\n",
                            "<text text-anchor=\"middle\" x=\"545.5132\" y=\"-144.73\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">[multiply] This is a multiply operator of x and y. .</text>\n",
                            "<text text-anchor=\"middle\" x=\"545.5132\" y=\"-129.73\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">tensor(0.5000, grad_fn=&lt;MulBackward0&gt;)</text>\n",
                            "</g>\n",
                            "<!-- multiply0&#45;&gt;add0 -->\n",
                            "<g id=\"edge2\" class=\"edge\">\n",
                            "<title>multiply0&#45;&gt;add0</title>\n",
                            "<path fill=\"none\" stroke=\"#000000\" d=\"M483.1182,-112.5598C463.9216,-101.524 442.6586,-89.3001 422.9611,-77.9763\"/>\n",
                            "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"424.4479,-74.7939 414.034,-72.8442 420.9591,-80.8626 424.4479,-74.7939\"/>\n",
                            "</g>\n",
                            "<!-- square0 -->\n",
                            "<g id=\"node4\" class=\"node\">\n",
                            "<title>square0</title>\n",
                            "<ellipse fill=\"#deebf6\" stroke=\"#5c9bd5\" stroke-width=\"1.2\" cx=\"350.5132\" cy=\"-259.3833\" rx=\"161.8549\" ry=\"37.4533\"/>\n",
                            "<text text-anchor=\"middle\" x=\"350.5132\" y=\"-270.6833\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">square0</text>\n",
                            "<text text-anchor=\"middle\" x=\"350.5132\" y=\"-255.6833\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">[square] .</text>\n",
                            "<text text-anchor=\"middle\" x=\"350.5132\" y=\"-240.6833\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">tensor(1., grad_fn=&lt;PowBackward0&gt;)</text>\n",
                            "</g>\n",
                            "<!-- square0&#45;&gt;multiply0 -->\n",
                            "<g id=\"edge3\" class=\"edge\">\n",
                            "<title>square0&#45;&gt;multiply0</title>\n",
                            "<path fill=\"none\" stroke=\"#000000\" d=\"M411.6774,-224.5814C431.3311,-213.3986 453.2835,-200.9079 473.6332,-189.3291\"/>\n",
                            "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"475.5888,-192.2434 482.5494,-184.2559 472.1269,-186.1593 475.5888,-192.2434\"/>\n",
                            "</g>\n",
                            "<!-- Tensor1 -->\n",
                            "<g id=\"node5\" class=\"node\">\n",
                            "<title>Tensor1</title>\n",
                            "<ellipse fill=\"#deebf6\" stroke=\"#5c9bd5\" stroke-width=\"1.2\" cx=\"727.5132\" cy=\"-259.3833\" rx=\"197.0658\" ry=\"37.4533\"/>\n",
                            "<text text-anchor=\"middle\" x=\"727.5132\" y=\"-270.6833\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">Tensor1</text>\n",
                            "<text text-anchor=\"middle\" x=\"727.5132\" y=\"-255.6833\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">[Node] This is a node in a computational graph.</text>\n",
                            "<text text-anchor=\"middle\" x=\"727.5132\" y=\"-240.6833\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">tensor(0.5000, requires_grad=True)</text>\n",
                            "</g>\n",
                            "<!-- Tensor1&#45;&gt;multiply0 -->\n",
                            "<g id=\"edge4\" class=\"edge\">\n",
                            "<title>Tensor1&#45;&gt;multiply0</title>\n",
                            "<path fill=\"none\" stroke=\"#000000\" d=\"M668.6744,-223.5132C651.0028,-212.7399 631.4742,-200.8346 613.2746,-189.7396\"/>\n",
                            "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"614.8213,-186.5834 604.461,-184.3665 611.1776,-192.5603 614.8213,-186.5834\"/>\n",
                            "</g>\n",
                            "<!-- Tensor0 -->\n",
                            "<g id=\"node6\" class=\"node\">\n",
                            "<title>Tensor0</title>\n",
                            "<polygon fill=\"#ffe5e5\" stroke=\"#ff7e79\" stroke-width=\"1.2\" points=\"452.0132,-385.86 59.0132,-385.86 59.0132,-332.86 452.0132,-332.86 452.0132,-385.86\"/>\n",
                            "<text text-anchor=\"middle\" x=\"255.5132\" y=\"-370.66\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">Tensor0</text>\n",
                            "<text text-anchor=\"middle\" x=\"255.5132\" y=\"-355.66\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">[ParameterNode] This is a ParameterNode in a computational graph.</text>\n",
                            "<text text-anchor=\"middle\" x=\"255.5132\" y=\"-340.66\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">tensor(1., requires_grad=True)</text>\n",
                            "</g>\n",
                            "<!-- Tensor0&#45;&gt;abs0 -->\n",
                            "<g id=\"edge6\" class=\"edge\">\n",
                            "<title>Tensor0&#45;&gt;abs0</title>\n",
                            "<path fill=\"none\" stroke=\"#000000\" d=\"M212.061,-332.8356C199.5916,-323.0738 187.2814,-310.914 179.5132,-296.86 162.65,-266.3512 158.2445,-227.0471 157.8701,-196.5491\"/>\n",
                            "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"161.3708,-196.1651 157.892,-186.1577 154.3709,-196.1503 161.3708,-196.1651\"/>\n",
                            "</g>\n",
                            "<!-- Tensor0&#45;&gt;square0 -->\n",
                            "<g id=\"edge5\" class=\"edge\">\n",
                            "<title>Tensor0&#45;&gt;square0</title>\n",
                            "<path fill=\"none\" stroke=\"#000000\" d=\"M280.7221,-332.8305C289.22,-323.8875 298.9489,-313.6489 308.3907,-303.7125\"/>\n",
                            "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"311.1786,-305.8595 315.5298,-296.1994 306.1042,-301.0377 311.1786,-305.8595\"/>\n",
                            "</g>\n",
                            "</g>\n",
                            "</svg>\n"
                        ],
                        "text/plain": [
                            "<graphviz.graphs.Digraph at 0x7f1f09c6c8b0>"
                        ]
                    },
                    "execution_count": 2,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": [
                "from opto.trace import bundle, node\n",
                "from opto.trace.propagators.propagators import Propagator\n",
                "\n",
                "@bundle()\n",
                "def abs(x):\n",
                "    return torch.abs(x)\n",
                "\n",
                "@bundle()\n",
                "def square(x):\n",
                "    return torch.square(x)\n",
                "\n",
                "param  = node(torch.tensor(1.0, requires_grad=True), trainable=True)\n",
                "def forward():\n",
                "    x = param\n",
                "    return abs(x) + square(x) * torch.tensor(0.5, requires_grad=True)\n",
                "\n",
                "forward().backward(visualize=True)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Version 1 Trace Implementation based on Optimizer\n",
                "\n",
                "The first way is to implement the back-propagation algorithm as part of the optimizer in Trace. By default, optimzers in Trace receive the propagated Trace graph at the parameter nodes. Trace graph is a generalization of gradient. Here we show how we can implement back-propagation on the Trace graph to recover the propagated gradient and use it for gradient descent. We can see the loss sequence here matches what we had above implemented by PyTorch.\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 3,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Version 1 gradient descent implementation using Trace\n",
                        "  Loss at iter 0: 1.5\n",
                        "  Loss at iter 1: 1.1200000047683716\n",
                        "  Loss at iter 2: 0.8122000098228455\n",
                        "  Loss at iter 3: 0.5628820061683655\n",
                        "  Loss at iter 4: 0.36093443632125854\n",
                        "  Loss at iter 5: 0.19735687971115112\n",
                        "  Loss at iter 6: 0.0648590698838234\n",
                        "  Loss at iter 7: 0.04434824362397194\n",
                        "  Loss at iter 8: 0.06279093772172928\n",
                        "  Loss at iter 9: 0.046178679913282394\n"
                    ]
                }
            ],
            "source": [
                "from opto.optimizers.optimizer import Optimizer\n",
                "from collections import defaultdict\n",
                "\n",
                "\n",
                "class BackPropagationGradientDescent(Optimizer):\n",
                "\n",
                "    def __init__(self, parameters, stepsize, *args, **kwargs):\n",
                "        super().__init__(parameters, *args, **kwargs)\n",
                "        self.stepsize = stepsize\n",
                "\n",
                "    def _step(self, *args, **kwargs):\n",
                "        \"\"\"Return the new data of parameter nodes based on the feedback.\"\"\"\n",
                "        trace_graph = self.trace_graph   # aggregate the trace graphes into one.\n",
                "        grads = defaultdict(lambda: torch.tensor(0.0))\n",
                "        # trace_graph.graph is a list of nodes sorted according to the topological order\n",
                "        for i, ( _, x) in enumerate(reversed(trace_graph.graph)):  # back-propagation starts from the last node\n",
                "            if len(x.parents) == 0:\n",
                "                continue\n",
                "            g = trace_graph.user_feedback if i == 0 else grads[x]\n",
                "            propagated_grads = torch.autograd.grad(x.data,  [p.data for p in x.parents], g)  # propagate the gradient\n",
                "            for p, pg in zip(x.parents, propagated_grads):\n",
                "                grads[p] += pg  #  accumulate gradient\n",
                "        return {p: p.data - self.stepsize * grads[p] for p in self.parameters}  # propose new update\n",
                "\n",
                "\n",
                "\n",
                "bp = BackPropagationGradientDescent([param], stepsize=stepsize)\n",
                "print('Version 1 gradient descent implementation using Trace')\n",
                "\n",
                "for i in range(10):\n",
                "    y = forward()\n",
                "    bp.zero_feedback()\n",
                "    bp.backward(y, torch.tensor(1.0))\n",
                "    bp.step()\n",
                "    print(f'  Loss at iter {i}: {y.data}')\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Version 2 Trace Implementation based on Propagator + Optimizer\n",
                "\n",
                "Another way is to override the what's propagated in the `backward` call of Trace. Trace has a generic backward routine performed on the computational graph that can support designing new end-to-end optimization algorithms. While by default Trace propagates Trace graphes in `backward` for generality, for the differentiable problems here we can override the behavior and let it directly propagate gradients. In this way, the optimizer would receive directly the propagted gradient instead of Trace graphs.\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 4,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Implementation by Propagator\n",
                        "Version 2 gradient descent implementation using Trace\n",
                        "  Loss at iter 0: 1.5\n",
                        "  Loss at iter 1: 1.1200000047683716\n",
                        "  Loss at iter 2: 0.8122000098228455\n",
                        "  Loss at iter 3: 0.5628820061683655\n",
                        "  Loss at iter 4: 0.36093443632125854\n",
                        "  Loss at iter 5: 0.19735687971115112\n",
                        "  Loss at iter 6: 0.0648590698838234\n",
                        "  Loss at iter 7: 0.04434824362397194\n",
                        "  Loss at iter 8: 0.06279093772172928\n",
                        "  Loss at iter 9: 0.046178679913282394\n"
                    ]
                }
            ],
            "source": [
                "\n",
                "print('Implementation by Propagator')\n",
                "\n",
                "\n",
                "# We create a custom propagator that back-propagates the gradient\n",
                "class BackPropagator(Propagator):\n",
                "\n",
                "    def init_feedback(self, node, feedback):\n",
                "        return feedback\n",
                "\n",
                "    def _propagate(self, child):\n",
                "        grad = sum(sum(v) for v in child.feedback.values())\n",
                "        propagated_grads = torch.autograd.grad(child.data,  [p.data for p in child.parents], grad)\n",
                "        return {p: pg for p, pg in zip(child.parents, propagated_grads)}\n",
                "\n",
                "\n",
                "class GradientDescent(Optimizer):\n",
                "\n",
                "    def __init__(self, parameters, stepsize, *args, **kwargs):\n",
                "        super().__init__(parameters, *args, **kwargs)\n",
                "        self.stepsize = stepsize\n",
                "\n",
                "    def default_propagator(self):\n",
                "        # use the custom propagator instead of the default one, which propagates Trace graph\n",
                "        return BackPropagator()\n",
                "\n",
                "    def _step(self, *args, **kwargs):\n",
                "        # simpel gradient descent\n",
                "        return {p: p.data - self.stepsize * sum(sum(v) for v in p.feedback.values()) for p in self.parameters}  # propose new update\n",
                "\n",
                "\n",
                "\n",
                "param  = node(torch.tensor(1.0, requires_grad=True), trainable=True)  # reset\n",
                "bp = GradientDescent([param], stepsize=stepsize)\n",
                "print('Version 2 gradient descent implementation using Trace')\n",
                "\n",
                "for i in range(10):\n",
                "    y = forward()\n",
                "    bp.zero_feedback()\n",
                "    bp.backward(y, torch.tensor(1.0))\n",
                "    bp.step()\n",
                "    print(f'  Loss at iter {i}: {y.data}')\n"
            ]
        }
    ],
    "metadata": {
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.8.19"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 2
}