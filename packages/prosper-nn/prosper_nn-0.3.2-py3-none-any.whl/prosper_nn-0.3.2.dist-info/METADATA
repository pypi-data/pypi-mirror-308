Metadata-Version: 2.1
Name: prosper-nn
Version: 0.3.2
Summary: Package contains, in PyTorch implemented, neural networks with problem specific pre-structuring architectures and utils that help building and understanding models.
Home-page: UNKNOWN
Author: Nico Beck, Julia Schemm
Author-email: nico.beck@iis.fraunhofer.de
License: UNKNOWN
Platform: UNKNOWN
Requires-Python: >=3.6
Description-Content-Type: text/markdown
License-File: LICENSE
Requires-Dist: matplotlib<3.7,>=3.4.1
Requires-Dist: mypy>=0.782
Requires-Dist: numpy<2,>=1.20.2
Requires-Dist: pandas<2,>=1.1.2
Requires-Dist: scipy<2,>=1.5.0
Requires-Dist: seaborn<1,>=0.11.0
Requires-Dist: torch<3,>=1.6.0

# Prosper_NN

## Problem-Specific Pre-Structuring of Neural Networks

Accurate data-driven forecasts can provide a crucial advantage in many application areas. One of the methods with the most promising results in forecasting time series are neural networks. However, especially in macro-economic applications, it can be difficult and time-consuming to adapt state-of-the-art neural network architectures in a way that leads to satisfying results. For instance, the final prices of materials and stocks result from a highly complex interplay between supply and demand. Additionally, there is often only one (albeit long) historical time series available for training which makes correlations in the data difficult to detect.

Under these circumstances, applying state-of-the-art neural networks architectures successfully poses a great challenge. Pre-structuring the models can solve this problem. For this purpose, Zimmermann, Tietz and Grothmann (Neural Networks: Tricks of the Trade, 2012) propose recurrent architectures for various time series problems that help recognize correlations. They recommend Error-Correction Neural Networks (ECNNs), Historical-Consistent Neural Networks (HCNNs) and Causal-Retro-Causal Neural Networks (CRCNNs). One of the main ideas of the pre-structuring is embedding the model in a larger architecture in order to use the past prediction errors for predicting the next time step. The three approaches mentioned use this idea and apply it in different settings. So far, the proposed architectures are not publicly available in common machine learning frameworks. Therefore, we have implemented the models in PyTorch. This way, we can easily test them on diverse datasets.
In this package the neural network architectures developed by Hans-Georg Zimmermann are implemented in PyTorch.
The full documentation can be found here https://iis-scs-a.pages.fraunhofer.de/prosper/prosper/. There are also tutorials that show how to work with the package.

