{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b628388-c814-4e58-9367-359f78e9f4b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pysam\n",
    "import pandas as pd\n",
    "from Bio.Seq import Seq\n",
    "import Levenshtein as lev\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "from typing import List, Dict, Tuple\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "\n",
    "def load_primers(primer_file):\n",
    "    \"\"\"Load and prepare primers dataframe\"\"\"\n",
    "    if not os.path.exists(primer_file):\n",
    "        raise FileNotFoundError(f\"Primer file not found: {primer_file}\")\n",
    "        \n",
    "    primers_df = pd.read_csv(primer_file, sep=\"\\t\")\n",
    "    primers_df = primers_df.dropna(subset=['Forward', 'Reverse'])\n",
    "    longest_primer_length = max(\n",
    "        primers_df['Forward'].apply(len).max(), \n",
    "        primers_df['Reverse'].apply(len).max()\n",
    "    )\n",
    "    return primers_df, longest_primer_length\n",
    "\n",
    "def is_match(seq1, seq2, max_distance=2):\n",
    "    \"\"\"Check for approx match using Levenshtein distance.\"\"\"\n",
    "    if not seq1 or not seq2:\n",
    "        return False\n",
    "    \n",
    "    try:\n",
    "        for i in range(len(seq1) - len(seq2) + 1):\n",
    "            window = seq1[i:i+len(seq2)]\n",
    "            if len(window) == len(seq2):\n",
    "                distance = lev.distance(str(window), str(seq2))\n",
    "                if distance <= max_distance:\n",
    "                    return True\n",
    "    except:\n",
    "        return False\n",
    "    return False\n",
    "\n",
    "def find_primers_in_region(sequence, primers_df, window_size=100, max_distance=2):\n",
    "    \"\"\"Find primers in a given sequence region\"\"\"\n",
    "    primers_found = []\n",
    "    \n",
    "    for _, primer in primers_df.iterrows():\n",
    "        forward_primer = primer['Forward']\n",
    "        reverse_primer = primer['Reverse']\n",
    "        reverse_complement_forward = str(Seq(forward_primer).reverse_complement())\n",
    "        reverse_complement_reverse = str(Seq(reverse_primer).reverse_complement())\n",
    "        \n",
    "        if is_match(sequence, forward_primer, max_distance):\n",
    "            primers_found.append(f\"{primer['Name']}_Forward\")\n",
    "        if is_match(sequence, reverse_primer, max_distance):\n",
    "            primers_found.append(f\"{primer['Name']}_Reverse\")\n",
    "        if is_match(sequence, reverse_complement_forward, max_distance):\n",
    "            primers_found.append(f\"{primer['Name']}_ForwardComp\")\n",
    "        if is_match(sequence, reverse_complement_reverse, max_distance):\n",
    "            primers_found.append(f\"{primer['Name']}_ReverseComp\")\n",
    "    \n",
    "    return list(set(primers_found))\n",
    "\n",
    "def process_read_chunk(chunk: List[pysam.AlignedSegment], primers_df: pd.DataFrame, \n",
    "                      search_window: int, unaligned_only: bool) -> List[Dict]:\n",
    "    \"\"\"Process a chunk of reads in parallel\"\"\"\n",
    "    chunk_data = []\n",
    "    \n",
    "    for read in chunk:\n",
    "        if unaligned_only and not read.is_unmapped:\n",
    "            continue\n",
    "        if read.query_sequence is None:\n",
    "            continue\n",
    "\n",
    "        read_sequence = read.query_sequence\n",
    "        read_length = len(read_sequence)\n",
    "        \n",
    "        start_region = read_sequence[:min(search_window, read_length)]\n",
    "        end_region = read_sequence[max(0, read_length - search_window):]\n",
    "        \n",
    "        start_primers_found = find_primers_in_region(start_region, primers_df, window_size=search_window)\n",
    "        end_primers_found = find_primers_in_region(end_region, primers_df, window_size=search_window)\n",
    "        \n",
    "        chunk_data.append({\n",
    "            'Read_Name': read.query_name,\n",
    "            'Start_Primers': ', '.join(start_primers_found) if start_primers_found else 'None',\n",
    "            'End_Primers': ', '.join(end_primers_found) if end_primers_found else 'None',\n",
    "            'Read_Length': read_length\n",
    "        })\n",
    "    \n",
    "    return chunk_data\n",
    "\n",
    "def create_analysis_summary(result_df, primers_df):\n",
    "    \"\"\"Create a comprehensive summary of primer analysis results\"\"\"\n",
    "    if result_df.empty:\n",
    "        print(\"No reads to analyze in the results dataframe\")\n",
    "        return pd.DataFrame(), pd.DataFrame(), pd.DataFrame()\n",
    "        \n",
    "    total_reads = len(result_df)\n",
    "    \n",
    "    def get_base_primer_name(primer_str):\n",
    "        if primer_str == 'None':\n",
    "            return None\n",
    "        return primer_str.rsplit('_', 1)[0]\n",
    "    \n",
    "    def get_primer_orientation(primer_str):\n",
    "        if primer_str == 'None':\n",
    "            return None\n",
    "        return primer_str.rsplit('_', 1)[1]\n",
    "    \n",
    "    result_df['Start_Primer_Name'] = result_df['Start_Primers'].apply(get_base_primer_name)\n",
    "    result_df['End_Primer_Name'] = result_df['End_Primers'].apply(get_base_primer_name)\n",
    "    result_df['Start_Orientation'] = result_df['Start_Primers'].apply(get_primer_orientation)\n",
    "    result_df['End_Orientation'] = result_df['End_Primers'].apply(get_primer_orientation)\n",
    "    \n",
    "    no_primers = result_df[\n",
    "        (result_df['Start_Primers'] == 'None') & \n",
    "        (result_df['End_Primers'] == 'None')\n",
    "    ]\n",
    "    \n",
    "    single_end_only = result_df[\n",
    "        ((result_df['Start_Primers'] != 'None') & (result_df['End_Primers'] == 'None')) |\n",
    "        ((result_df['Start_Primers'] == 'None') & (result_df['End_Primers'] != 'None'))\n",
    "    ]\n",
    "    \n",
    "    both_ends = result_df[\n",
    "        (result_df['Start_Primers'] != 'None') & \n",
    "        (result_df['End_Primers'] != 'None')\n",
    "    ]\n",
    "    \n",
    "    matched_pairs = both_ends[\n",
    "        both_ends['Start_Primer_Name'] == both_ends['End_Primer_Name']\n",
    "    ]\n",
    "    \n",
    "    mismatched_pairs = both_ends[\n",
    "        both_ends['Start_Primer_Name'] != both_ends['End_Primer_Name']\n",
    "    ]\n",
    "    \n",
    "    primer_sizes = primers_df.set_index('Name')['Size'].to_dict()\n",
    "    \n",
    "    def is_correct_orientation(row):\n",
    "        if pd.isna(row['Start_Orientation']) or pd.isna(row['End_Orientation']):\n",
    "            return False\n",
    "        return (\n",
    "            (row['Start_Orientation'].startswith('Forward') and \n",
    "             row['End_Orientation'].startswith('Reverse')) or\n",
    "            (row['Start_Orientation'].startswith('Reverse') and \n",
    "             row['End_Orientation'].startswith('Forward'))\n",
    "        )\n",
    "    \n",
    "    def is_size_compliant(row):\n",
    "        expected = primer_sizes.get(row['Start_Primer_Name'])\n",
    "        if pd.isna(expected):\n",
    "            return False\n",
    "        tolerance = expected * 0.10\n",
    "        return abs(row['Read_Length'] - expected) <= tolerance\n",
    "    \n",
    "    matched_pairs['Correct_Orientation'] = matched_pairs.apply(is_correct_orientation, axis=1)\n",
    "    matched_pairs['Size_Compliant'] = matched_pairs.apply(is_size_compliant, axis=1)\n",
    "    \n",
    "    summary_data = [\n",
    "        {\n",
    "            'Category': 'No primers detected',\n",
    "            'Count': len(no_primers),\n",
    "            'Percentage': (len(no_primers) / total_reads) * 100\n",
    "        },\n",
    "        {\n",
    "            'Category': 'Single-end primers only',\n",
    "            'Count': len(single_end_only),\n",
    "            'Percentage': (len(single_end_only) / total_reads) * 100\n",
    "        },\n",
    "        {\n",
    "            'Category': 'Mismatched primer pairs',\n",
    "            'Count': len(mismatched_pairs),\n",
    "            'Percentage': (len(mismatched_pairs) / total_reads) * 100\n",
    "        },\n",
    "        {\n",
    "            'Category': 'Matched pairs - incorrect orientation',\n",
    "            'Count': len(matched_pairs[~matched_pairs['Correct_Orientation']]),\n",
    "            'Percentage': (len(matched_pairs[~matched_pairs['Correct_Orientation']]) / total_reads) * 100\n",
    "        },\n",
    "        {\n",
    "            'Category': 'Matched pairs - correct orientation, wrong size',\n",
    "            'Count': len(matched_pairs[\n",
    "                matched_pairs['Correct_Orientation'] & \n",
    "                ~matched_pairs['Size_Compliant']\n",
    "            ]),\n",
    "            'Percentage': (len(matched_pairs[\n",
    "                matched_pairs['Correct_Orientation'] & \n",
    "                ~matched_pairs['Size_Compliant']\n",
    "            ]) / total_reads) * 100\n",
    "        },\n",
    "        {\n",
    "            'Category': 'Matched pairs - correct orientation and size',\n",
    "            'Count': len(matched_pairs[\n",
    "                matched_pairs['Correct_Orientation'] & \n",
    "                matched_pairs['Size_Compliant']\n",
    "            ]),\n",
    "            'Percentage': (len(matched_pairs[\n",
    "                matched_pairs['Correct_Orientation'] & \n",
    "                matched_pairs['Size_Compliant']\n",
    "            ]) / total_reads) * 100\n",
    "        }\n",
    "    ]\n",
    "    \n",
    "    summary_df = pd.DataFrame(summary_data)\n",
    "    summary_df['Percentage'] = summary_df['Percentage'].round(2)\n",
    "    \n",
    "    return summary_df, matched_pairs, mismatched_pairs\n",
    "\n",
    "def bam_to_fasta_parallel(bam_path: str, primer_file: str, unaligned_only: bool = False, \n",
    "                         max_reads: int = 200, num_threads: int = 4, chunk_size: int = 50) -> pd.DataFrame:\n",
    "    \"\"\"Process BAM file and find primers in reads using multiple threads\"\"\"\n",
    "    # Validate input files\n",
    "    if not os.path.exists(bam_path):\n",
    "        raise FileNotFoundError(f\"BAM file not found: {bam_path}\")\n",
    "    \n",
    "    # Load primers\n",
    "    primers_df, longest_primer_length = load_primers(primer_file)\n",
    "    search_window = 100 + longest_primer_length\n",
    "    \n",
    "    print(f\"Loading BAM file: {bam_path}\")\n",
    "    try:\n",
    "        bam_file = pysam.AlignmentFile(bam_path, \"rb\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error opening BAM file: {e}\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    # Read all reads into memory (up to max_reads)\n",
    "    print(\"Loading reads into memory...\")\n",
    "    all_reads = []\n",
    "    for read in tqdm(bam_file.fetch(until_eof=True), total=max_reads if max_reads > 0 else None):\n",
    "        all_reads.append(read)\n",
    "        if max_reads > 0 and len(all_reads) >= max_reads:\n",
    "            break\n",
    "    \n",
    "    print(f\"Processing {len(all_reads)} reads with {num_threads} threads...\")\n",
    "    \n",
    "    # Split reads into chunks\n",
    "    chunks = [all_reads[i:i + chunk_size] for i in range(0, len(all_reads), chunk_size)]\n",
    "    \n",
    "    all_data = []\n",
    "    \n",
    "    # Process chunks in parallel with progress bar\n",
    "    with ThreadPoolExecutor(max_workers=num_threads) as executor:\n",
    "        future_to_chunk = {\n",
    "            executor.submit(\n",
    "                process_read_chunk, \n",
    "                chunk, \n",
    "                primers_df, \n",
    "                search_window, \n",
    "                unaligned_only\n",
    "            ): chunk for chunk in chunks\n",
    "        }\n",
    "        \n",
    "        for future in tqdm(as_completed(future_to_chunk), total=len(chunks), desc=\"Processing chunks\"):\n",
    "            try:\n",
    "                chunk_data = future.result()\n",
    "                all_data.extend(chunk_data)\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing chunk: {e}\")\n",
    "    \n",
    "    bam_file.close()\n",
    "    \n",
    "    if not all_data:\n",
    "        print(\"No data was processed successfully\")\n",
    "        return pd.DataFrame()\n",
    "        \n",
    "    return pd.DataFrame(all_data)\n",
    "\n",
    "def parallel_analysis_pipeline(bam_path: str, primer_file: str, num_threads: int = 4, \n",
    "                             max_reads: int = 200, chunk_size: int = 50):\n",
    "    \"\"\"Complete analysis pipeline using parallel processing\"\"\"\n",
    "    print(f\"Starting analysis with {num_threads} threads...\")\n",
    "    \n",
    "    try:\n",
    "        # Process BAM file in parallel\n",
    "        result_df = bam_to_fasta_parallel(\n",
    "            bam_path=bam_path,\n",
    "            primer_file=primer_file,\n",
    "            max_reads=max_reads,\n",
    "            num_threads=num_threads,\n",
    "            chunk_size=chunk_size\n",
    "        )\n",
    "        \n",
    "        if result_df.empty:\n",
    "            print(\"No results generated. Check input files and parameters.\")\n",
    "            return None\n",
    "        \n",
    "        print(f\"\\nProcessed {len(result_df)} reads successfully\")\n",
    "        \n",
    "        # Load primers for analysis\n",
    "        primers_df, _ = load_primers(primer_file)\n",
    "        \n",
    "        # Create analysis summary\n",
    "        summary_df, matched_pairs, mismatched_pairs = create_analysis_summary(result_df, primers_df)\n",
    "        \n",
    "        print(\"\\nAnalysis Summary:\")\n",
    "        print(summary_df.to_string(index=False))\n",
    "        \n",
    "        return {\n",
    "            'results': result_df,\n",
    "            'summary': summary_df,\n",
    "            'matched_pairs': matched_pairs,\n",
    "            'mismatched_pairs': mismatched_pairs\n",
    "        }\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error in analysis pipeline: {str(e)}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c902bed4-7e62-4b8b-be75-3d4785fa40d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def debug_pipeline(bam_path: str, primer_file: str, num_threads: int = 4, \n",
    "                  max_reads: int = 200, chunk_size: int = 50):\n",
    "    \"\"\"Debug version of the analysis pipeline with detailed logging\"\"\"\n",
    "    print(\"\\n=== Starting Debug Pipeline ===\")\n",
    "    \n",
    "    # Check file existence\n",
    "    print(f\"\\nChecking input files:\")\n",
    "    print(f\"BAM file path: {bam_path}\")\n",
    "    print(f\"BAM file exists: {os.path.exists(bam_path)}\")\n",
    "    print(f\"Primer file path: {primer_file}\")\n",
    "    print(f\"Primer file exists: {os.path.exists(primer_file)}\")\n",
    "    \n",
    "    # Try to load primers\n",
    "    print(\"\\nAttempting to load primers...\")\n",
    "    try:\n",
    "        primers_df, longest_primer_length = load_primers(primer_file)\n",
    "        print(f\"Successfully loaded {len(primers_df)} primers\")\n",
    "        print(f\"Longest primer length: {longest_primer_length}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading primers: {str(e)}\")\n",
    "        return None\n",
    "    \n",
    "    # Try to open BAM file\n",
    "    print(\"\\nAttempting to open BAM file...\")\n",
    "    try:\n",
    "        bam_file = pysam.AlignmentFile(bam_path, \"rb\")\n",
    "        # Try to read first read to verify file is valid\n",
    "        first_read = next(bam_file.fetch(until_eof=True), None)\n",
    "        if first_read is None:\n",
    "            print(\"BAM file appears to be empty\")\n",
    "            return None\n",
    "        print(\"Successfully opened BAM file and read first entry\")\n",
    "        bam_file.close()\n",
    "    except Exception as e:\n",
    "        print(f\"Error opening BAM file: {str(e)}\")\n",
    "        return None\n",
    "    \n",
    "    print(\"\\nStarting parallel analysis...\")\n",
    "    try:\n",
    "        results = parallel_analysis_pipeline(\n",
    "            bam_path=bam_path,\n",
    "            primer_file=primer_file,\n",
    "            num_threads=num_threads,\n",
    "            max_reads=max_reads,\n",
    "            chunk_size=chunk_size\n",
    "        )\n",
    "        \n",
    "        if results:\n",
    "            print(\"\\nAnalysis completed successfully\")\n",
    "            print(f\"Results contain {len(results['results'])} processed reads\")\n",
    "            return results\n",
    "        else:\n",
    "            print(\"\\nAnalysis completed but no results were returned\")\n",
    "            return None\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"\\nError in analysis: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "# Now run the debug pipeline\n",
    "debug_results = debug_pipeline(\n",
    "    bam_path=\"test.bam\",\n",
    "    primer_file=\"primers.tsv\",\n",
    "    num_threads=16,\n",
    "    max_reads=10000,\n",
    "    chunk_size=50\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "185beece-8420-48b4-b249-22a9f9052de8",
   "metadata": {},
   "outputs": [],
   "source": [
    "debug_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ef9b92a-d7f4-4377-9642-4b0d30d9d95a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
