import os
import sys
import re
from urllib.parse import parse_qs, urlparse
from alembic import command, config
from alembic.script import ScriptDirectory

from schemon.dao import (
    add_revision,
    add_revision_log,
    get_all_revision_ids,
    update_revision,
)
from schemon.migration.database import get_database
from schemon.migration.databricks_service import upgrade_table_properties
from schemon.migration.revision import (
    get_last_applied_revision_id,
    get_revision_full_content,
    get_revisions,
)


def get_database_url(db: str) -> str:
    """
    Retrieves the database URL based on the specified database key.

    Args:
        db (str): The database key.

    Returns:
        str: The database URL.
    """
    database = get_database({"key": db})
    if database is None:
        print(f"Database {db} not found.")
        sys.exit(1)
    
    database_url = database.value
    protocol = database_url.split("://")[0]
    if protocol == "databricks":
        # Retrieve database information
        databricks_token = os.getenv("DATABRICKS_TOKEN")
        database_url = database_url.format(DATABRICKS_TOKEN=databricks_token)
    elif protocol == "sqlite":
        NotImplemented
    elif protocol == "mssql+pyodbc":
        sqlserver_username = os.getenv("SQLSERVER_USERNAME")
        sqlserver_password = os.getenv("SQLSERVER_PASSWORD")
        database_url = database_url.format(
            SQLSERVER_USERNAME=sqlserver_username,
            SQLSERVER_PASSWORD=sqlserver_password
        )
    elif protocol == "mysql+mysqlconnector":
        mysql_password = os.getenv("MYSQL_PASSWORD")
        datatabase_certificate_path = os.getenv("DATABASE_CERTIFICATE_PATH")
        database_url = database_url.format(
            MYSQL_PASSWORD=mysql_password,
            DATABASE_CERTIFICATE_PATH=datatabase_certificate_path,
        )

    return database_url, protocol


def init_migration(database_url: str, yaml_dir: str) -> config.Config:
    """
    Initialize the Alembic configuration for migration.

    Args:
        database_url (str): The database URL.
        yaml_dir (str): The directory containing YAML files.

    Returns:
        config.Config: The initialized Alembic configuration object.
    """
    # Define the Alembic configuration file path
    cfg_path = "alembic.ini"

    # Create an Alembic config object and set necessary options
    cfg = config.Config(cfg_path)

    # Simulate the x_arg parsing by setting the arguments in the configuration
    cfg.set_section_option("x", "database_url", database_url)
    cfg.set_section_option("x", "yaml_dir", yaml_dir)
    cfg.set_section_option("x", "command", "downgrade")

    return cfg


def autogenerate_migration(db: str, yaml_dir: str) -> str:
    """
    Programmatically calls Alembic to autogenerate a migration using the specified db and yaml_dir.

    Args:
        db (str): The database key to use (passed with -x db).
        yaml_dir (str): The directory containing YAML files (passed with -x yaml_dir).

    Returns:
        str: The revision ID of the newly created migration.
    """
    try:
        database_url, protocol = get_database_url(db)
        cfg = init_migration(database_url, yaml_dir)

        script_location = cfg.get_main_option("script_location")
        migrations_dir = f"{script_location}/versions"

        last_revision_id = get_last_applied_revision_id(database_url)
        all_revision_ids = get_revisions(last_revision_id)
        for id in all_revision_ids:
            revision_full_content = get_revision_full_content(id)

            if revision_full_content:
                # Restore the last revision file from database
                filename = f"{id}_autogenerated_migration.py"

                # Construct the full file path
                file_path = os.path.join(migrations_dir, filename)

                # Write the revision content to the file
                with open(file_path, "w") as file:
                    file.write(revision_full_content)
                print(f"Restored revision {id} from the database to {file_path}")

        # Autogenerate the migration and get the revision ID
        revision_script = command.revision(
            cfg, "autogenerated migration", autogenerate=True
        )
        revision_id = revision_script.revision

        # Save the revision to the database
        migration_file = None
        for filename in os.listdir(migrations_dir):
            if filename.startswith(revision_id):
                migration_file = filename
                break

        if not migration_file:
            raise FileNotFoundError(
                f"Migration file starting with '{revision_id}' not found in '{migrations_dir}'."
            )

        # Load the file content
        migration_file_path = os.path.join(migrations_dir, migration_file)
        with open(migration_file_path, "r") as file:
            revision = file.read()

        prev_revision_id = extract_pre_revision_id(revision)

        add_revision(revision_id, prev_revision_id, db, revision)

        add_revision_log(revision_id, "autogenerate")

        print(
            f"Autogenerated migration created successfully with revision ID: {revision_id}."
        )
        return revision_id

    except Exception as e:
        print(f"An error occurred during migration autogeneration: {e}")
        raise


def upgrade_database(db: str, yaml_dir: str, revision: str = "head"):
    try:
        database_url, protocol = get_database_url(db)
        cfg = init_migration(database_url, yaml_dir)

        if revision == "head":
            # Load the script directory (migrations directory)
            script = ScriptDirectory.from_config(cfg)

            # Retrieve the head revision ID
            revision = script.get_current_head()

        # Upgrade the database to the specified revision
        command.upgrade(cfg, revision)

        # Upgrade the table properties
        if protocol == "databricks":
            parsed_url = urlparse(database_url)
            query_params = parse_qs(parsed_url.query)
            database = query_params.get("catalog", [None])[0]
            schema = query_params.get("schema", [None])[0]
            upgrade_table_properties(yaml_dir, database, schema)

        update_revision(revision, {"migrated": True})

        add_revision_log(revision, "upgrade")

        print(f"Upgraded successfully with revision ID: {revision}.")
        return revision

    except Exception as e:
        print(f"An error occurred during upgrade process: {e}")
        raise


def downgrade_database(db: str, yaml_dir: str, revision: str = "base"):
    try:
        database_url, protocol = get_database_url(db)
        cfg = init_migration(database_url, yaml_dir)

        # Downgrade the database to the specified revision
        command.downgrade(cfg, revision)

        if revision == "base":
            # Load the script directory (migrations directory)
            script = ScriptDirectory.from_config(cfg)

            # Retrieve the head revision ID
            revision = script.get_current_head()

        update_revision(revision, {"migrated": False})

        add_revision_log(revision, "downgrade")

        print(f"Downgraded successfully to revision ID: {revision}.")
        return revision

    except Exception as e:
        print(f"An error occurred during the downgrade process: {e}")
        raise


def extract_pre_revision_id(file_content: str) -> str:
    """
    Extract the value for the 'Revises' field from the given file content.

    Args:
        file_content (str): The content of the file as a string.

    Returns:
        str: The value of the 'Revises' field, or an empty string if not found.
    """
    # Define the regex pattern to extract the "Revises" value
    pattern = r"Revises: ([\w\d]+)"

    # Search for the pattern in the file content
    match = re.search(pattern, file_content)

    # Return the matched value or an empty string if not found
    if match:
        return match.group(1)
    else:
        return ""
