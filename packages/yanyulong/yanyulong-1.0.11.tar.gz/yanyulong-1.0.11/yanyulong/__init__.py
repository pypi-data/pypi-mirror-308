import pandas as pd
def get_yu():
    yu='''
    yu/
    '''
    print(yu)

def get_long():
    long='''
    long/
    '''
    print(long)
def lunshu_by_t(key):
    json_data=[
    {
        "title": "关联规则",
        "content": "反映了一个事物与其他事物之间相互依存性和关联性，如果两个事物或多个事物之间存在关联规则，那么其中的一个事物就可以由其他事物来预测到。关联规则必须在频繁项集中诞生且满足一定的置信度阈值\n整体步骤：1、生成频繁项集和生成规则2、找到强关联规则3、找出所有满足强关联规则的项集\n涉及的概念：项集（项的集合，包含k项的集合称为k项集）频繁项集（满足最小支持度阈值的项集）支持度（如果有两个不相交的非空集合X、Y(物品集)，N为数据记录总数，X集合和Y集合共同出现的次数占记录总数的比例：support(X->Y)=|X交Y|/N）置信度（集合X和集合Y共同出现的次数占集合X出现次数的比例，confidence(X->Y)=|X交Y|/|X|）提升度（表示置信度与Y总体发生的概率之比，lift(X->Y)=confidence(X->Y)/P(Y)）强关联规则（满足最小支持度阈值和最小置信度阈值的规则被称为强关联规则）"
    },
    {
        "title": "关联规则-Apriori算法",
        "content": "工作原理：1、首先扫描整个数据集，然后产生一个大的候选项集，计算每个候选项的次数，然后基于预先设定的最小支持度阈值，找出频繁一项集集合2、然后基于频繁一项集和原数据集找到频繁二项集3、同样的办法直到生成频繁N项集，其中已不可在生成满足最小支持度的N+1项集，也就是极大频繁项集4、根据Apriori定律1：频繁项集的子集一定是频繁项集，所以到此就找到了所有的频繁项集，然后又可以根据Apriori定律2：非频繁项集的超集一定是非频繁项集来帮助算法构建频繁项集树，加快收敛速度5、然后为每一个频繁项集创建一颗置信树（并不只为极大频繁项集创建置信树，还要为极大频繁项集的所有子集都创建置信树）6、最后可以得出数据之间的关联规则，且该关联规则满足支持度和置信度的阈值\n优点：1、使用先验原理，大大提高了频繁项集逐层产生的效率    2、简单易理解，数据集要求低\n缺点：1、每一步产生的候选项集时循环产生的组合过多，没有排除不该参与组合的元素    2、每次计算项集的支持度时，都需要将数据库中的记录全部扫描一遍，如果是一个大型数据库的话，这种扫描会大大增加计算机I/O的开销，而这种代价是伴随着数据库记录的增加呈几何级增长的，因此人们开始追求更好的算法\n应用：推荐系统：用关联算法做协同过滤，Apriori不适于非重复项集数元素较多的案例，建议分析的商品种类为10类左右。"
    },
    {
        "title": "关联规则-FP-growth算法",
        "content": "定义：该算法建立在Apriori算法概念之上，不同之处是它采用了更高级的数据结构FP-tree减少数据扫描次数，只需要扫描两次数据库，相比于Apriori减少了I/O操作，克服了Apriori算法需要多次扫描数据库的问题\n为了减少I/O次数，FP-growth引入了一些数据结构来临时存储数据，数据结构包括三部分：1、一个项头表里面记录了所有的频繁一项集出现的次数，并且按照次数降序排序  2、FP-Tree将原始数据集映射到了内存中的一棵FP树  3、节点链表，所有项头表的频繁一项集都是一个节点链表的头，它依次指向FP树中的该频繁一项集出现的位置。这样做主要是方便项头表和FP-tree之间的联系查找和更新，也好理解。\n算法流程：1、扫描数据，得到所有频繁一项集的计数，然后删除低于支持度阈值的项集，将频繁一项集放入项头表，并按支持度降序排列。2、扫描数据，将读到的原始数据剔除非频繁一项集，并将每一条再按支持度降序排列  3、读入排序后的数据集，逐条插入FP树，插入时按照排序后的顺序插入FP树中，排序靠前的是祖先节点，靠后的是子孙节点，如果有共用的祖先，则对应的共用祖先节点计数加1，插入后如果有新的节点出现，则项头表对应的节点会通过节点链表连接上新节点，直到所有数据都插入到FP树上，FP树建立完成。  4、从项头表的底部依次向上找到项头表对应的条件模式基。从条件模式基递归挖掘得到项头表项的频繁项集 5、如果不限制频繁项集的项数，则返回步骤4所有的频繁项集，否则只返回满足项数要求的频繁项集\n优点：优点：FP-growth一般快于Apriori，因为只扫描两次数据库\n缺点：缺点：1、FP-growth实现比较困难，在某些数据集上性能可能会下降   2、适用数据类型：离散型数据"
    },
    {
        "title": "集成学习、随机森林",
        "content": "概述：在机器学习中，直接建立一个高性能的分类器是很困难的，但是如果构建一系列性能较差的弱分类器，再将这些弱分类器集成起来，也许就能得到一个性能较高的分类器。通常根据训练集的不同，会训练得到不同的基分类器，这时可以通过训练集的不同来构造不同的基分类器，最终把他们集成起来，形成一个组合分类器。  组合分类器是一个复合模型，由多个基分类器组合而成，基分类器通过投票，组合分类器基于投票的结果进行预测。组合分类器往往比基分类器更加准确，常用的组合方法：装袋、提升、随机森林\n构建分类器的过程一般有两种集成方法1、利用训练集的不同子集训练得到不同的基分类器2、利用同一个训练集的不同属性子集构建不同的基分类器\n随机森林是bagging的一个扩展变体，它是以决策树为基学习器构建bagging的基础上，进一步在决策树的训练过程中引入了随机属性选择。具体步骤：1、从样本集中用自助法选出n个样本组成子集  2、从所有属性n中随机选择K个属性，选择最佳分裂属性(ID3、C4.5、CART)作为节点建立决策树，每棵树都最大程度生长，不进行剪枝 3、重复以上两步m次，建立m棵决策树  4、这m个决策树形成随机森林，在分类问题中通过投票决定输出属于哪一类，在回归问题中输出所有决策树输出的平均值\n优点：1、准确率高。2、各个初级学习器可以并行计算。3、能处理高维持征的数据样本，不需要降维。4、能够评估各个特征在分类问题中的重要性。5、因为采用随机选择特征，对部分特征缺失不数感。6、由于采用有放回抽样，训练出来的模型方差小,泛化能力强\n缺点：1、取值较多的特征会对RF的决策树产生更大影响有可能影响模型的效果。2、bagging保证了预测准确率，但损失了解释率。3、在某些噪音比较大的特征上，RF还是容易陷入过拟合"
    },
    {
        "title": "Bagging",
        "content": "对训练集进行有放回的抽取训练样例，从而为每一个基学习器都构建一个与训练集相当大小但各不相同的训练集，从而训练出不同的基学习器。Bagging是并行计算来训练每个弱学习器，且每个弱学习器的权重相等，且训练集权重也相等\n算法流程:1、从大小为N的原始数据集中独立随机地抽取m个数据(m<=n),形成一个自助数据集 2、重复第一步K次，产生K个独立的自助数据集。3、利用K个数据集训练出K个最优模型(K次可以并行进行) 4、分类问题中的分警结果根据K个模型的结果投票决定，回归问题:对K个模型的值求平均得到结果\n特点：1、通过降低基学习器的方差改善了泛化误差。2、由于每一个样本被选中的概率相同，所以装袋并不侧重于训练数据集中的任何实例，因为对于噪声数据，装袋不太受过分拟合的影响。3、由于是多个决策树组成，所以装袋提升了准确率的同时损失了解释性，哪个变是起到重要作用未知"
    },
    {
        "title": "boosting",
        "content": "主要作用和bagging类似，都是将昔干基分类器整合为一个分类器的方法，boosting是个顺序的过程，每个后续模里都会尝试纠正先前模型的错误，后续的模型依赖之前的模型\n算法步骤:1、首先给每一个训练样例赋予相同的权重 2、然后训练第一个基分类器并用它对训练集进行测试，对于那些分类错误的测试样本提高权重(实际算法中是降低分类正确的样本的权重)3、随后用调整后的带权训练集训练第二个基分类器 4、最后重复这个过程直到最后得到一个足够好的学习器\n提升是一个选代的过程，用于自适应地改变训练样本的分布，使得基分类警聚焦在那些很难分的样本上，不像bagging，提升给每一个训练样本赋予一个权值，而且可以在每一轮提升过程结束时自动地调整权值"
    },
    {
        "title": "adaboost",
        "content": "自适应在于前一个基分类器分错的样本会得到加权，加权后的全体样本再次被用来训练下一个基分类器，采用选代的思想，继承boosting，每次选代只训练一个弱学习器，训练好的弱学习器将参与下一次选代。\n步骤:1、初始化训练数据的权值分布，如果有N个样本，则每一个训练样本最开始时都被赋予相同的权值:1/N 2、通过训练集训练得到弱分类器，将分类正确的样本降低权重，同时提升分类错误样本的权重，将权重更新后的训练集用于训练下一个分类器，如此选代下去 3、将这些得到的弱分类器泪合成强分类器，加大那些分类误差率小的分类器的权重，使其在最终的分类函数中起较大的决定作用，降低分类误差率大的弱分类器的权重，使其在最终的分类函数中起着较小的作用，总之就是误差率低的弱分类器在最终分类器中权重较大，起较大的决定作用，否则较小。\n优点：1、很好地利用了弱分类器进行级联。2、提供的是框架，可以使用各种方法构建弱分类器，很灵活。3、不容易发生过拟合。4、具有很高的精度。5、相对于bagging和随机森林，adaboost充分考虑了每个分类器的权重\n缺点：对异常样本敏感，异常样本可能会在迭代过程中获得较高的权重值，最终影响模型效果。"
    },
    {
        "title": "XGboost",
        "content": "（1）xgboost本质上还是GBDT,但是把速和效率做到了极致，不同于传统的GBDT，只利用了一阶导数信息，xGboost对损失函数做了二阶求导泰勒展开，并在目标函数之外加入了正则项整体求最优解，用以权衡目标函数的下降和模型的复杂程度，避免过拟合，传统GBDTI以CART作为基学习器，XGboost还支持线性分类器，也就是xgboost相当于带L1和L2正则化项的逻辑回归(分类问题)或者线性回归(回归问题)\n（2）表勒展开的一次项和二次项系数不依赖于损失函数，所以xgboost支持自定义损失函数\n（3）XGboost在损失区数里加入正则项，用于控制模型复杂度，正则项降低了模型的复杂度，使学习出来的模型更加简单，防止过拟合，这点优于GBDT\n（4）xGboost还借鉴了随机森林算法，支持列抽样不仅能够降低过拟合还能减少计算，优于GBDT\n（5）Xgboost进行完一次选代后，会将叶子节点的权重乘上一个缩减系数，相当于学习速率\n(XxGboost中的eta)，主要是为了削弱每棵树的影响，让后面有更大的学习空间，实际应用中一般把eta设置的小一点(小学习率可使得后面的学习更加仔细)，选代次数设置的大一点\n（6）应用场景：预测用户行为喜好，商品推荐"
    },
    {
        "title": "梯度提升树（GBDT）",
        "content": "（1）梯度提升决策树，在GBDT中基学习器都是分类回归树，也就是CART，且使用的都是CART中回树。\n（2）对于初级学习器的结果来说一般都是低方差，高混差，因此GBDT的训练过程就是通过降低差来不断提高精度\n（3）GBDT的核心是在于累加所有树的结果作为最终结果，例如对年龄的累加，10岁加5岁，分类树的结果显然是累加不了的，所以GBDT中都是回归树，不是分类树，尽管GBDT调整后也可用于分类，但是不代表GBDT的树是分类树\n（4）GBDT的核心在于每一颗树学习的是之前所有树的结论和残差，这个残差就是一个加预测值后能得到真实值的累加是\n（5）主要使用到的损失函数有:0-1损失函数，对数损失区数，平方损失函数等，目标是:希望损失函数能够不断减小，还有希望损失函数能够尽快减小。\n（6）应用场景：常用于大数据挖掘竟赛；可用于几乎所有的回归问题，也可用于二分类但不适合多分类问题；广告推荐；排序问题"
    },
    {
        "title": "ELT，ETL",
        "content": "ELT，概念：即extract、load 、transform(数据抽取，加载，转换)\n（1）抽取：（1全量抽取）当数据源中有新数据加入或发生数据更新操作时，系统此时采用全量抽取，类似于数据迁移或复制，将数据源中的数据原封不动的从数据库中抽取出来，转换成自己的ETL工具可以识别的格式，一般在系统初始化时使用，全量一次后，就要每天采用增量抽取（2增量抽取）当数据源中有新数据加入或数据更新操作时，系统可以识别出更新的数据，此时采用增量抽取，增量抽取只抽取自上次抽取以来，数据库中新增或修改的数据，在ETL中增量抽取使用更广泛；适合场景：1源数据数据量小2源数据不易发生变化3源数据规律性变化4目标数据量巨大（3更新提醒）当数据源中有新数据加入或数据更新操作时，系统会发出提醒，是最简单的一种抽取方式\n（2）转换：抽取完数据后，要根据具体业务对数据进行转换操作，数据转换一般包括清洗和转换两部分，清洗掉数据集中重复、不完整的数据以及错误的数据\n（3）加载：是将已按照业务需求清洗、转换、整理后的数据加载到各类数据仓库或数据库中，进而进行智能商业分析、数据挖掘等。（1全量加载）全表删除后在进行数据加载（2增量加载）目标表仅更新源表中变化的数据。。。。\nELT优势：1、相对于传统的ETL来说，ELT数据处理管道中无需单独的转换引擎，数据转换和数据消耗在同个地方。2、减少了数据预处理的时间开销，在实际应用中，下游各应用的目的各不相同，同一份数据可能有不同的应用，进而做不同的转换操作，面对这种情况，ETL需要多次对数据进行抽取、转换、加载，而ELT只需要进行一次数据的抽取加载，多次转换，从而实现一份数据的多次应用，大大降低了时间的开销。\n案例：电商用户收集用户信息进行商业分析，有两个核心目标：1、提高用户转化率  2、提高商品的精准营销效率  这两个项目都需要从同一数据源抽取数据，但是项目目标不同，所以需要的细节数据也就不同。\n对于ETL来说，相关人员需要从数据源抽取两次数据（一个项目各需要抽取一次），然后每个项目各需要进行一次ETL的完整流程；对于ELT来说，相关人员只需从数据源抽取一次数据，然后将数据加载到目的地(Hive、Hbase中)，最终转换成我们项目所需要的细节数据即可，无需重复的抽取和加载过程。"
    },
    {
        "title": "正则化。Lasso回归和岭回归",
        "content": "正则化：根据奥卡姆剃刀原理，在所有能解释数据的模型中，越简单的模型越靠谱。但是在实际问题中，为了拟合复杂的数据，不得不采用更复杂的模型，使用更复杂的模型通常会产生过拟合，而正则化就是防止过拟合的工具之一，通过限制参数过大或者过多来避免模型的复杂\nL1（Lasso Regression）、L2正则化（Ridge Regression）的目的都是为了防止过拟合，两者差别在于：岭回归中的L2正则项能够将一些特征变成很小的值，而Lasso回归中的L1正则项得到的特征是稀疏的。Lasso回归会趋向于减少特征的数量，相当于删除特征，类似于降维，而岭回归会把一些特征的权重调小，这些特征都是接近于0的，因此Lasso回归在特征选择的时候非常有用，而岭回归就是一种规则化而已。\n在所有特征中，如果只有少数特征起重要作用的话，选择Lasso回归比较合适，它能自动选择特征。而大部分特征能起到作用而且作用比较平均的话，选择岭回归更合适"
    },
    {
        "title": "spark mllib，rdd",
        "content": "数据对象：1Dataframe、2RDD、3Dataset(具有RDD和dataframe的优点，同时避免他们的缺点)。\n相同点：1、都有惰性机制，在进行转换操作时不会立即执行，只有遇到Active操作时才会执行2、都是spark的核心，只是弹性分布式数据集的不同体现3、都具有分区的概念4、都有相通的算子，比如map、filter等5、都会根据spark的内存情况自动做缓存计算即使数据量很大也不用担心内存的溢出，不用担心OOM-当请求的内存过大时，JVM无法满足而自杀\n不同点：1、RDD以person作为类型参数，但spark并不知道person的内部结构，而Dataframe提供了详细的结构信息，使得sparkSQL可以清楚的知道数据中包含了哪些列，以及列的名称和类型Dataframe提供了数据的结构信息，即schema2、RDD是分布式JAVA的对象集合，Dataframe是row对象的集合3、Dataframe除了提供比RDD更加丰富的算子以外，更重要的是提升执行效率、减少数据读取和执行计划的优化，比如filter下推、裁剪等。\nRDD操作：（优点：类型安全，面向对象；缺点：序列化和反序列化开销大，GC垃圾回收机制的性能开销，频繁的创建和销毁对象，势必增加GC）基本操作主要为Transformation算子（该算子是通过转换从一个或多个RDD生成新的RDD，该操作是惰性的，只有调用action算子时才发起job,典型算子如map、filter、flatMap,distinct等）和Action算子（当代码调用该类型算子的时候，立即启动job，典型算子包括：count、saveasTextfile、takeordered等）\nDataframe操作：（优点：自带schema信息，降低序列化和反序列化的开销；缺点：不是面向对象的，编译期不安全）"
    },
    {
        "title": "决策树",
        "content": "定义：决策树是一种分类算法，它通过对有标签的数据进行学习，学习数据的特征，得到一个模型，什么样的数据就打上什么样的标签\n算法思想：选择属性特征对训练集进行分类，使得各个子数据集有更好的分类，其中的关键点就是寻找分裂规则，因为它们决定了给定节点上的元组如何分裂\n决策树生成步骤：1、根据特征度是选择，从上至下递归地生成子节点，直到数据集不可分则停止决策树生长。2、剪枝:决策树容易过拟合，需要剪枝来缩小树的结构和规模(包括预剪枝和后萝枝)。3、先剪枝:通过提前停止树的构建而对树萝枝。4、后剪枝:由完全生长的树减去子树\n特征度量选择：ID3采用信息增益大的特征作为分裂属性；C4.5采用信息增益率大的属性分裂，同时避免了ID3信息增益偏向取值较多的属性的缺点，但是其实是偏向于取值较少的特征；CART使用基尼指数克服了C4.5计算复杂度的缺点，偏向取值较多的属性\n场景：ID3和C4.5都只能处理分类问题，CART可以用于分类和回归； ID3与C4.5是多叉树，速度慢，CART是二叉树，计算速度快；ID3没有剪枝策略，C4.5有预剪枝和后剪枝，CART采用CCP代价复杂度后剪枝；\n预剪枝：提前设置好树的深度；后剪枝：用完全生长的树减去子树，在测试集上定义损失函数C，目标是通过剪枝使得在测试集上的C值下降，就是说通过剪枝使在测试集上误差率降低\n后剪枝步骤：\n1.自底向上的遍历每一个非叶节点（除了根节点），将当前的非叶节点从树中减去，其下所有的叶节点合并成一个节点，代替原来被剪掉的节点。 \n2. 计算剪去节点前后的损失函数，如果剪去节点之后损失函数变小了，则说明该节点是可以剪去的，并将其剪去；如果发现损失函数并没有减少，说明该节点不可剪去，则将树还原成未剪去之前的状态。 \n3. 重复上述过程，直到所有的非叶节点（除了根节点）都被尝试了。"
    },
    {
        "title": "决策树-id3",
        "content": "核心思想：选择信息增益最大的属性进行分裂，求信息增益时要先求出信息熵和条件熵，信息增益=信息熵-条件熵 ，信息增益：信息的不确定性减少的程度      信息熵：信息的不纯度，信息熵越大，数据分布越混乱，也就是概率分布越均匀，信息熵也越大；\nID3算法建立在奥卡姆剃刀的思想上，越是小型的决策树越优于大的决策树；算法流程：1、初始化属性集合和数据集合  2、计算数据集合的信息熵，和所有属性的条件熵，然后得出信息增益，选择信息增益最大的属性作为当前决策树的分裂节点  3、更新数据集合合属性集合，也就是删除掉上一步使用的属性，并按照属性值来划分不同分支的数据集合   4、依次对每种取值情况下的子集重复第2步  5、若子集只包含单一属性，则为分支叶子节点，根据其属性值标记  6、完成所有属性集合的划分；\nID3优点：1、概念简单，计算复杂度不高，可解释强，易于理解\n2、数据的准备工作简单，能够同时处理数据型和常规型居性\n3、对中间值的缺失不敏感，比较适合处理有缺失属性值的样本，能够处理不相关特征\n4、可以对很多属性的数据集构造决策树，可扩展性强，可用于不熟悉的数据集，并从中提取一些属性规则\nID3缺点：1、没有剪枝策略，可能会产生过度匹配问题，决策树过深，容易导致过拟合，泛化能力差，因此希要简直\n2、信息增益会对取值较多的属性有所好，也就是作为分类属性\n应用场景：适用于特征取值字段不多的数据集，因为信息增益会对取值较多的属性偏好，更容易选择这种属性作为分类属性"
    },
    {
        "title": "决策树-C4.5算法",
        "content": "ID3算法容易选择那些取值较多的特征来划分，因为根据这些属性划分出的数据纯度较高，比如身份证的例子。而且ID3算法属性只能是离散的，当然属性值也可以是连续的数值型，但是需要对这些数据进行数据预处理，变为离散型的才可以用ID3算法，所以C4.5继承了ID3的优点，改进了它的缺点(信息增益会对取值较多的属性有所偏好，也就是作为分类属性)，并在此基础上做了改进的算法，能够处理属性是连续型的\n信息增益率=信息增益/属性a的一个固有值，当属性a\n改进优化的点：1、用信息增益率代替信息增益来选择特征，克眼了用信息增益选择特征时偏向取值较多的属性的不足\n2、能够完成对连续型数值属性的离散化处理\n3、能处理居性值缺失的情况\n4、在决策树构造完成之后剪枝\n"
    },
    {
        "title": "决策树-CART算法",
        "content": "分类回归树，在ID3的基础上，进行优化的决策树，CART既可以是分类树，也可以是回归树，当作为分类树时，采用基尼指数作为节点的分裂依据；当作为回归树时，采用最小方差作为节点的分裂依据。 CART只能用来建立二叉树\nCART在分类问题中采用基尼值来衡量节点的纯度，节点越不纯，基尼值越大，以二分类为例，如果节点的所有数据只有一个类别，则基尼值为0；此外，CART算法采用基于CCP(代价复杂度)的后剪枝方法"
    },
    {
        "title": "算法评估指标",
        "content": "在机器学习算法中，我们可以将算法分为分类算法、回归算法和聚类算法，当我们建立完模型以后，需要用一定的评估指标来判断模型的优劣\n回归算法评估指标（预测连续型数值问题）：最常用的评估指标是 MSE均方误差（mean squared error），均方误差是描述估计量与被估计量之间差异程度的一种评估指标，也就是预测值与实际值误差平方和的均值\n聚类算法评估：聚类的评价方式从大方向上分为两类，一种是分析外部信息，一种是分析内部信息。外部信息就是可看得见的直观信息，比如聚类完成后的类别号。内部信息就是聚类结束后通过一些模型生成这个聚类的相关信息，比如熵值、纯度这种数学评价指标，常见的聚类评估指标有：互信息法、兰德系数、轮毂系数等。\n轮毂系数，适用于实际类别信息未知的情况。对于单个样本来说，设a为其与类别内各样本的平均距离，设b为其与距离最近的类别内的样本的平均距离，轮毂系数公式：(b-a)/max(a,b)\n分类算法评估指标（预测离散型数值问题）：1、错误率，指预测错误的样本数占总样本数的比例，又被叫做汉明损失。表达式：(FP+FR)/P+R。2、精度，指预测正确的样本数占总样本数的比例，又叫做预测准确率。表达式：(TP+TR)/P+R。3、查准率，又叫做精确率，指在预测为正的样本数中真正正例的比例，表示预测是否分类为1中实际为0的误报率，表达式：TP/(TP+FP)。4、查全率，又叫做召回率，指在所有实际正例样本中预测为正的样本数的比例，表示漏掉了一该被分类为1的，却被分为0的漏报成分。5、f1-score，在理想状态中，模型的查准率和查全率越高越好，但是在现实状况下，查准率和查全率会出现一个升高，一个降低的情况。所以就需要一个能够综合考虑查准率和查全率的评估指标，因此引入F值，F值表达式为：(a^2+1)pr/a^2(p+r)，a为权重系数，当a=1时，F值便是F1值，代表查准率和查全率权重相等，是最常用的一种评估指标。表达式为：f1=2pr/p+r。6、PR曲线，是描述查准率和查全率变化的曲线，以查准率和查全率为纵、横坐标轴，根据学习器的预测结果对测试样本进行排序，将最可能是正例的样本排在前面，最不可能是正例的样本排在后面，按照此顺序，依次将每个样本当作正例进行预测，每次计算P值和R值。7、ROC曲线，与PR曲线类似，都是按照排序的顺序将样本当作正例进行预测，不同的是ROC曲线引进TPR、FPR的概念，FPR是假正例率，TPR是真正例率，ROC曲线以TPR(真正例率)为横轴、FPR(假正例率)为纵轴。ROC更加偏重于测试样本评估值的排序好坏。8、AUC面积，进行模型性能比较时，如果模型A的ROC曲线被模型B的ROC曲线完全包住，那么认为B模型的性能更好。若A和B的ROC曲线有交叉的地方，则比较两个模型ROC曲线与坐标轴围成的面积，AUC被定义为ROC曲线下的面积，面积越大,AUC值越大，模型分类的质量就越好。AUC为1时，所有正例排在负例前面，AUC为0时，所有负例排在正例前面\n备注1：查准率是宁愿漏掉，不可错杀。应用场景：一般应用于识别垃圾邮件的场景中。因为我们不希望很多的正常邮件被误杀，这样会造成严重的困扰。因此在这种场景下，查准率是一个很重要的指标\n2：查全率是宁愿错杀，不可漏掉。应用场景：一般用于金融风控领域，我们希望系统能够筛选出所有风险的行为或用户，然后进行人工鉴别，如果漏掉一个可能会造成灾难性后果。\n3：对于类别不平衡问题，ROC曲线的表现会比较稳定（不会受不均衡数据的影响），但如果我们希望看出模型在正类上的表现效果，还是用PR曲线更好\n4：ROC曲线由于兼顾正例与负例，适用于评估分类器的整体性能（通常是计算AUC，表示模型的排序性能）；PR曲线则完全聚焦于正例，因此如果我们主要关心的是正例，那么用PR曲线比较好"
    },
    {
        "title": "最优化方法-无约束-梯度下降",
        "content": "除了目标函数以外，对参与优化的各变量没有其他函数或变量约束，称之为无约束最优化问题；目标函数f(x)，当对其进行最小化时，也把它称作为代价函数、损失函数或误差函数\n求解方法1直接法:通常用于当目标函数表达式十分复杂或者写不出具体表达式时。通过数值计算，经过一系列迭代过程产生点列，在其中搜索最优点\n求解方法2解析法:根据无约束最优化问题的目标函数的解析式给出一种最优解的方法，主要有梯度下降、牛顿法、拟牛顿法、共轭梯度法和共轭方向法等\n概念：1、方向导数:函数沿任意方向的变化率，需要求得某一点在某一方向的导数即方向导数。2、梯度方向:函数在变量空间中的某一点沿着哪个方向有最大的变化率?最大方向导数方向，即梯度方向。3、梯度:函数在某一点的梯度是一个矢量，具有大小和方向。它的方向与取得最大方向导数的方向一致，而它的模为方向导数的最大值。4、正梯度向量指向上坡，负梯度向量指向下坡。我们在负梯度方向上移动可以减小f(x)，被称为最速下降法或梯度下降法\n一、批量梯度下降：每更新一次权重需要对所有的数据样本点进行遍历。在最小化损失函数的过程中，需要不断反复的更新权重使得误差函数减小；特点：每一次的参数更新都用到了所有的训练数据，因此批量梯度下降法会非常耗时，样本数据量越大，训练速度也变得越慢\n二、随机梯度下降：为了解决批量梯度下降训练速度过慢的问题。它是利用随机选取每个样本的损失函数对sita求偏导得到对应的梯度来更新sita；因为随机梯度下降是随机选择一个样本进行迭代更新一次，所以伴随的一个问题是噪音比批量梯度下降的多，使得随机梯度下降并不是每次都向着整体优化的方向\n三、小批量梯度下降：为了解决前两者的缺点，使得算法的训练过程较快，而且也要保证最终参数训练的准确率。它是通过增加每次送代个数来实现的；每次在一批数据上优化参数并不会比单个数据慢太多，但是每次使用一批数据可以大大减小收敛所需的迭代次数，同时可以使得收敛的结果更加接近批量梯度下降的效果"
    },
    {
        "title": "最优化方法-有约束",
        "content": "等式约束最优化：拉格朗日乘子法是解决等式约束最优化的问题的最常用方法，基本思想就是通过引入拉格朗日乘子来将含有n个变量和k个约束条件的约束优化问题转化为含有（n+k）个变量的无约束优化问题\n不等式约束最优化：大部分实际问题的约束都是不超过多少时间，不超过多少人力等等，因此对拉格朗日乘子法进行了扩展，增加了KKT条件，求解不等式约束的优化问题；使用拉格朗日函数对目标函数进行了处理，生成了一个新的目标函数。通过一些条件可以求出最优值的必要条件，这个条件就是KKT条件。经过拉格朗日函数处理之后的新目标函数，保证原目标函数与限制条件有交点，也就是必须要有解"
    },
    {
        "title": "异常值",
        "content": "定义：异常值是偏离整体样本的观察值，也是偏离正常范围的点\n影响：异常值会影响模型的精度，降低模型的准确性。增加了整体数据方差；异常值是随机分布的，可能会改变数据集的正态分布。增加因此异常值处理是数据预处理中重要的一步，异常检测场景：入侵检测、欺诈检测、安全监测等\n出现原因：1、数据输入错误，相关人员故意或者无意导致数据异常，比如客户年收入13万美元，数据登记为130万美元。2、数据测量，实验误差，比如测量仪器不精准导致数据异常。3、数据处理错误，比如ETL操作不当，发送数据异常。4、抽样错误，数据采集时包含了错误数据或无关数据。5、自然异常值，非人为因素导致数据异常，比如今年某月份的降水量远超前几年同月份降水量。\n异常值检测方法：1、散点图：将数据用散点图可视化出来，可以观测到异常值。2、基于分类模型的异常检测：根据现有数据建立模型，然后对新数据进行判断从而确定是否偏离，偏离则为异常值，比如贝叶斯模型，SVM模型等。3、3sita原则：若数据集服从期望为u，方差为σ^2的正态分布，异常值被定义为其值与平均值的偏差超过三倍标准差的值。4、箱型图分析：箱型图分为上界，下界，上四分位数，下四分位数，以及离群点   四分位数就是将所有数值按从小到大排列并分成四等份，处于三个分割点位置的数值就是四分位数。（上界:上限是非异常范围内的最大值；下界:下限是非异常范围内的最小值；上四分位数:数值排序后处于75%位置上的值；下四分位数:数值排序后处于25%位置上的值）\n异常值处理办法：1、删除异常值 -适用于异常值较少的情况。2、将异常值视为缺失值，按照缺失值的处理方法处理异常值。3、估算异常值，均值、中位数、众数填充异常值"
    },
    {
        "title": "训练集，测试集，验证集合",
        "content": "在训练过程中使用验证数据集来评估模型并更新模型超参数，训练结束后使用测试数据集评估训练好的模型的性能\n训练集：是用来构建机器学习模型，用于模型拟合的数据样本；测试集：用来评估训练好的模型的性能；验证集：辅助构建模型，用于构建过程中评估模型，为模型提供无偏估计，进而调整模型超参数和用于对模型的能力进行初步评估。通常用来在模型迭代训练时，用以验证当前模型的泛化能力（准确率，召回率等），以决定是否停止继续训练\n划分方法：（1留出法）直接将数据集划分为互斥的集合，一个用作训练集，一个用作测试集，且满足训练集U测试集=全集；训练集交测试集=空集  常见的划分为2/3-4/5的样本用作模型训练，剩下的用作测试。通常选择70%的数据作为训练集，30%的数据作为测试集；通常单次使用留出法得到的结果不够稳定可靠，一般采取分层抽样或者若干次随机划分作为留出法的评估结果\n（2自助法）给定包含M个样本的数据集，每次随机从中抽取一个样本，放入新的数据集中，然后从原始样本中有放回的抽取M次，就可以得到包含M个样本的数据集，平均情况下会有63.2%的原始样本出现在数据集中，而剩下的36.8%的原始样本不出现在数据集中，也称.632自助法\n（3交叉验证）将数据集划分为K个大小相同的互斥子集，同样保持数据分布的一致性，即采用分层抽样的方法获得这些子集。目的：在实际训练中，模型的结果通常对训练数据好，但是对训练数据以外的数据拟合程度较差，交叉验证的目的就是用作评价模型的泛化能力，从而进行模型选择\nK-折交叉验证：每次用K-1个子集的并集作为训练集，余下的那个子集作为测试集，这样就会产生K种数据集划分的情况，从而可以进行K次训练和测试，最终返回K次测试结果的均值\nK折交叉验证的K最常用的取值是10，此时为10折交叉验证，常用的还有5、20等\n股用于模型调优，找到使得模型泛化性能最优的超参数，在全部训练集上重新训练模型，并使用独立测试集对模型性能做出最终评价\n如果训练集相对较小，则增大K值：增大K的话，在每次选代过程中将会有更多的数据参与模型的训练，能够得到最小偏差，同时算法时间也会变长，且不同训练集间高度相似，会导致结果方差较高\n如果训练集相对较大，则减小K值：减小K的话，降低模型在数据集上重复拟合的时间和成本，在平均性能的基础上获得模型的准确评估"
    },
    {
        "title": "特征选择",
        "content": "为什么要进行特征选择：背景：现实中大数据挖掘任务，往往属性特征过多，而一个普遍存在的事实是，大数据集带来的关键信息之聚集在部分或少数特征上，因此需要从中选择出重要的特征使得后续建模过程只在一部分特征上建立，减少维数灾难出现的可能，同时去除不相关的特征，留下关键因素，降低学习的任务难度，更容易挖掘数据本身带有的规律，同时在特征选择的过程中，会对数据的特征有更充分的理解\n如何进行特征选择：当数据预处理完成后，需要选择有意义的特征进行模型训练，通常从三方面考虑：1、特征是否发散:如果一个特征不发散，例如方差接近于0，也就是说样本在这个特征上基本没有差异，这个特征对样本的区分作用不明显，区分度不高。2、待征之间的相关性:特征与待征之间的线性相关性，去除相关性高的特征。3、特征与目标之间的相关性:与目标相关性高的特征，应当优先选择\n三种常见特征选择方法对比：1、过去由于和特定的学习器无关，计算开销小，泛化能力强于后两种特征选择方法，因此，在实际应用中由于数据集很大，特征维度高，过港式特征选择应用的更广泛些，但它是通过一些统计指标对特征进行排字来选择，通常不能发现冗余。2、从模型性能的角度出发，包装法的性能要优于过考去，但时间开销较大。3、嵌入法中的L正则化方法对于特征理解和特征选择来说是非常强大的工具，它能够生成稀疏的模型，对于选择待征子集来说非常有用。4、嵌入法中的随机森林方法是当前比较主流的方法之一，它易于使用，一般不需要其他特征工程操作、调参等繁琐的步亲，有直接的工具包都提供平均不纯堂下降方法，它的两个主要问题：一是重要的特征有可能得分很低，二是这种方法对特征变量类别多的特征有利。"
    },
    {
        "title": "特征选择-Filter过滤法",
        "content": "按照特征发散性或者相关性对各个特征评分，设定阈值或待选择阈值的个数，选择特征；总的缺点是：若特征之间具有强关联，且非线性时，Filter方法不能避免选择的最优特征组合冗余。   Filter总结：利用不同的打分规则，对每个特征进行打分，相当于给每个特征赋予权重，按权重排序，对不达标的特征进行过滤\n1、方差选择法：方差越大的特征，对于分析目标影响越大，就越有用；如果方差较小，比如小于一，那么这个特征可能对算法的作用就比较小；如果某个特征方差为0，即所有的样本在这个特征上的取值都是一样的，那么对模型训练没有任何作用，可以直接舍弃（特征的方差越大越好）    ；实现方法：设定一个方差的阈值，当方差小于这个阈值的特征就会被删除；适用场景：只适用于连续变量。\n2、相关系数法（皮尔逊相关系数）：该方法衡量的是变量之间的线性相关性，两个变量之间的皮尔逊相关系数定义为两个变量之间的协方差与标准差的商，结果的取值区间为【-1，1】，-1表示完全的负相关，+1表示完全正相关，0表示没有线性相关    实现方法：R为相关性，P为显著性，首先看P值，P值用于判断R值，即相关系数有没有统计学意义，判断标准一般为0.05，当P值>0.05时，则相关性系数没有统计学意义，此时无论R值大小，都表明两者之间没有相关性；当P值<0.05，则表明两者之间有相关性，此时再看R值，R值越大相关性越大，正数则正相关，负数就是负相关   ；适用场景：适用于特征类型均为数值特征的情况；缺陷：只对线性关系敏感，如果特征与响应变量的关系是非线性的，即便两个变量具有一 一对应的关系，相关系数也可能会接近0。建议最好把数据可视化出来，以免得出错误的结论。\n3、卡方检验(𝜒^2检验)：它可以检验某个特征分布和输出值分布之间的相关性，𝝌^𝟐值描述了自变量与因变量之间的相关程度，𝜒^2值衡量实际值与理论值的差异程度，𝜒^2值越大，相关程度也就越大   实现方法：1、计算无关性假设(随机抽取一条实际值计算)   2、根据无关性假设生成新的理论值四格表   3、根据计算公式算出𝜒^2   4、计算该相依表的自由度，查询卡方分布的临界值表来判断𝜒^2值是否合理(需要用100%-卡方分布临界表的值才能得出相关性)\n4、互信息法：互信息表示两个变量是否有关系以及关系的强弱。互信息可以理解为，X引入导致Y的熵减小的量，从信息熵的角度分析特征和输出值之间的关系评分；互信息值越大，说明该特征和输出值之间的相关性越大，越需要保留；缺陷：它不属于度量方式，也没有办法归一化，在不同数据集上的结果无法做比较    2、对于连续变量通常需要先离散化，而互信息的结果对离散化的方式敏感\n"
    },
    {
        "title": "特征选择-wrapper包装法",
        "content": "根据目标函数(通常是预测效果评分)，每次选择若干特征，或者排除若干特征\n常用技术 -- 递归特征消除法RFE；\n使用一个基模型来进行多轮训练，每轮训练后，移除若干权值系数的特征，在基于新的特征集进行下一轮训练。  步骤：1、指定一个有N个特征的数据集 2、选择一个算法模型  3、指定保留特征的数量K(K<N)  4、第一轮对所有特征进行训练，算法会根据基模型的目标函数给出每个特征的评分或排名，将最小得分或排名的特征移除，这时候特征减少为N-1，对其进行第二轮训练，持续迭代，直到特征保留为K,这K个特征就是选择的特征\nRFE的稳定性很大程度上取决于在迭代时底层所采用的模型。例如，假如RFE采用的普通的回归，没有经过正则化的回归是不稳定的，那么RFE就是不稳定的；假如采用的是Ridge，而用Ridge正则化的回归是稳定的，那么RFE就是稳定的。"
    },
    {
        "title": "特征选择-Embedded嵌入法",
        "content": "先使用某些机器学习的算法或模型进行训练，得到各个特征的权值系数，根据系数从大到小选择特征\n（1基于L1正则化方法）：1、L1正则化将回归系数的L范数作为惩罚项加到损失函数上，由于正则化非0，这就使得那些弱的待征所对应的系数变为0，因此比L1正则化往往会使学习到的模型很保统(系数w经常为0)这个特性使得L1正则化成为一种很好的特征选择的方法。2、L1正则化像非正则化线性模型一样也是不稳走的，如果待征集合中具有相关联的持征，当数据发生轻微变化时，也有可能导致很大的差异。3、L1正则化能够生成稀疏的模型。4、L1正则化可以产生稀疏权值短阵，即产生一个稀疏模型，可以用于特征选择，也可以防止过拟合\n数据稀疏性说明:L1正则化本来就是为了降低过拟合风险，但是L1正则化的结果往往会得到稀疏数据，即L1方法选择后的数据拥有更多0分量，所以被当作特征选择的强大方法。稀疏数据对后续建模的好处:数据集表示的矩阵中有很多列与当前任务无关，通过特征选择去除这些列，如果数据備疏性比较突出，意味着去除了较多的无关列，模型训练过程实际上可以在较小的列上进行，降低学习任务的难度，计算和存储开销小，\n（2基于树模型方法）：基于树的预测模型能够用来计算特征的重要程度，因此能用来去除不相关的特征，随机森林具有准确率高，稳定性强、易于使用的优点，是目前最流行的机器学习算法之一，随机森林提供的特征选择方法：平均不纯度减少和平均精度下降。（一）、平均不纯度减少：1、随机森林由多个决策树构成，决策树的每一个节点都是关于某个特征的条件，目的时将数据集按照不同的取值一分为二。对于一个决策树森林来说，可以算出每个特征平均减少了多少不纯度，并把它平均减少的不纯度作为特征选择的值  2、若特征之间存在关联，一旦某个特征被选择后，其他特征的重要度就会急剧下降，而不纯度已经被选中的那个特征降下来，其他特征就很难在降低那么多不纯度。在理解数据时，容易错误的认为先被选中的特征是很重要的，而其余的特征是不重要的，但实际上这些特征对响应变量的作用可能非常接近；。（二）、平均精度下降：该方法直接度量每个特征对模型的精确率的影响，基本思路是打乱每个特征的特征值顺序，并且度量顺序变动对模型精确率的影响，因为对于不重要的变量，打乱其顺序对模型的准确率影响不会太大，对于重要的变量，打乱顺序就会降低模型的准确率；在特征选择上，需要注意：尽管在所有特征上进行了训练得到模型，然后才得到每个特征的重要性测试，这并不等于筛选掉某个或某几个重要特征后模型的性能就一定会下降很多，因为即便删掉某个特征后，其关联特征一样可以发挥作用，让模型性能不变"
    },
    {
        "title": "特征处理-特征缩放(数值归一化)",
        "content": "特征缩放可以提高模型的精度和模型的收敛速度，在实际业务中，当数据的量纲不同、数量级差距大时，会影响最终的模型，因此需要用到特征缩放\n1、标准化（常用）：概念:将训练集中的某一列特征缩放到均值为0，方差为1的状态；特点:标准化对数据进行规范化，去除数据的单位限制，将数据转换为无量纲的纯数值，便于不用单位的数据进行比较和加权，同时不改变数据的原始分布状态，标准化要求原始数据近似满足高斯分布，数据越接近高斯分布，标准化效果越佳；适用场景:数据存在异常值和较多的噪音值\n2、最小值-最大值归一化：概念:将训练集中某一列特征数值缩放到-1到1，或0到1之间；特点:受训练集中最大值和最小值影响大，存在数据集中最大值与最小值动态变化的可能；适用场景:数据较为稳定，不存在极端的最大值和最小值\n3、均值归一化：公式：(x-x均值)/max(x)-min(x)\n4、缩放成单位向量：公式：x/||x||\n5、基于树的方法是不需要特征归一化或标准化，比如随机森林，bagging 和 boosting等。基于参数的模型或基于距离的模型，需要特征归一化或标准化。\n"
    },
    {
        "title": "特征处理-数值离散化",
        "content": "概念:把无限空间中的有限个体映射到有限空间中去，提高算法的时空效率，简单讲就是在不改变数据相对大小的情况下，对数据进行相应缩小;离散化仅适用于只关注元素之间的大小关系而不关注元素数值本身的情况 在数据挖掘理论研究中，研究表明离散化数值也能在提高建模速度和提高模型精度上有显著作用；\n作用:离散化可以降低特征中的噪音节点，提升特征的表达能力但是离散化的过程都会带来一定的信息丢失\n（一）、数值变量分离散变量和连续变量\n连续变量中的有监督连续变量离散方法包括1R（把连续的区间分成小区间，然后根据类标签对区间内变量调整）、基于信息熵（自顶向下的方法，运用决策树理念进行离散化）、基于卡方（自底向上的方法，运用卡方检验的方法，自底向上合并数值进行有监督离散化，核心操作是merge）；\n连续变量中的无监督连续变量离散方法包括聚类划分（使用聚类算法将数据分为K类，需要制定K值的大小）；分箱-数据先排序（1等宽划分-把连续变量按照相同的区间间隔划分成几等份，也就是根据数值的最大值和最小值进行划分，分为N份，每份的数值间隔相同；2等频划分-把连续变量划分成几等份，保证每份数值个数相同）\n（二）、分类变量分有序分类变量和无序分类变量\n有序分类变量离散化方法包括Label-Encoding（有序分类变量数值之间存在一定的顺序关系，可直接使用划分后的数据进行数据建模。优点:解决了分类变量的编码问题。缺点:可解释性差）\n无序分类变量离散化方法包括独热编码（使用M位状态寄存器对M个状态进行编码，每个状态都有独立的寄存器位，这些特征互斥，所以在任意时候只有一位有效，也就是说这M位状态中只有一个状态位值为1，其他都是0，换句话说就是M个变量用M维表示，每个维度的数值为1或为0。；优点:解决了分类器不好处理分类变量的问题。；缺点:分类变量不宜过多，可能会造成稀疏矩阵）哑编码（哑编码和独热编码类似，唯一区别就是哑编码采用M-1位状态寄存器对M个状态进行编码。；优点:解决了分类器不好处理分类变量的问题。；缺点:分类变量不宜过多，可能会造成稀疏矩阵）"
    },
    {
        "title": "特征处理-特征编码",
        "content": "数据挖掘中，一些算法可以直接计算分类变量，比如决策树模型，但许多机器学习算法不能直接处理分类变量，他们的输入和输出都是数值型数据，因此把分类变量转换成数值型数据是必要的，可用独热编码和哑编码实现\n应用场景1、对逻辑回归中的连续变量做离散化处理，然后对离散特征进行独热编码或者哑编码，这样会使模型具有较强的非线性能力\n应用场景2、对于不能处理分类变量的模型，必须要先使用独热编码或哑编码，将变量转换成数值型；但若模型可以处理分类变量，那就无须转换数据，如树模型"
    },
    {
        "title": "用户画像",
        "content": "用户画像：即用户信息标签化，就是收集这个用户的各种行为和数据，从而分析得到这个用户的一些基本的信息和典型特征，最后形成一个人物原型，从中挖掘用户价值，从而提供业务推荐、精准营销等服务\n用户画像构建流程：1、收集用户相关数据：可以来源于网站上用户交易数据，用户日志数据，用户行为数据等；。\n2、数据预处理：对收集到的用户数据做一些预处理工作，包括检查数据是否完整，即数据是否含有缺失值，数据是否含有异常值以及数据是否存在不均衡现象，从而决定是否对缺失值填充，或者删除异常值，对不均衡数据进行重采样等数据清洗工作。最终将杂乱无章的数据转化为结构化数据，即我们后续机器学习模型可以识别的数据格式。；。\n3、用户行为数据建模：对于不携带标签的用户数据，也就是我们事先并不知道该用户属于哪一类别，或者说他和哪些用户行为数据比较相似，此时我们可以采用无监督学习中的聚类算法来对它们进行聚类，相似相近、关联性大的用户数据放在一起，不相似不相近，关联性不大的用户不放在一起，可以用这些聚类算法，针对不同形状的数据进行相应的聚类，此时聚类结束后，我们可以用相应的聚类评价指标来评价我们聚类模型性能的优劣，或者将聚类结果与专家给出的标准做对比，将聚类结果回归到真实商业逻辑上。对聚类后不同类别的用户进行打标签，从而得到带有标签的用户数据，对于携带标签的数据，我们接着选择机器学习中有监督学习的相关算法来对这些有标签的数据进行统计分析。\n4、举例：例如某银行推行一种信用卡，该银行的目标就是想知道哪些客户群体会办理信用卡，哪些客户群体不会办理信用卡，所以我们可以通过收集关于用户办理信用卡相关的数据，比如年龄、性别、婚姻状况、名下是否有房产、月收入年收入、之前是否办理过信用卡等等信息，通过聚类算法将用户分为三类群体，即会办理、不会办理以及未知三种客户群体，银行可据此向会办理信用卡的客户群体推送办卡相关业务以及活动福利，实现业务推荐和精准营销。最终我们可以通过现有的携带标签的用户数据，构建机器学习分类模型，例如LR、SVM等算法模型，然后通过该模型对未知用户进行预测，预测该用户是否会办理信用卡"
    },
    {
        "title": "无监督-聚类",
        "content": "无监督：是指在未加入标签的数据中，根据数据之间的属性特征和关联性对数据进行区分，相似相近、关联性大的数据放在一起，不相似不相近的、关联性不大的数据不放在一起；无监督本质：利用无标签的数据去学习数据的分布和数据与数据之间的关系；无监督算法包括如聚类算法、关联算法。\n分类：基于原型（K-means算法、K-means++算法、k-mediods算法）基于层次（HierarchicalClustring算法、Birch算法）基于密度（DBSCAN算法）\n（1）、K-means算法：思想：输入聚类个数K，已经包含n个数据样本的数据集，输出标准的K个聚类的算法，然后将n个数据样本划分为K个聚类，最终结果所满足：同一聚类中数据相似度较高，而不同聚类的数据相似度低；步骤：1、随机选取K个对象作为初始质心  2、计算样本到K个质心的欧氏距离，按就近原则将它们划分到距离最近的质心所对应的类中   3、计算各类别中所有样本对应的均值，将均值作为新的质心，计算目标函数  4、判断聚类中心或目标函数是否改变，若不变则输出，若改变，则返回2；；；；；\nK-means算法核心问题：K值如何选取：1、人工指定：多次选取K值，选择聚类效果最好的K值。2、均方根：假设我们有m个样本，该方法认为K=根号下m/2。3、枚举法：计算类内距离均值与类间距离均值之比，选择最小的K值，将所有K值进行二次聚类，选择两次聚类结果最相似的K值。4、手肘法：随着K值得增大，样本划分越来越精细，聚类得程度越来越高，误差平方和SSE便逐渐减小(误差平方和是所有样本的聚类误差，代表聚类效果的好坏，SSE越小越好)，当K小于真实聚类数时，随着K值的增大会大幅增加每个簇的聚合程度，SSE的下降幅度也会很大，当K等于真实聚类数时，随着K值的继续增大，聚合程度也会迅速减小，SSE的下降幅度便会骤减，然后随着K的继续增大而趋于平缓，所以K值和SSE的关系图就像一个手肘的图形，肘部对应的值便是数据真实聚类的个数\n（2）、K-means++：背景：为了解决K-means初始质心敏感的问题。；技术原理：不同于K-means算法是第一次随机选取K个样本作为初始质心，K-menas++是假设已经选取了p个初始质心，只有在选取第p+1个质心时，距离这p个质心越远的点会有更高的概率当选为第p+1个聚类中心(为了避免异常点的存在，第二个点的选择会从距离较远的几个点中通过加权选取第二个点)，只有在选取第一个聚类中心时是随机选取(p=1)，该方法的改进符合一般直觉：聚类中心之间距离的越远越好\n（3）、K-mediods：能够避免数据中异常值的影响；算法步骤：1、随机选取一组样本点作为中心点集  2、每个中心点对应一个簇 3、计算各样本点到各个中心点的距离，将样本点放入距离最近的中心点的类中。  4、计算各簇距簇内各样本点的距离绝对误差最小的点，作为新的聚类中心    5、如果新的聚类中心与原中心点相同，则过程结束，如果不同，则返回2\nK-mediods与k-means算法对比：1、算法流程基本一致。；2、质心的计算方式不同:k-means算法是将所有样本点对应的均值作为新的中心点，可能是样本中不存在的点 K-mediods是计算簇内每一个点到簇内其他点的距离之和，将绝对误差最小的点作为新的聚类中心，质心必须是某个样本点的值。；3、k-mediods可以避免数据中异常值带来的影响。；4、质心的计算复杂度更高:k-means直接将均值点作为新的聚类中心，而k-mediods需要计算簇内任意两点之间的距离，在对每个距离进行比较狭取新的质心，计算复杂度增加，速度变慢。；5、稳定性高，执行速度变慢:在具有异常值的小样本数据集中，k-mediods算法比k-means算法效果好，但是随着数据集规模的增加，k-mediods算法执行的速度会很慢，所以如果数据集本身不存在很多的异常值的话，就不用k-mediods代者k-means。；\n（4）、Hierarchical Clustering算法：思想:确保距离近的样本落在同一个族中\n步骤:1、每个样本点都作为一个簇，形成族的集合C 。2、将距离最近的两个簇合并，形成一个簇3、从C中去除这对簇。 4、最终形成层次树形的聚类结构树形图（判断两个簇之间的距离方法：1单链接 -- 不同两个簇之间最近的两个点的距离；2全链接-- 不同两个簇之间最远的两个点的距离；3均链接 -- 不同两个簇中所有点两两之间的平均距离）\n优点:可排除噪声点的干扰，但有可能和噪声点分为一簇 2、适合形状不规则，不要求聚类完全的情况 3、不必确定K值，可根聚类结果不同有不同的结果 4、原埋简单，易于理解\n缺点:计算量很大，耗费的存储空间相对于其他几种方法要高。 2、合并操作不能撤销 3、合并操作必须有一个合并限制比例，否则可能发生过度合并导致所有分类中心聚集，造成聚类失败\n适用场景:适合形状不规则，不要求聚类完全的情况\n（5）、Birch：使用聚类特征三元组表示一个簇的有关信息，而不用且体的一组点来表示该簇，通过构造满足分支因子和簇直径限制的聚类特征树来进行聚类三元组(数据点样本个数，数据点样本特征之和，数据点样本特征平方和)，分支因子:树的每个节点的样本个数，簇直径:一类点的距离范国\n算法步骤:1、扫描数据，建立聚类特征树 2.使用某种算法对聚类特征树的叶节点进行聚类\n优点:一次扫描就能进行很好的聚类\n缺点:要求是球形聚类，因为CF树存储的都是半径类的数据，都是球形才适合\n适用场景:因为Birch算法通过一次扫描就可以进行比较好的聚类，所以适用于大数据集，而且数据的分布呈凸型以及球形的情况，并且由于Birch算法需要提供正确的器类个数和簇直径限制，对不可视的高维数据不可行\n（6）、DBSCAN：一个聚类可以由其中任何核心对象唯一确定，该算法利用基于密度的概念，要求聚类空间中某一区域内的样本个数不小于某一给定闻值，该方法能够在具有噪声的空间数据库中发现任意形状的簇，可将密度足够大的相邻区域连接，能够有效处理异常数据，主要用于对空间教据的聚类。\n步骤:1、DBSCAN通过检查数据中每个样本的eps邻域来搜索簇，如果点p的eps邻域内包含的样本个数大于给定的闽值，那么就建立一个以点P为核心对象的簇。\n2、然后DBSCAN送代的聚集这些核心对象直接密度可达的对象，这个过程可能还会涉及密度可达簇的合并\n3、当没有新的样本点加入到簇中时，过程结束"
    },
    {
        "title": "数据属性",
        "content": "标称属性：标称，意味着与名称有关，标称属性的值是一些符号或事物的名称，每个值代表事物的某种类别、状态或编码。因此标称属性又被看做是分类的。标称属性的值不具有有意义的顺序，而且是不定量的。也就是说给定一个数据集，找出这种属性的均值没有意义;举例：头发颜色和婚姻状况，头发的颜色={黑色、棕色、红色、白色}；婚姻状况={单身、已婚、离异、丧偶}；其次还有职业，具有教师、程序员、农民等\n二元属性:二元属性是一种标称属性，只有两种状态，用来描述事物的两种状态，用值0或1来体现，0表示该属性不出现，1表示出现。二元属性又称布尔属性，如果两种状态对应的是True和False;对称的二元属性(两种状态具有相等的价值，携带相同的权重，例如性别，具有男女两种状态)非对称的二元属性(其状态的结果不是同等重要的，例如艾滋病化验结果的阳性和阴性，用1对最重要的结果(HIV阳性)编码，而另一个用0编码(HIV阴性))\n序数属性:其可能的值之间具有有意义的序或秩评定，但是相继值之间的差是未知的，也就是对应的值有先后顺序。序数属性可以把数量值的值域划分成有限个有序类别。（如0-很不满意，1-不满意，2-满意，3-很满意），把数值属性离散化得到。可以用众数和中位数表示序数属性的中心趋势，但不能定义均值.;举例：成绩={优、良、中、差}；    drink_size表示饮料杯的大小：大、小、中。\n数值属性:是定量的，即它是可以度量的量，用整数或实数值表示;(1)区间标度属性:用相等的单位尺度度量，区间标度属性的值有序，可以评估值之间的差，但不能评估倍数，没有固有的零点(也就是说0°C并不是指现在没有温度)。举例：摄氏温度、华氏温度，日历日期。不能说2020年是1010年的两倍，两者的差有意义，但是比值没有意义；(2)比率标度属性:具有固有零点的数值属性。比率标度属性的值有序，可以评估值之间的差，也可以评估倍数.举例：开氏温度、重量、高度、速度、货币量。拿货币来说，可以说200元比100元多100元，也可以说200元是100元的两倍\n离散属性：具有有限或无限可数个值，可以用或不用整数表示。例如，属性customer_ID是无限可数的。顾客数量是无限增长的，但事实上实际的值集合是可数的（可以建立这些值与整数集合的一一对应）举例：邮编、省份数目\n连续属性:如果属性不是离散的，则它是连续的。属性值为实数，一般用浮点变量表示。\n总结：（1）标称、二元、序数属性都是定性的，他们只描述对象的特征，而不给出实际大小和数值；（2）标称、二元属性的中心趋势可以用众数度量。序数属性的中心趋势可以用它的众数和中位数度量，但不能定义均值（3）所有属性都能用中心趋势来表述。标称、二元属性用众数度量；序数属性用众数、中位数度量。均值是数值属性的中心趋势描述"
    },
    {
        "title": "降维-PCA，奇异值、主成份分析",
        "content": "思想：降维就是将事物的特征进行压缩和筛选，将原始高维空间中的数据映射到低维空间中，该项任务比较抽象，如果没有特定领域的知识，很难事先决定采用哪些数据。比如在人脸识别任务中，如果直接采用图片的原始像素信息，那么数据的维度是非常大的，所以此时就要用到降维的方法，对图片信息进行处理，选出区分度最大的像素组合\n1、SVD（奇异值分解）：1、奇异值分解可以适用于任意矩阵的一种分解方法 2、奇异值分解可以发现数据中隐藏的特征来建立矩阵行列之间的关系 3、奇异值分解能够发现矩阵中的几余，并提供用于消除它的格式\n优点:原理简单，仅涉及简单的矩阵线性变换知识。可以有效处理数据噪音，矩阵处理过程中得到的三个短阵也具有物理意义\n缺点:分解出的矩阵可解释性差，计算量大\n应用场景:广泛的用来求解线性最小平方、最小二乘问题，低秩通近问题，数据压缩问题，应用于推荐系统(找到用户没有评分的物品，经过SVD压缩后低维空间中，计算未评分物品与其他物品的相似性，得到一个预测打分，再对这些物品评分从高到低排序，返回前N个物品推荐给用户)\n2、PCA（主成份分析法）：思想:寻找表示数据分布的最优子空间(降维，去掉线性相关性)，就是将n维特征映射到k维上(k<n)，k维是全新的正交特征(k维特征称为主成份，是重新构造出来的k维特征，而不是简单的从n维特征中去除n-k维特征)，PCA目的是在高维数据中寻找最大方差的方向，然后将原始数据映射到维数小的新空间中。数学原理:根据协方差矩阵选取前s个最大特征值所对应的特征向量构建映射矩阵，进行降维\n优点:1、方差衡量的无监督学习，不受样本标签的限制 2、各主成份之间正交，可消除原始数据中各特征之间的影响 3、计算方法简单，主要运算是奇异值分解，容易在计算机上实现\n缺点:主成份解释某含义具有一定的模糊性，不如原始样本特征解释性强 2、方差小的非主成份也可能含有重要信息，因降维丢弃后可能对后续数据处理产生影响\n算法流程：1、对所有样本构成的矩阵X去中心化2、求X的协方差矩阵C   3、利用特征值分解，求出协方差矩阵C的特征值和特征向量      4、取前s个最大特征值对应的特征向量构成变换矩阵W    5、用原数据集和变换矩阵W相乘，得到降维后的新数据集（矩阵）\n追问:详细介绍PCA：在PCA中，数据从原始坐标系转换到新的坐标系中，转换坐标系时，选择数据方差最大的方向作为坐标轴的方向，（方差最大的方向给出了数据最重要的信息），第一个坐标轴的方向是方差最大的方向，第二个新坐标轴的方向是与第一个新坐标轴正交且方差次大的方向，重复过程N次，N是数据原始维度，通过这种方式可以获得新的坐标系，其中大部分方差包含在前几个坐标轴中，而后几个坐标轴中方差基本为0，这时可以忽略后面的坐标轴，只保留前面几个包含大部分方差的坐标轴；通过计算协方差矩阵，可以得到协方差矩阵的特征值和特征向量，选取前几个最大特征值所对应的特征向量组成变换矩阵，就可以将矩阵转换到新的坐标系中，实现数据的降维\n3、LDA（线性判别分析）：思想:寻找可分性判据最大的子空间，即投影后类内间距最小，类间距离最大 2、用到了fisher的思想，即寻找一个向量，可以使得类内散度最小，类间散度最大，其实也就是选取特征向量构建映射矩阵，然后对数据进行处理，该方法能使投影后样本的类间散步矩阵最大，类内散步矩阵最小\n优点:降维过程中可以使用类别的先验经验 2、LDA在样本分类信息依赖均值而不是方差的时候，比PCA之类的算法更优\n缺点:LDA不适合对非高斯分布的样本进行降维，PCA也有这个问题 2、LDA在样本分类信息依赖方差而不是均值的时候，降维效果不好 3、LDA可能过度拟合数据 4、LDA降维最多降到类别数k-1的维数，如果我们降维的维度大于k-1，则不能使用LDA\n4、LLE（局部线性嵌入）：相比较于传统的PCA、LDA这种只关注样本方差的降维方法，LLE在保持降维效果的同时也保留了样本局部的线性特征，因为其保留了局部的线性特征，所以常被应用在高维数据可视化、图像识别等领域\n5、LDA VS PCA：共同点：1、都属于线性方法。2、在降维时都采用矩阵分解的方法。3、假设数据符合正态分布。；不同点：1、LDA是有监督的，不能保证投影到新空间的坐标系是正交的，选择分类性能最好的投影方向。2、LDA可以用于降维也可以用于分类。3、LDA降维保留个数与其对应的类别数有关，与数据本身维度无关\n奇异值分解定义：将矩阵分解为奇异向量和奇异值，可以将矩阵A分解为三个矩阵的乘积，即A=UDV^t，其中矩阵U和V为正交矩阵，U的列向量为左奇异向量，V的列向量为右奇异向量，D矩阵的对角线上的值为矩阵A的奇异值，D矩阵为对角矩阵，奇异值按大小进行排序，对角线之外的值为0\n奇异值分解应用：1、在机器学习和数据挖掘领域，奇异值的应用很广泛，比如应用在用于降维的PCA(主成份分析法)和LDA(线性判别分析法)，数据压缩（以图像压缩为代表）算法，还有做搜索引擎语义层次检索的LSI。；2、求矩阵伪逆：奇异值分解可以被应用于求矩阵的伪逆，若矩阵M的奇异值分解为M=UDV^t，那么M的伪逆为M=VD^+U^t，其中D+为D的伪逆，并将其主对角线上非零元素求倒数在转置得到的。求伪逆通常可以用来求解线性最小平方、最小二乘法问题。；3、矩阵近似值：奇异值分解在统计中的主要应用为PCA（主成分分析），一种数据分析的方法，用来发现大量数据中的隐含模式，可以用于模式识别，数据压缩等方面.PCA方法的作用是将数据集映射到低维空间中的坐标系中，数据集的特征值(可以用奇异值来表征)按照重要性排列，降维的过程其实就是舍弃不重要的特征向量，而剩下的特征向量组成的空间就是降维后的空间。；4、平行奇异值：用于频率选择性的衰落信道的分解"
    },
    {
        "title": "数据挖掘流程",
        "content": "数据挖掘是通过对大量的数据进行分析，以发现和提取隐含在其中的具有价值的信息和知识的过程。跨行业数据挖掘标准流程，是当今数据挖掘业界通用流行的标准之一，它强调数据挖掘技术在商业中的应用，是用以管理并指导Data Miner有效、准确开展数据挖掘工作获得最佳挖掘成果的一系列工作步骤的规范标准\n1、商业理解：从商业角度理解项目的目标和要求，然后把理解转化为数据挖问题的定义和一个旨在实现目标的初步计划。确定业务目标：分析项目背录，从业务角度分析项目的需求和目标，确定业务角度成功标准。；项目可行性分析：分析现有资源、条件和限制，风险估计、成本和效益估计。；确定数据控掘目标：明确数据挖狂的目标和成功标隹。；提出项目计划：对整个项目做出计划，初步估计用到的工具和技术。\n2、数据理解：开始于原始数据的收集，然后是熟悉数据，表明数据质量问题，探索数据进而对数据初步理解，发掘有趣的子集以形成对隐藏信息的假设。收集原始数据：收集项目所涉及到的数据，如有必要，将数据传入数据处理工具中，并做一些初步数据集成的工作，生成相应报告。；描述数据：对数据做一些大致描述，例如属性数、记录数等，生成报告。；探索数据：对数据做简单的统计分析，例如关键属性的分布。；检查数据质量：包括数据是否完整、是否有错误，是否会有缺失值等\n3、数据准备：从原始未加工的数据构造最终数据集的过程(这些数据指将要嵌入建模工具中的数据)，数据准备任务可能要实施多次，并且没有任何规定的顺序。这些任务包括表格、属性和记录的选择以及按照建模工具要求，对数据进行转换、清洗等。；数据选择：根据数据挖掘目标和数据质量选择合适数据，包括表格选择、属性选择和记是选择。；数据清洁：对选择出的教据进行清洁，提高数据质量，例去除噪音、填充缺失值等。；数据创建：在原始数据上生成新的记录或属性。；数据合并：利用表连接等方式将几个数据集合并在一起。；教据格式化：将数据转化为数据挖掘处理的格式。\n4、建立模型：选择和应用各种建模技术，同时对他们的参数进行调整，以达到最优值。选择建模技术 -- 确定数据挖症算法模型和参数；测试方案设计：设计某测试模型的质是和有效性机制模型训练：在准备好的数据集上运行数据挖掘算法，得出一个或多个模型模型测试评估：根据测试方案，从数据挖狂技术角度确定数据挖掘目标是否成功\n5、模型评估：更为彻底的评估模型和检查建立模型的各个步骤，从而确保它真正的达到了商业目标。结果评估：从商业角度评估得到的模型，甚至实际试用该模型测试其效果；过程回顾：回顾项目的所有流程，确定每一个阶段都没有错误；确定下一步工作：根据结果评估和过程回顾得出结论，确定部晋该控掘模型还是从某个阶段重新开始\n6、模型实施：实施计划 ：在业务运作中部署模型做出计划；监控和维护计划：如何监控模型在实际业务中的使用情况，如何维护该模型；作出最终报告 ：项目总结，项目经验和项目结果；项目回顾：回顾项目的实施过程，总结经验教训;对数据挖握的运行效果做一个预测"
    },
    {
        "title": "数据采集抽样方法",
        "content": "简单随机抽样：先将调查总体进行编号，然后根据抽签法或者随机数字表抽取部分所观察到的数据组成样本数据，分为有放回抽样和无放回抽样。常被应用于压缩数据量以减少费用和时间开销\n系统抽样：又被称为等距抽样，首先设定抽样间距为n，然后从前n个数据样本中抽取初始数据，然后按照顺序每隔n个单位抽取一个数据组成样本数据\n分层抽样：将总体数据按照特征划分为若干层次或类型，然后从各个类型和层次的数据中采用简单随机抽样或者系统抽样抽取子样本，最终将这些子样本组合为总体数据样本，常被应用于离网预警模型和金融欺诈模型等严重有偏的数据\n整群抽样：将总体数据按照属性拆分为互不相交、互不重复的群，这些群中的数据尽可能具有不同属性，尽量能代表总体数据的信息，然后以群为单位进行抽样"
    },
    {
        "title": "数据清洗",
        "content": "定义：数据清洗是指通过删除、转换、组合等方法或策略清洗数据中的异常样本，为数据建模提供优质数据的过程；场景：不均衡数据处理、缺失值处理、异常值处理\n1.不均衡数据处理：类别数据不平衡是分类任务中出现的经典问题，一般在数据清洗环节进行处理，不均衡简单来讲就是数据集中一个类别的数据远超其他类别数据的数据量，比如在1000条用户数据中，男性数据占950条，女性数据只占50条；处理办法1重采样数据（过采样：对少的一类进行重复选择，欠采样：对多的一类进行少量随机选择）2.K-fold交叉验证3.一分类4.组合不同的重采样数据集（核心原理：建立N个模型。    假设稀有样本有100个，然后从丰富样本中抽取1000(100*10)个数据，将这1000个分为N份，分别和100个稀有数据合并建立模型）\n2.异常值处理：异常值指偏离正常范围的值，不是错误值，异常值出现频率较低，但又会对实际项目分析造成影响；检测方式：异常值一般通过箱型图或分布图来判断；处理办法：采取盖帽法或者数据离散化   2、删除异常值、使用与异常值较少的时候  3、将异常值视为缺失值，按照缺失值的处理方法处理   4、估算异常值，mean/mode/median\n3.缺失值处理：数据缺失产生原因：1、人为疏忽、机器故障等客观原因造成数据缺失   2、人为故意隐瞒部分数据，比如在数据表中有意将一列属性视为空值，此时缺失值可以被看作为特殊的特征值   3、数数据本身不存在，例如银行在做用户信息收集时，对于学生群体来说，薪资这一列就不存在，所以在数据集中显示为空值   4、系统实时性要求较高  5、历史局限性导致数据收集不完整；；；.缺失值处理方法包括1.删除（适用场景：数据量大，缺失值少的数据集，完全随机缺失时可以直接使用删除操作）2.填充。3.不处理（补齐得缺失值毕竟不是原始数据，所以不一定符合客观事实，数据填充在一定程度上改变了数据的原始分布，也不排除加入噪音点的可能，因此对于一些无法容忍缺失值的模型可以进行填充，但有些模型本身可以容忍一定的数据缺失，此时选用不处理的方式，比如XGboost模型）\n缺失值填充方法包括如下4种：（1）.数值填充：众数填充-以类别数据量较多的类别填充，适用于数据倾斜时。；均值、中位数填充：以所有非缺失值的Mean或Median填充缺失值(广义插补)   2、分类别计算非缺失值的Mean或Median填充各类别的缺失值(相似填充)；均值填充：适用于数据中没有极端值的场景；中位数填充：适用于数据中有奇异值的场景\n（2）KNN：通过KNN算法将所有样本进行划分，通过计算欧氏距离，选取与缺失数据样本最近的K个样本，然后通过投票法或者K个值加权平均来估计该缺失样本的缺失数据；优点：不需要为含有缺失值的每个属性都建立预测模型  2、一个属性有多个缺失值时也可以很好解决   3、缺失值处理时把数据结构之间的相关性考虑在内；缺点：面对大数据集，KNN时间开销大  2、K值得选择是关键，过大过小都会影响结果。\n（3）回归：把数据中不含缺失值得部分当作训练集，建立回归模型，将此回归模型用来预测缺失值；适用场景：只适用缺失值是连续得情况；优缺点：预测填充理论上比值填充效果好，但如果缺失值与其他变量没有关系，那么预测出得缺失值没有意义\n（4）变量映射：把变量映射到高维空间中，优点：可以保留数据得完整性，无需考虑缺失值，缺点：1、计算开销增加  2、可能会出现稀疏矩阵，影响模型质量。"
    },
    {
        "title": "总结-机器学习，有监督，无监督",
        "content": "（一）、有监督学习：\n1.逻辑回归:\n优点:速度快,适合二分类问题;简单易懂,直接看到各个特征的权重;能容易地更新模型吸收新的数据.\n缺点:容易欠拟合,一般准确度不太高;不能很好地处理大量多类特征或变量;只能处理两分类问题,且必须线性可分.\n适用场景: 用于二分类领域,可以得出概率值.\n2.KNN(K最近邻算法):\n三要素: K值的选取、距离度量方式(欧氏距离、曼哈顿距离)、分类决策规则(多数表决法、平均法).\n优点:理论成熟,思想简单,既可以用来做分类也可以用来做回归;可用于非线性分类;\n对数据没有假设,准确度高,对噪声不敏感.\n缺点:计算量大;样本不平衡问题;需要大量的内存.\n适用场景: 可用于回归,分类,对于类域的交叉或重叠较多的待分样本集来说,KNN方法较其他方法更为适合.\n3.朴素贝叶斯:\n公式: P(Ci | X) = P(X | Ci)P(Ci)/P(X)\n优点:容易实现;对小规模的数据表现好;对缺失数据不大敏感.\n缺点:算法成立的前提是假设各属性之间互相独立.当数据集满足这种独立性假设时,分类度较高.而实际领域中,数据集可能并不完全满足独立性假设；需要计算先验概率；\n适用场景: 数据的各个维度对target的影响不相关或者相关度较小、适合向懂数学的客户解释、联合处理高维的数据,效果不太好.\n4.支持向量机(SVM):\n分类:线性可分:计算分类超平面;线性不可分:通过核函数将数据映射到更高的维度,然后再计算分类超平面.\n核函数: 线性核函数、多项式核函数、高斯核函数、Sigmoid核函数.\n优点:分类效果好;原理简单,有良好的解释性.\n缺点:不适用于大规模数据集;只能用于二分类场景;对缺失值敏感,对核函数的选择敏感.\n适用场景: 一般会应用于数据量较小的二分类场景.\n5.决策树:\n优点：概念简单，计算复杂度不高；可解释性强；能同时处理数据型和常规型属性；对中间值缺失不敏感；应用范围广；可以扩展性很强。\n缺点：完全生长的决策树容易导致过拟合；信息增益来度量会偏向于取值较多的属性；剪枝过程较难控制。\n适用场景：算法可解释性好，算法速度快，对硬件性能要求不高。\n6.随机森林:\n优点：相对于决策树算法具有更好的准确率；构建树的过程中采用并行的方式提高了算法运行效率；能够直接处理高维度数据；不需要提前降维；使用有放回的抽样方式使模型方差小，泛化能力强。\n缺点：取值比较多的特征对随机森林的决策会产生更大的影响；Bagging改进了预测的准确率但损失了解释性；在某些噪音比较大的特征上RF模型还是容易陷入过拟合。\n适用场景：数据维度相对较低，几十维；客户要求高准确性，无需调参就可以达到好的分类效果。\n7.自适应提升(Adaboost):\n优点：作为分类器时，分类精度很高；在AdaBoost的框架下可以使用各种回归分类模型来构建弱学习器非常灵活；构造简单，结果可理解；不容易发生过拟合。\n缺点：对异常样本敏感，异常样本在迭代中可能会获得较高的权重，影响最终的强学习器的预测准确性。\n适用场景：适用于二分类问题。\n8.梯度提升树(GBDT):\n优点：可以灵活处理各种类型的数据；在相对少的调参时间情况下，预测的准备事也可以比较高；使用一些健壮的损失函数，对异常值的鲁棒性非常强；很好的利用了弱分类器进行级联；充分考虑了每个分类器的权重。\n缺点：对异常样本敏感，异常样本在迭代中可能会获得较高的权重，影响最终的强学习器的预测准确性。\n适用场景：几乎可用于所有回归问题（线性/非线性）；也可用于二分类问题（设定阈值，大于阈值为正例，反之为负例）。\n9.朴素贝叶斯(Naive Bayes):\n优点：对缺失值不敏感，可以给缺失值自动划分方向；引入阔值限制树分裂，控制树的规模，防止过拟合问题；分裂数据分开后与未分割前的差值增益，不用每个节点排序算增益，减少计算量，可以并行计算。\n（二）、无监督学习（关联规则和聚类）：\nK-Means算法优点：1.简单、易于理解、运算速度快；2.对处理大数据集，该算法保持可伸缩性和高效性；3.当簇接近高斯分布时，它的效果较好。缺点：1.在 K-Means算法是局部最优的，容易受到初始质心的影Ⅱ向；2.在 K-Means算法中K值需要事先给定的，有时候K值的选定非常难以估计；3.在簇的平均值可被定义的情况下才能使用，只能应用连续型数据；4.大数据情况算法开销是非常大的；5.对噪声和孤立点数据敏感。\nK-mediods算法优点：1.K-mediods算法具有能够处理大型数据集 2.结果能相当紧凑，并且簇与簇之间明显分明 3.相比于K-means对噪声点不敏感。缺点：1.只适用于连续性数据；2.只适用于聚类结果为凸形的数据集等；3.必须事先确定K值；4.一般在获得一个局部最优的解后就停止了。\nK-means与k-mediods的区别：1、与K-means相比，K-mediods算法对于噪声不那么敏感，这样对于离群点就不会造成划分的结果偏差过大，少数数据不会造成重大影响。2、K-mediods由于上述原因被认为是对K-means的改进，但由于按照中心点选择的方式进行计算，算法的时间复杂度也比K-means上升了O(n)。\nDBScan优点：1.可以解决数据分布特殊（非凸，互相包络，长条形等）的情况。2.对于噪声不敏感，速度较快，不需要指定簇的个数；可适用于较大的数据集。3.在邻域参数给定的情况下结果是确定的，只要数据进入算法的顺序不变，与初始值无关。缺点：1.因为对整个数据集我们使用的是一组领域参数，簇之间密度差距过大时效果不好。2.数据集较大的时候很消耗内存。3.对于高维数据距离的计算会比较麻烦，造成维数灾难。\nApriori优点：1.使用先验原理，大大提高了频繁项集逐层产生的效率；2.简单易理解；数据集要求低。缺点：1.每二步产生候选项目集时循环产生的组合过多，没有排除不应该参与组合的元素；2.面对大数据集是，每次扫描比较数据和阈值会大大增加计算机系统的I/O开销。\nFP-growth优点：1.能适应海量数据场景。缺点：1.FP-Growth实现比较困难，在某些数据集上性能会下降；2.FP-Growth适用数据类型：离散型数据。\nApriori与Fp-growth算法对比：ariori算法多次扫描交易数据库，每次利用候选频集集产生频集；而FP-growth则利用树形结构，无需产生候选项集而是直接得到频繁集，大大减少扫描交易数据库的次数，从而提高了算法的效率，但是apriori的算法扩展性好，可以用于并行计算等领域。"
    },
    {
        "title": "总结-PCA与LDA对比",
        "content": "共同点:\nPCA与LDA都属于线性方法。\n两者在降维时都采用矩阵分解的方法。\n假设数据符合正态分布。\n不同点:\nLDA是有监督的，不能保证投影到新空间的坐标系是正交的，选择分类性能最好的投影方向。\nLDA降维保留个数与其对应类别的个数有关，与数据本身的维度无关。（原始数据是n维的，有c个类别，降维后一般是到c-1维）\nLDA可以用于降维，还可用于分类。"
    },
    {
        "title": "大数据ppt",
        "content": "大数据架构是关于大数据平台系统整体结构与组件的抽象和全局描述，用于指导大数据平台系统各个方面的设计和实施。\n一个典型的大数据平台系统架构应包括以下层次：1数据平台层（数据采集、数据处理、数据分析）；2数据服务层（开放接口、开放流程、开放服务）；3数据应用层（针对企业业务特点的数据应用）；4数据管理层（应用管理、系统管理）。\n(1)数据平台层 是大数据体系中最基础和最根本的部分。数据平台层一般包含三个层次。1.数据采集层：包括传统的ETL离线采集、实时采集等。 2.数据处理层：根据数据处理场景要求不同，对采集回来的数据进行一些规范化的预处理。常用处理方式可以分为Hadoop离线处理、实时流处理等。 3.数据分析层：包括传统的数据挖掘和进一步的机器学习、 深度学习等。\n(2)数据服务层是基于数据平台层，以开放接口、开放流程为基础，采用基于云计算的大数据存储和处理架构、分布式数据挖掘算法和基于互联网的大数据存储、处理和挖掘大数据服务模式。\n构建基于服务的大数据分析模式，提供大数据处理和分析的服务功能。\n基于互联网和云计算的大数据存储、处理和挖掘的数据中心系统架构，提供多用户、多任务的大数据分析服务。　\n(3)数据应用层 是各个企业根据自身的具体业务及应用所规划和实施的大数据应用和服务。主要将大数据应用到行业领域，实现基于行业的应用。\n根据企业的特点不同划分不同类别的应用，比如针对运营商，对内有精准营销、客服投诉、基站分析等，对外有基于位置的客流、基于标签的广告应用等等。\n主流的应用层面的技术包括大数据统计、分析、挖掘、展现等等。\n(4)数据管理层包括应用管理和系统管理。\n应用管理主要是从数据设计、开发到数据销毁的全生命周期管理，建立数据标准、质量规则和安全策略等，从而实现从事前管理、事中控制和事后稽核、审计的全方位的数据质量管理，元数据管理和安全管理。\n系统管理主要是将大数据平台纳入统一的云管理平台管理，云管理平台包括支持一键部署、增量部署的可视化运维工具、面向多租户的计算资源管控体系(多租户管理、安全管理、资源管理、负载管理、配额管理以及计量管理)和完善的用户权限管理体系，提供企业级的大数据平台运维管理能力支撑。\n主数据是描述数据产品特征的任何信息，如名字、位置、可感知的、重要性、质量、对企业的价值，以及与企业认为值得管理的其他数据产品的关系等。主数据决定信息架构的如何满足业务需求，因此主数据是数据治理计划的关键。\n\n大数据平台构建完成并投入使用时，可能会面临的问题：\n数据标准缺乏结构化管理；\n源数据变化造成数据平台数据混乱；\n来自组织不同部分的数据在多个报告上的不一致性；\n从数据生命周期角度经常存在的数据存储库、策略、标准和计算流程中的风险。\n\n 数据标准规范化--规范化管理构成数据平台的业务和技术基础设施，包括数据管控制度与流程规范文档、信息项定义等。\n 数据关系脉络化--实现对数据间流转、依赖关系的影响和血缘分析。\n 数据质量度量化--全方位管理数据平台的数据质量，实现可定义的数据质量检核和维度分析，以及问题跟踪。\n组织可通过治理其数据而达到以下目标：\n改进用户对报告的信任级别。\n确保数据在来自组织不同部分的多个报告上的一致性。\n确保恰当地保护企业信息，以满足审计者和监管者的需求。\n改进客户的洞察水平，推动营销计划的实施。\n直接影响组织最关注的 3 个因素：提高收入、降低成本和减少风险。\n将数据视为战略性企业资产，意味着组织需要建立其现有数据的清单，就像建立物理资产的清单一样。\n典型的组织拥有与其客户、供应商和产品相关的过量的信息。这样的组织甚至可能不知道所有这些数据位于何处。\n组织需要防御其财务、企业资源规划和人力资源应用程序中的关键业务数据受到未授权更改，因为这可能影响到其财务报告的完整性，以及日常业务决策的质量和可靠性。\n数据治理是一门将数据视为一项企业资产的学科。它涉及到以企业资产的形式对数据进行优化、保护和利用的决策权利。它涉及到对组织内的人员、流程、技术和策略的编排，以从企业数据获取最优的价值。\n\n\nDUGP(Unified Data Governance Platform)华为大数据统一数据治理平台，为运营商提供全面高效的数据资产管控环境，实现了数据的集中、统一和共享。包括统一的数据采集和整合，统一的安全、标准、生命周期和质量管理, 以及多维度数据云图功能。提供开箱即用的可以实现全生命周期的主数据管理，包括主数据的集中存储、主数据合并、主数据清洗、主数据监管和主数据的共享，满足集团对于企业级别主数据管理平台的需求。\n"
    }
]
    results = []
    for entry in json_data:
        if key.lower() in entry['title'].lower():
            results.append({'title': entry['title'], 'content': entry['content']})

    df = pd.DataFrame(results)
    df.style.set_properties(**{'white-space': 'pre-wrap', 'text-align': 'left'})
def lunshu_by_c(key):
    json_data=[
    {
        "title": "关联规则",
        "content": "反映了一个事物与其他事物之间相互依存性和关联性，如果两个事物或多个事物之间存在关联规则，那么其中的一个事物就可以由其他事物来预测到。关联规则必须在频繁项集中诞生且满足一定的置信度阈值\n整体步骤：1、生成频繁项集和生成规则2、找到强关联规则3、找出所有满足强关联规则的项集\n涉及的概念：项集（项的集合，包含k项的集合称为k项集）频繁项集（满足最小支持度阈值的项集）支持度（如果有两个不相交的非空集合X、Y(物品集)，N为数据记录总数，X集合和Y集合共同出现的次数占记录总数的比例：support(X->Y)=|X交Y|/N）置信度（集合X和集合Y共同出现的次数占集合X出现次数的比例，confidence(X->Y)=|X交Y|/|X|）提升度（表示置信度与Y总体发生的概率之比，lift(X->Y)=confidence(X->Y)/P(Y)）强关联规则（满足最小支持度阈值和最小置信度阈值的规则被称为强关联规则）"
    },
    {
        "title": "关联规则-Apriori算法",
        "content": "工作原理：1、首先扫描整个数据集，然后产生一个大的候选项集，计算每个候选项的次数，然后基于预先设定的最小支持度阈值，找出频繁一项集集合2、然后基于频繁一项集和原数据集找到频繁二项集3、同样的办法直到生成频繁N项集，其中已不可在生成满足最小支持度的N+1项集，也就是极大频繁项集4、根据Apriori定律1：频繁项集的子集一定是频繁项集，所以到此就找到了所有的频繁项集，然后又可以根据Apriori定律2：非频繁项集的超集一定是非频繁项集来帮助算法构建频繁项集树，加快收敛速度5、然后为每一个频繁项集创建一颗置信树（并不只为极大频繁项集创建置信树，还要为极大频繁项集的所有子集都创建置信树）6、最后可以得出数据之间的关联规则，且该关联规则满足支持度和置信度的阈值\n优点：1、使用先验原理，大大提高了频繁项集逐层产生的效率    2、简单易理解，数据集要求低\n缺点：1、每一步产生的候选项集时循环产生的组合过多，没有排除不该参与组合的元素    2、每次计算项集的支持度时，都需要将数据库中的记录全部扫描一遍，如果是一个大型数据库的话，这种扫描会大大增加计算机I/O的开销，而这种代价是伴随着数据库记录的增加呈几何级增长的，因此人们开始追求更好的算法\n应用：推荐系统：用关联算法做协同过滤，Apriori不适于非重复项集数元素较多的案例，建议分析的商品种类为10类左右。"
    },
    {
        "title": "关联规则-FP-growth算法",
        "content": "定义：该算法建立在Apriori算法概念之上，不同之处是它采用了更高级的数据结构FP-tree减少数据扫描次数，只需要扫描两次数据库，相比于Apriori减少了I/O操作，克服了Apriori算法需要多次扫描数据库的问题\n为了减少I/O次数，FP-growth引入了一些数据结构来临时存储数据，数据结构包括三部分：1、一个项头表里面记录了所有的频繁一项集出现的次数，并且按照次数降序排序  2、FP-Tree将原始数据集映射到了内存中的一棵FP树  3、节点链表，所有项头表的频繁一项集都是一个节点链表的头，它依次指向FP树中的该频繁一项集出现的位置。这样做主要是方便项头表和FP-tree之间的联系查找和更新，也好理解。\n算法流程：1、扫描数据，得到所有频繁一项集的计数，然后删除低于支持度阈值的项集，将频繁一项集放入项头表，并按支持度降序排列。2、扫描数据，将读到的原始数据剔除非频繁一项集，并将每一条再按支持度降序排列  3、读入排序后的数据集，逐条插入FP树，插入时按照排序后的顺序插入FP树中，排序靠前的是祖先节点，靠后的是子孙节点，如果有共用的祖先，则对应的共用祖先节点计数加1，插入后如果有新的节点出现，则项头表对应的节点会通过节点链表连接上新节点，直到所有数据都插入到FP树上，FP树建立完成。  4、从项头表的底部依次向上找到项头表对应的条件模式基。从条件模式基递归挖掘得到项头表项的频繁项集 5、如果不限制频繁项集的项数，则返回步骤4所有的频繁项集，否则只返回满足项数要求的频繁项集\n优点：优点：FP-growth一般快于Apriori，因为只扫描两次数据库\n缺点：缺点：1、FP-growth实现比较困难，在某些数据集上性能可能会下降   2、适用数据类型：离散型数据"
    },
    {
        "title": "集成学习、随机森林",
        "content": "概述：在机器学习中，直接建立一个高性能的分类器是很困难的，但是如果构建一系列性能较差的弱分类器，再将这些弱分类器集成起来，也许就能得到一个性能较高的分类器。通常根据训练集的不同，会训练得到不同的基分类器，这时可以通过训练集的不同来构造不同的基分类器，最终把他们集成起来，形成一个组合分类器。  组合分类器是一个复合模型，由多个基分类器组合而成，基分类器通过投票，组合分类器基于投票的结果进行预测。组合分类器往往比基分类器更加准确，常用的组合方法：装袋、提升、随机森林\n构建分类器的过程一般有两种集成方法1、利用训练集的不同子集训练得到不同的基分类器2、利用同一个训练集的不同属性子集构建不同的基分类器\n随机森林是bagging的一个扩展变体，它是以决策树为基学习器构建bagging的基础上，进一步在决策树的训练过程中引入了随机属性选择。具体步骤：1、从样本集中用自助法选出n个样本组成子集  2、从所有属性n中随机选择K个属性，选择最佳分裂属性(ID3、C4.5、CART)作为节点建立决策树，每棵树都最大程度生长，不进行剪枝 3、重复以上两步m次，建立m棵决策树  4、这m个决策树形成随机森林，在分类问题中通过投票决定输出属于哪一类，在回归问题中输出所有决策树输出的平均值\n优点：1、准确率高。2、各个初级学习器可以并行计算。3、能处理高维持征的数据样本，不需要降维。4、能够评估各个特征在分类问题中的重要性。5、因为采用随机选择特征，对部分特征缺失不数感。6、由于采用有放回抽样，训练出来的模型方差小,泛化能力强\n缺点：1、取值较多的特征会对RF的决策树产生更大影响有可能影响模型的效果。2、bagging保证了预测准确率，但损失了解释率。3、在某些噪音比较大的特征上，RF还是容易陷入过拟合"
    },
    {
        "title": "Bagging",
        "content": "对训练集进行有放回的抽取训练样例，从而为每一个基学习器都构建一个与训练集相当大小但各不相同的训练集，从而训练出不同的基学习器。Bagging是并行计算来训练每个弱学习器，且每个弱学习器的权重相等，且训练集权重也相等\n算法流程:1、从大小为N的原始数据集中独立随机地抽取m个数据(m<=n),形成一个自助数据集 2、重复第一步K次，产生K个独立的自助数据集。3、利用K个数据集训练出K个最优模型(K次可以并行进行) 4、分类问题中的分警结果根据K个模型的结果投票决定，回归问题:对K个模型的值求平均得到结果\n特点：1、通过降低基学习器的方差改善了泛化误差。2、由于每一个样本被选中的概率相同，所以装袋并不侧重于训练数据集中的任何实例，因为对于噪声数据，装袋不太受过分拟合的影响。3、由于是多个决策树组成，所以装袋提升了准确率的同时损失了解释性，哪个变是起到重要作用未知"
    },
    {
        "title": "boosting",
        "content": "主要作用和bagging类似，都是将昔干基分类器整合为一个分类器的方法，boosting是个顺序的过程，每个后续模里都会尝试纠正先前模型的错误，后续的模型依赖之前的模型\n算法步骤:1、首先给每一个训练样例赋予相同的权重 2、然后训练第一个基分类器并用它对训练集进行测试，对于那些分类错误的测试样本提高权重(实际算法中是降低分类正确的样本的权重)3、随后用调整后的带权训练集训练第二个基分类器 4、最后重复这个过程直到最后得到一个足够好的学习器\n提升是一个选代的过程，用于自适应地改变训练样本的分布，使得基分类警聚焦在那些很难分的样本上，不像bagging，提升给每一个训练样本赋予一个权值，而且可以在每一轮提升过程结束时自动地调整权值"
    },
    {
        "title": "adaboost",
        "content": "自适应在于前一个基分类器分错的样本会得到加权，加权后的全体样本再次被用来训练下一个基分类器，采用选代的思想，继承boosting，每次选代只训练一个弱学习器，训练好的弱学习器将参与下一次选代。\n步骤:1、初始化训练数据的权值分布，如果有N个样本，则每一个训练样本最开始时都被赋予相同的权值:1/N 2、通过训练集训练得到弱分类器，将分类正确的样本降低权重，同时提升分类错误样本的权重，将权重更新后的训练集用于训练下一个分类器，如此选代下去 3、将这些得到的弱分类器泪合成强分类器，加大那些分类误差率小的分类器的权重，使其在最终的分类函数中起较大的决定作用，降低分类误差率大的弱分类器的权重，使其在最终的分类函数中起着较小的作用，总之就是误差率低的弱分类器在最终分类器中权重较大，起较大的决定作用，否则较小。\n优点：1、很好地利用了弱分类器进行级联。2、提供的是框架，可以使用各种方法构建弱分类器，很灵活。3、不容易发生过拟合。4、具有很高的精度。5、相对于bagging和随机森林，adaboost充分考虑了每个分类器的权重\n缺点：对异常样本敏感，异常样本可能会在迭代过程中获得较高的权重值，最终影响模型效果。"
    },
    {
        "title": "XGboost",
        "content": "（1）xgboost本质上还是GBDT,但是把速和效率做到了极致，不同于传统的GBDT，只利用了一阶导数信息，xGboost对损失函数做了二阶求导泰勒展开，并在目标函数之外加入了正则项整体求最优解，用以权衡目标函数的下降和模型的复杂程度，避免过拟合，传统GBDTI以CART作为基学习器，XGboost还支持线性分类器，也就是xgboost相当于带L1和L2正则化项的逻辑回归(分类问题)或者线性回归(回归问题)\n（2）表勒展开的一次项和二次项系数不依赖于损失函数，所以xgboost支持自定义损失函数\n（3）XGboost在损失区数里加入正则项，用于控制模型复杂度，正则项降低了模型的复杂度，使学习出来的模型更加简单，防止过拟合，这点优于GBDT\n（4）xGboost还借鉴了随机森林算法，支持列抽样不仅能够降低过拟合还能减少计算，优于GBDT\n（5）Xgboost进行完一次选代后，会将叶子节点的权重乘上一个缩减系数，相当于学习速率\n(XxGboost中的eta)，主要是为了削弱每棵树的影响，让后面有更大的学习空间，实际应用中一般把eta设置的小一点(小学习率可使得后面的学习更加仔细)，选代次数设置的大一点\n（6）应用场景：预测用户行为喜好，商品推荐"
    },
    {
        "title": "梯度提升树（GBDT）",
        "content": "（1）梯度提升决策树，在GBDT中基学习器都是分类回归树，也就是CART，且使用的都是CART中回树。\n（2）对于初级学习器的结果来说一般都是低方差，高混差，因此GBDT的训练过程就是通过降低差来不断提高精度\n（3）GBDT的核心是在于累加所有树的结果作为最终结果，例如对年龄的累加，10岁加5岁，分类树的结果显然是累加不了的，所以GBDT中都是回归树，不是分类树，尽管GBDT调整后也可用于分类，但是不代表GBDT的树是分类树\n（4）GBDT的核心在于每一颗树学习的是之前所有树的结论和残差，这个残差就是一个加预测值后能得到真实值的累加是\n（5）主要使用到的损失函数有:0-1损失函数，对数损失区数，平方损失函数等，目标是:希望损失函数能够不断减小，还有希望损失函数能够尽快减小。\n（6）应用场景：常用于大数据挖掘竟赛；可用于几乎所有的回归问题，也可用于二分类但不适合多分类问题；广告推荐；排序问题"
    },
    {
        "title": "ELT，ETL",
        "content": "ELT，概念：即extract、load 、transform(数据抽取，加载，转换)\n（1）抽取：（1全量抽取）当数据源中有新数据加入或发生数据更新操作时，系统此时采用全量抽取，类似于数据迁移或复制，将数据源中的数据原封不动的从数据库中抽取出来，转换成自己的ETL工具可以识别的格式，一般在系统初始化时使用，全量一次后，就要每天采用增量抽取（2增量抽取）当数据源中有新数据加入或数据更新操作时，系统可以识别出更新的数据，此时采用增量抽取，增量抽取只抽取自上次抽取以来，数据库中新增或修改的数据，在ETL中增量抽取使用更广泛；适合场景：1源数据数据量小2源数据不易发生变化3源数据规律性变化4目标数据量巨大（3更新提醒）当数据源中有新数据加入或数据更新操作时，系统会发出提醒，是最简单的一种抽取方式\n（2）转换：抽取完数据后，要根据具体业务对数据进行转换操作，数据转换一般包括清洗和转换两部分，清洗掉数据集中重复、不完整的数据以及错误的数据\n（3）加载：是将已按照业务需求清洗、转换、整理后的数据加载到各类数据仓库或数据库中，进而进行智能商业分析、数据挖掘等。（1全量加载）全表删除后在进行数据加载（2增量加载）目标表仅更新源表中变化的数据。。。。\nELT优势：1、相对于传统的ETL来说，ELT数据处理管道中无需单独的转换引擎，数据转换和数据消耗在同个地方。2、减少了数据预处理的时间开销，在实际应用中，下游各应用的目的各不相同，同一份数据可能有不同的应用，进而做不同的转换操作，面对这种情况，ETL需要多次对数据进行抽取、转换、加载，而ELT只需要进行一次数据的抽取加载，多次转换，从而实现一份数据的多次应用，大大降低了时间的开销。\n案例：电商用户收集用户信息进行商业分析，有两个核心目标：1、提高用户转化率  2、提高商品的精准营销效率  这两个项目都需要从同一数据源抽取数据，但是项目目标不同，所以需要的细节数据也就不同。\n对于ETL来说，相关人员需要从数据源抽取两次数据（一个项目各需要抽取一次），然后每个项目各需要进行一次ETL的完整流程；对于ELT来说，相关人员只需从数据源抽取一次数据，然后将数据加载到目的地(Hive、Hbase中)，最终转换成我们项目所需要的细节数据即可，无需重复的抽取和加载过程。"
    },
    {
        "title": "正则化。Lasso回归和岭回归",
        "content": "正则化：根据奥卡姆剃刀原理，在所有能解释数据的模型中，越简单的模型越靠谱。但是在实际问题中，为了拟合复杂的数据，不得不采用更复杂的模型，使用更复杂的模型通常会产生过拟合，而正则化就是防止过拟合的工具之一，通过限制参数过大或者过多来避免模型的复杂\nL1（Lasso Regression）、L2正则化（Ridge Regression）的目的都是为了防止过拟合，两者差别在于：岭回归中的L2正则项能够将一些特征变成很小的值，而Lasso回归中的L1正则项得到的特征是稀疏的。Lasso回归会趋向于减少特征的数量，相当于删除特征，类似于降维，而岭回归会把一些特征的权重调小，这些特征都是接近于0的，因此Lasso回归在特征选择的时候非常有用，而岭回归就是一种规则化而已。\n在所有特征中，如果只有少数特征起重要作用的话，选择Lasso回归比较合适，它能自动选择特征。而大部分特征能起到作用而且作用比较平均的话，选择岭回归更合适"
    },
    {
        "title": "spark mllib，rdd",
        "content": "数据对象：1Dataframe、2RDD、3Dataset(具有RDD和dataframe的优点，同时避免他们的缺点)。\n相同点：1、都有惰性机制，在进行转换操作时不会立即执行，只有遇到Active操作时才会执行2、都是spark的核心，只是弹性分布式数据集的不同体现3、都具有分区的概念4、都有相通的算子，比如map、filter等5、都会根据spark的内存情况自动做缓存计算即使数据量很大也不用担心内存的溢出，不用担心OOM-当请求的内存过大时，JVM无法满足而自杀\n不同点：1、RDD以person作为类型参数，但spark并不知道person的内部结构，而Dataframe提供了详细的结构信息，使得sparkSQL可以清楚的知道数据中包含了哪些列，以及列的名称和类型Dataframe提供了数据的结构信息，即schema2、RDD是分布式JAVA的对象集合，Dataframe是row对象的集合3、Dataframe除了提供比RDD更加丰富的算子以外，更重要的是提升执行效率、减少数据读取和执行计划的优化，比如filter下推、裁剪等。\nRDD操作：（优点：类型安全，面向对象；缺点：序列化和反序列化开销大，GC垃圾回收机制的性能开销，频繁的创建和销毁对象，势必增加GC）基本操作主要为Transformation算子（该算子是通过转换从一个或多个RDD生成新的RDD，该操作是惰性的，只有调用action算子时才发起job,典型算子如map、filter、flatMap,distinct等）和Action算子（当代码调用该类型算子的时候，立即启动job，典型算子包括：count、saveasTextfile、takeordered等）\nDataframe操作：（优点：自带schema信息，降低序列化和反序列化的开销；缺点：不是面向对象的，编译期不安全）"
    },
    {
        "title": "决策树",
        "content": "定义：决策树是一种分类算法，它通过对有标签的数据进行学习，学习数据的特征，得到一个模型，什么样的数据就打上什么样的标签\n算法思想：选择属性特征对训练集进行分类，使得各个子数据集有更好的分类，其中的关键点就是寻找分裂规则，因为它们决定了给定节点上的元组如何分裂\n决策树生成步骤：1、根据特征度是选择，从上至下递归地生成子节点，直到数据集不可分则停止决策树生长。2、剪枝:决策树容易过拟合，需要剪枝来缩小树的结构和规模(包括预剪枝和后萝枝)。3、先剪枝:通过提前停止树的构建而对树萝枝。4、后剪枝:由完全生长的树减去子树\n特征度量选择：ID3采用信息增益大的特征作为分裂属性；C4.5采用信息增益率大的属性分裂，同时避免了ID3信息增益偏向取值较多的属性的缺点，但是其实是偏向于取值较少的特征；CART使用基尼指数克服了C4.5计算复杂度的缺点，偏向取值较多的属性\n场景：ID3和C4.5都只能处理分类问题，CART可以用于分类和回归； ID3与C4.5是多叉树，速度慢，CART是二叉树，计算速度快；ID3没有剪枝策略，C4.5有预剪枝和后剪枝，CART采用CCP代价复杂度后剪枝；\n预剪枝：提前设置好树的深度；后剪枝：用完全生长的树减去子树，在测试集上定义损失函数C，目标是通过剪枝使得在测试集上的C值下降，就是说通过剪枝使在测试集上误差率降低\n后剪枝步骤：\n1.自底向上的遍历每一个非叶节点（除了根节点），将当前的非叶节点从树中减去，其下所有的叶节点合并成一个节点，代替原来被剪掉的节点。 \n2. 计算剪去节点前后的损失函数，如果剪去节点之后损失函数变小了，则说明该节点是可以剪去的，并将其剪去；如果发现损失函数并没有减少，说明该节点不可剪去，则将树还原成未剪去之前的状态。 \n3. 重复上述过程，直到所有的非叶节点（除了根节点）都被尝试了。"
    },
    {
        "title": "决策树-id3",
        "content": "核心思想：选择信息增益最大的属性进行分裂，求信息增益时要先求出信息熵和条件熵，信息增益=信息熵-条件熵 ，信息增益：信息的不确定性减少的程度      信息熵：信息的不纯度，信息熵越大，数据分布越混乱，也就是概率分布越均匀，信息熵也越大；\nID3算法建立在奥卡姆剃刀的思想上，越是小型的决策树越优于大的决策树；算法流程：1、初始化属性集合和数据集合  2、计算数据集合的信息熵，和所有属性的条件熵，然后得出信息增益，选择信息增益最大的属性作为当前决策树的分裂节点  3、更新数据集合合属性集合，也就是删除掉上一步使用的属性，并按照属性值来划分不同分支的数据集合   4、依次对每种取值情况下的子集重复第2步  5、若子集只包含单一属性，则为分支叶子节点，根据其属性值标记  6、完成所有属性集合的划分；\nID3优点：1、概念简单，计算复杂度不高，可解释强，易于理解\n2、数据的准备工作简单，能够同时处理数据型和常规型居性\n3、对中间值的缺失不敏感，比较适合处理有缺失属性值的样本，能够处理不相关特征\n4、可以对很多属性的数据集构造决策树，可扩展性强，可用于不熟悉的数据集，并从中提取一些属性规则\nID3缺点：1、没有剪枝策略，可能会产生过度匹配问题，决策树过深，容易导致过拟合，泛化能力差，因此希要简直\n2、信息增益会对取值较多的属性有所好，也就是作为分类属性\n应用场景：适用于特征取值字段不多的数据集，因为信息增益会对取值较多的属性偏好，更容易选择这种属性作为分类属性"
    },
    {
        "title": "决策树-C4.5算法",
        "content": "ID3算法容易选择那些取值较多的特征来划分，因为根据这些属性划分出的数据纯度较高，比如身份证的例子。而且ID3算法属性只能是离散的，当然属性值也可以是连续的数值型，但是需要对这些数据进行数据预处理，变为离散型的才可以用ID3算法，所以C4.5继承了ID3的优点，改进了它的缺点(信息增益会对取值较多的属性有所偏好，也就是作为分类属性)，并在此基础上做了改进的算法，能够处理属性是连续型的\n信息增益率=信息增益/属性a的一个固有值，当属性a\n改进优化的点：1、用信息增益率代替信息增益来选择特征，克眼了用信息增益选择特征时偏向取值较多的属性的不足\n2、能够完成对连续型数值属性的离散化处理\n3、能处理居性值缺失的情况\n4、在决策树构造完成之后剪枝\n"
    },
    {
        "title": "决策树-CART算法",
        "content": "分类回归树，在ID3的基础上，进行优化的决策树，CART既可以是分类树，也可以是回归树，当作为分类树时，采用基尼指数作为节点的分裂依据；当作为回归树时，采用最小方差作为节点的分裂依据。 CART只能用来建立二叉树\nCART在分类问题中采用基尼值来衡量节点的纯度，节点越不纯，基尼值越大，以二分类为例，如果节点的所有数据只有一个类别，则基尼值为0；此外，CART算法采用基于CCP(代价复杂度)的后剪枝方法"
    },
    {
        "title": "算法评估指标",
        "content": "在机器学习算法中，我们可以将算法分为分类算法、回归算法和聚类算法，当我们建立完模型以后，需要用一定的评估指标来判断模型的优劣\n回归算法评估指标（预测连续型数值问题）：最常用的评估指标是 MSE均方误差（mean squared error），均方误差是描述估计量与被估计量之间差异程度的一种评估指标，也就是预测值与实际值误差平方和的均值\n聚类算法评估：聚类的评价方式从大方向上分为两类，一种是分析外部信息，一种是分析内部信息。外部信息就是可看得见的直观信息，比如聚类完成后的类别号。内部信息就是聚类结束后通过一些模型生成这个聚类的相关信息，比如熵值、纯度这种数学评价指标，常见的聚类评估指标有：互信息法、兰德系数、轮毂系数等。\n轮毂系数，适用于实际类别信息未知的情况。对于单个样本来说，设a为其与类别内各样本的平均距离，设b为其与距离最近的类别内的样本的平均距离，轮毂系数公式：(b-a)/max(a,b)\n分类算法评估指标（预测离散型数值问题）：1、错误率，指预测错误的样本数占总样本数的比例，又被叫做汉明损失。表达式：(FP+FR)/P+R。2、精度，指预测正确的样本数占总样本数的比例，又叫做预测准确率。表达式：(TP+TR)/P+R。3、查准率，又叫做精确率，指在预测为正的样本数中真正正例的比例，表示预测是否分类为1中实际为0的误报率，表达式：TP/(TP+FP)。4、查全率，又叫做召回率，指在所有实际正例样本中预测为正的样本数的比例，表示漏掉了一该被分类为1的，却被分为0的漏报成分。5、f1-score，在理想状态中，模型的查准率和查全率越高越好，但是在现实状况下，查准率和查全率会出现一个升高，一个降低的情况。所以就需要一个能够综合考虑查准率和查全率的评估指标，因此引入F值，F值表达式为：(a^2+1)pr/a^2(p+r)，a为权重系数，当a=1时，F值便是F1值，代表查准率和查全率权重相等，是最常用的一种评估指标。表达式为：f1=2pr/p+r。6、PR曲线，是描述查准率和查全率变化的曲线，以查准率和查全率为纵、横坐标轴，根据学习器的预测结果对测试样本进行排序，将最可能是正例的样本排在前面，最不可能是正例的样本排在后面，按照此顺序，依次将每个样本当作正例进行预测，每次计算P值和R值。7、ROC曲线，与PR曲线类似，都是按照排序的顺序将样本当作正例进行预测，不同的是ROC曲线引进TPR、FPR的概念，FPR是假正例率，TPR是真正例率，ROC曲线以TPR(真正例率)为横轴、FPR(假正例率)为纵轴。ROC更加偏重于测试样本评估值的排序好坏。8、AUC面积，进行模型性能比较时，如果模型A的ROC曲线被模型B的ROC曲线完全包住，那么认为B模型的性能更好。若A和B的ROC曲线有交叉的地方，则比较两个模型ROC曲线与坐标轴围成的面积，AUC被定义为ROC曲线下的面积，面积越大,AUC值越大，模型分类的质量就越好。AUC为1时，所有正例排在负例前面，AUC为0时，所有负例排在正例前面\n备注1：查准率是宁愿漏掉，不可错杀。应用场景：一般应用于识别垃圾邮件的场景中。因为我们不希望很多的正常邮件被误杀，这样会造成严重的困扰。因此在这种场景下，查准率是一个很重要的指标\n2：查全率是宁愿错杀，不可漏掉。应用场景：一般用于金融风控领域，我们希望系统能够筛选出所有风险的行为或用户，然后进行人工鉴别，如果漏掉一个可能会造成灾难性后果。\n3：对于类别不平衡问题，ROC曲线的表现会比较稳定（不会受不均衡数据的影响），但如果我们希望看出模型在正类上的表现效果，还是用PR曲线更好\n4：ROC曲线由于兼顾正例与负例，适用于评估分类器的整体性能（通常是计算AUC，表示模型的排序性能）；PR曲线则完全聚焦于正例，因此如果我们主要关心的是正例，那么用PR曲线比较好"
    },
    {
        "title": "最优化方法-无约束-梯度下降",
        "content": "除了目标函数以外，对参与优化的各变量没有其他函数或变量约束，称之为无约束最优化问题；目标函数f(x)，当对其进行最小化时，也把它称作为代价函数、损失函数或误差函数\n求解方法1直接法:通常用于当目标函数表达式十分复杂或者写不出具体表达式时。通过数值计算，经过一系列迭代过程产生点列，在其中搜索最优点\n求解方法2解析法:根据无约束最优化问题的目标函数的解析式给出一种最优解的方法，主要有梯度下降、牛顿法、拟牛顿法、共轭梯度法和共轭方向法等\n概念：1、方向导数:函数沿任意方向的变化率，需要求得某一点在某一方向的导数即方向导数。2、梯度方向:函数在变量空间中的某一点沿着哪个方向有最大的变化率?最大方向导数方向，即梯度方向。3、梯度:函数在某一点的梯度是一个矢量，具有大小和方向。它的方向与取得最大方向导数的方向一致，而它的模为方向导数的最大值。4、正梯度向量指向上坡，负梯度向量指向下坡。我们在负梯度方向上移动可以减小f(x)，被称为最速下降法或梯度下降法\n一、批量梯度下降：每更新一次权重需要对所有的数据样本点进行遍历。在最小化损失函数的过程中，需要不断反复的更新权重使得误差函数减小；特点：每一次的参数更新都用到了所有的训练数据，因此批量梯度下降法会非常耗时，样本数据量越大，训练速度也变得越慢\n二、随机梯度下降：为了解决批量梯度下降训练速度过慢的问题。它是利用随机选取每个样本的损失函数对sita求偏导得到对应的梯度来更新sita；因为随机梯度下降是随机选择一个样本进行迭代更新一次，所以伴随的一个问题是噪音比批量梯度下降的多，使得随机梯度下降并不是每次都向着整体优化的方向\n三、小批量梯度下降：为了解决前两者的缺点，使得算法的训练过程较快，而且也要保证最终参数训练的准确率。它是通过增加每次送代个数来实现的；每次在一批数据上优化参数并不会比单个数据慢太多，但是每次使用一批数据可以大大减小收敛所需的迭代次数，同时可以使得收敛的结果更加接近批量梯度下降的效果"
    },
    {
        "title": "最优化方法-有约束",
        "content": "等式约束最优化：拉格朗日乘子法是解决等式约束最优化的问题的最常用方法，基本思想就是通过引入拉格朗日乘子来将含有n个变量和k个约束条件的约束优化问题转化为含有（n+k）个变量的无约束优化问题\n不等式约束最优化：大部分实际问题的约束都是不超过多少时间，不超过多少人力等等，因此对拉格朗日乘子法进行了扩展，增加了KKT条件，求解不等式约束的优化问题；使用拉格朗日函数对目标函数进行了处理，生成了一个新的目标函数。通过一些条件可以求出最优值的必要条件，这个条件就是KKT条件。经过拉格朗日函数处理之后的新目标函数，保证原目标函数与限制条件有交点，也就是必须要有解"
    },
    {
        "title": "异常值",
        "content": "定义：异常值是偏离整体样本的观察值，也是偏离正常范围的点\n影响：异常值会影响模型的精度，降低模型的准确性。增加了整体数据方差；异常值是随机分布的，可能会改变数据集的正态分布。增加因此异常值处理是数据预处理中重要的一步，异常检测场景：入侵检测、欺诈检测、安全监测等\n出现原因：1、数据输入错误，相关人员故意或者无意导致数据异常，比如客户年收入13万美元，数据登记为130万美元。2、数据测量，实验误差，比如测量仪器不精准导致数据异常。3、数据处理错误，比如ETL操作不当，发送数据异常。4、抽样错误，数据采集时包含了错误数据或无关数据。5、自然异常值，非人为因素导致数据异常，比如今年某月份的降水量远超前几年同月份降水量。\n异常值检测方法：1、散点图：将数据用散点图可视化出来，可以观测到异常值。2、基于分类模型的异常检测：根据现有数据建立模型，然后对新数据进行判断从而确定是否偏离，偏离则为异常值，比如贝叶斯模型，SVM模型等。3、3sita原则：若数据集服从期望为u，方差为σ^2的正态分布，异常值被定义为其值与平均值的偏差超过三倍标准差的值。4、箱型图分析：箱型图分为上界，下界，上四分位数，下四分位数，以及离群点   四分位数就是将所有数值按从小到大排列并分成四等份，处于三个分割点位置的数值就是四分位数。（上界:上限是非异常范围内的最大值；下界:下限是非异常范围内的最小值；上四分位数:数值排序后处于75%位置上的值；下四分位数:数值排序后处于25%位置上的值）\n异常值处理办法：1、删除异常值 -适用于异常值较少的情况。2、将异常值视为缺失值，按照缺失值的处理方法处理异常值。3、估算异常值，均值、中位数、众数填充异常值"
    },
    {
        "title": "训练集，测试集，验证集合",
        "content": "在训练过程中使用验证数据集来评估模型并更新模型超参数，训练结束后使用测试数据集评估训练好的模型的性能\n训练集：是用来构建机器学习模型，用于模型拟合的数据样本；测试集：用来评估训练好的模型的性能；验证集：辅助构建模型，用于构建过程中评估模型，为模型提供无偏估计，进而调整模型超参数和用于对模型的能力进行初步评估。通常用来在模型迭代训练时，用以验证当前模型的泛化能力（准确率，召回率等），以决定是否停止继续训练\n划分方法：（1留出法）直接将数据集划分为互斥的集合，一个用作训练集，一个用作测试集，且满足训练集U测试集=全集；训练集交测试集=空集  常见的划分为2/3-4/5的样本用作模型训练，剩下的用作测试。通常选择70%的数据作为训练集，30%的数据作为测试集；通常单次使用留出法得到的结果不够稳定可靠，一般采取分层抽样或者若干次随机划分作为留出法的评估结果\n（2自助法）给定包含M个样本的数据集，每次随机从中抽取一个样本，放入新的数据集中，然后从原始样本中有放回的抽取M次，就可以得到包含M个样本的数据集，平均情况下会有63.2%的原始样本出现在数据集中，而剩下的36.8%的原始样本不出现在数据集中，也称.632自助法\n（3交叉验证）将数据集划分为K个大小相同的互斥子集，同样保持数据分布的一致性，即采用分层抽样的方法获得这些子集。目的：在实际训练中，模型的结果通常对训练数据好，但是对训练数据以外的数据拟合程度较差，交叉验证的目的就是用作评价模型的泛化能力，从而进行模型选择\nK-折交叉验证：每次用K-1个子集的并集作为训练集，余下的那个子集作为测试集，这样就会产生K种数据集划分的情况，从而可以进行K次训练和测试，最终返回K次测试结果的均值\nK折交叉验证的K最常用的取值是10，此时为10折交叉验证，常用的还有5、20等\n股用于模型调优，找到使得模型泛化性能最优的超参数，在全部训练集上重新训练模型，并使用独立测试集对模型性能做出最终评价\n如果训练集相对较小，则增大K值：增大K的话，在每次选代过程中将会有更多的数据参与模型的训练，能够得到最小偏差，同时算法时间也会变长，且不同训练集间高度相似，会导致结果方差较高\n如果训练集相对较大，则减小K值：减小K的话，降低模型在数据集上重复拟合的时间和成本，在平均性能的基础上获得模型的准确评估"
    },
    {
        "title": "特征选择",
        "content": "为什么要进行特征选择：背景：现实中大数据挖掘任务，往往属性特征过多，而一个普遍存在的事实是，大数据集带来的关键信息之聚集在部分或少数特征上，因此需要从中选择出重要的特征使得后续建模过程只在一部分特征上建立，减少维数灾难出现的可能，同时去除不相关的特征，留下关键因素，降低学习的任务难度，更容易挖掘数据本身带有的规律，同时在特征选择的过程中，会对数据的特征有更充分的理解\n如何进行特征选择：当数据预处理完成后，需要选择有意义的特征进行模型训练，通常从三方面考虑：1、特征是否发散:如果一个特征不发散，例如方差接近于0，也就是说样本在这个特征上基本没有差异，这个特征对样本的区分作用不明显，区分度不高。2、待征之间的相关性:特征与待征之间的线性相关性，去除相关性高的特征。3、特征与目标之间的相关性:与目标相关性高的特征，应当优先选择\n三种常见特征选择方法对比：1、过去由于和特定的学习器无关，计算开销小，泛化能力强于后两种特征选择方法，因此，在实际应用中由于数据集很大，特征维度高，过港式特征选择应用的更广泛些，但它是通过一些统计指标对特征进行排字来选择，通常不能发现冗余。2、从模型性能的角度出发，包装法的性能要优于过考去，但时间开销较大。3、嵌入法中的L正则化方法对于特征理解和特征选择来说是非常强大的工具，它能够生成稀疏的模型，对于选择待征子集来说非常有用。4、嵌入法中的随机森林方法是当前比较主流的方法之一，它易于使用，一般不需要其他特征工程操作、调参等繁琐的步亲，有直接的工具包都提供平均不纯堂下降方法，它的两个主要问题：一是重要的特征有可能得分很低，二是这种方法对特征变量类别多的特征有利。"
    },
    {
        "title": "特征选择-Filter过滤法",
        "content": "按照特征发散性或者相关性对各个特征评分，设定阈值或待选择阈值的个数，选择特征；总的缺点是：若特征之间具有强关联，且非线性时，Filter方法不能避免选择的最优特征组合冗余。   Filter总结：利用不同的打分规则，对每个特征进行打分，相当于给每个特征赋予权重，按权重排序，对不达标的特征进行过滤\n1、方差选择法：方差越大的特征，对于分析目标影响越大，就越有用；如果方差较小，比如小于一，那么这个特征可能对算法的作用就比较小；如果某个特征方差为0，即所有的样本在这个特征上的取值都是一样的，那么对模型训练没有任何作用，可以直接舍弃（特征的方差越大越好）    ；实现方法：设定一个方差的阈值，当方差小于这个阈值的特征就会被删除；适用场景：只适用于连续变量。\n2、相关系数法（皮尔逊相关系数）：该方法衡量的是变量之间的线性相关性，两个变量之间的皮尔逊相关系数定义为两个变量之间的协方差与标准差的商，结果的取值区间为【-1，1】，-1表示完全的负相关，+1表示完全正相关，0表示没有线性相关    实现方法：R为相关性，P为显著性，首先看P值，P值用于判断R值，即相关系数有没有统计学意义，判断标准一般为0.05，当P值>0.05时，则相关性系数没有统计学意义，此时无论R值大小，都表明两者之间没有相关性；当P值<0.05，则表明两者之间有相关性，此时再看R值，R值越大相关性越大，正数则正相关，负数就是负相关   ；适用场景：适用于特征类型均为数值特征的情况；缺陷：只对线性关系敏感，如果特征与响应变量的关系是非线性的，即便两个变量具有一 一对应的关系，相关系数也可能会接近0。建议最好把数据可视化出来，以免得出错误的结论。\n3、卡方检验(𝜒^2检验)：它可以检验某个特征分布和输出值分布之间的相关性，𝝌^𝟐值描述了自变量与因变量之间的相关程度，𝜒^2值衡量实际值与理论值的差异程度，𝜒^2值越大，相关程度也就越大   实现方法：1、计算无关性假设(随机抽取一条实际值计算)   2、根据无关性假设生成新的理论值四格表   3、根据计算公式算出𝜒^2   4、计算该相依表的自由度，查询卡方分布的临界值表来判断𝜒^2值是否合理(需要用100%-卡方分布临界表的值才能得出相关性)\n4、互信息法：互信息表示两个变量是否有关系以及关系的强弱。互信息可以理解为，X引入导致Y的熵减小的量，从信息熵的角度分析特征和输出值之间的关系评分；互信息值越大，说明该特征和输出值之间的相关性越大，越需要保留；缺陷：它不属于度量方式，也没有办法归一化，在不同数据集上的结果无法做比较    2、对于连续变量通常需要先离散化，而互信息的结果对离散化的方式敏感\n"
    },
    {
        "title": "特征选择-wrapper包装法",
        "content": "根据目标函数(通常是预测效果评分)，每次选择若干特征，或者排除若干特征\n常用技术 -- 递归特征消除法RFE；\n使用一个基模型来进行多轮训练，每轮训练后，移除若干权值系数的特征，在基于新的特征集进行下一轮训练。  步骤：1、指定一个有N个特征的数据集 2、选择一个算法模型  3、指定保留特征的数量K(K<N)  4、第一轮对所有特征进行训练，算法会根据基模型的目标函数给出每个特征的评分或排名，将最小得分或排名的特征移除，这时候特征减少为N-1，对其进行第二轮训练，持续迭代，直到特征保留为K,这K个特征就是选择的特征\nRFE的稳定性很大程度上取决于在迭代时底层所采用的模型。例如，假如RFE采用的普通的回归，没有经过正则化的回归是不稳定的，那么RFE就是不稳定的；假如采用的是Ridge，而用Ridge正则化的回归是稳定的，那么RFE就是稳定的。"
    },
    {
        "title": "特征选择-Embedded嵌入法",
        "content": "先使用某些机器学习的算法或模型进行训练，得到各个特征的权值系数，根据系数从大到小选择特征\n（1基于L1正则化方法）：1、L1正则化将回归系数的L范数作为惩罚项加到损失函数上，由于正则化非0，这就使得那些弱的待征所对应的系数变为0，因此比L1正则化往往会使学习到的模型很保统(系数w经常为0)这个特性使得L1正则化成为一种很好的特征选择的方法。2、L1正则化像非正则化线性模型一样也是不稳走的，如果待征集合中具有相关联的持征，当数据发生轻微变化时，也有可能导致很大的差异。3、L1正则化能够生成稀疏的模型。4、L1正则化可以产生稀疏权值短阵，即产生一个稀疏模型，可以用于特征选择，也可以防止过拟合\n数据稀疏性说明:L1正则化本来就是为了降低过拟合风险，但是L1正则化的结果往往会得到稀疏数据，即L1方法选择后的数据拥有更多0分量，所以被当作特征选择的强大方法。稀疏数据对后续建模的好处:数据集表示的矩阵中有很多列与当前任务无关，通过特征选择去除这些列，如果数据備疏性比较突出，意味着去除了较多的无关列，模型训练过程实际上可以在较小的列上进行，降低学习任务的难度，计算和存储开销小，\n（2基于树模型方法）：基于树的预测模型能够用来计算特征的重要程度，因此能用来去除不相关的特征，随机森林具有准确率高，稳定性强、易于使用的优点，是目前最流行的机器学习算法之一，随机森林提供的特征选择方法：平均不纯度减少和平均精度下降。（一）、平均不纯度减少：1、随机森林由多个决策树构成，决策树的每一个节点都是关于某个特征的条件，目的时将数据集按照不同的取值一分为二。对于一个决策树森林来说，可以算出每个特征平均减少了多少不纯度，并把它平均减少的不纯度作为特征选择的值  2、若特征之间存在关联，一旦某个特征被选择后，其他特征的重要度就会急剧下降，而不纯度已经被选中的那个特征降下来，其他特征就很难在降低那么多不纯度。在理解数据时，容易错误的认为先被选中的特征是很重要的，而其余的特征是不重要的，但实际上这些特征对响应变量的作用可能非常接近；。（二）、平均精度下降：该方法直接度量每个特征对模型的精确率的影响，基本思路是打乱每个特征的特征值顺序，并且度量顺序变动对模型精确率的影响，因为对于不重要的变量，打乱其顺序对模型的准确率影响不会太大，对于重要的变量，打乱顺序就会降低模型的准确率；在特征选择上，需要注意：尽管在所有特征上进行了训练得到模型，然后才得到每个特征的重要性测试，这并不等于筛选掉某个或某几个重要特征后模型的性能就一定会下降很多，因为即便删掉某个特征后，其关联特征一样可以发挥作用，让模型性能不变"
    },
    {
        "title": "特征处理-特征缩放(数值归一化)",
        "content": "特征缩放可以提高模型的精度和模型的收敛速度，在实际业务中，当数据的量纲不同、数量级差距大时，会影响最终的模型，因此需要用到特征缩放\n1、标准化（常用）：概念:将训练集中的某一列特征缩放到均值为0，方差为1的状态；特点:标准化对数据进行规范化，去除数据的单位限制，将数据转换为无量纲的纯数值，便于不用单位的数据进行比较和加权，同时不改变数据的原始分布状态，标准化要求原始数据近似满足高斯分布，数据越接近高斯分布，标准化效果越佳；适用场景:数据存在异常值和较多的噪音值\n2、最小值-最大值归一化：概念:将训练集中某一列特征数值缩放到-1到1，或0到1之间；特点:受训练集中最大值和最小值影响大，存在数据集中最大值与最小值动态变化的可能；适用场景:数据较为稳定，不存在极端的最大值和最小值\n3、均值归一化：公式：(x-x均值)/max(x)-min(x)\n4、缩放成单位向量：公式：x/||x||\n5、基于树的方法是不需要特征归一化或标准化，比如随机森林，bagging 和 boosting等。基于参数的模型或基于距离的模型，需要特征归一化或标准化。\n"
    },
    {
        "title": "特征处理-数值离散化",
        "content": "概念:把无限空间中的有限个体映射到有限空间中去，提高算法的时空效率，简单讲就是在不改变数据相对大小的情况下，对数据进行相应缩小;离散化仅适用于只关注元素之间的大小关系而不关注元素数值本身的情况 在数据挖掘理论研究中，研究表明离散化数值也能在提高建模速度和提高模型精度上有显著作用；\n作用:离散化可以降低特征中的噪音节点，提升特征的表达能力但是离散化的过程都会带来一定的信息丢失\n（一）、数值变量分离散变量和连续变量\n连续变量中的有监督连续变量离散方法包括1R（把连续的区间分成小区间，然后根据类标签对区间内变量调整）、基于信息熵（自顶向下的方法，运用决策树理念进行离散化）、基于卡方（自底向上的方法，运用卡方检验的方法，自底向上合并数值进行有监督离散化，核心操作是merge）；\n连续变量中的无监督连续变量离散方法包括聚类划分（使用聚类算法将数据分为K类，需要制定K值的大小）；分箱-数据先排序（1等宽划分-把连续变量按照相同的区间间隔划分成几等份，也就是根据数值的最大值和最小值进行划分，分为N份，每份的数值间隔相同；2等频划分-把连续变量划分成几等份，保证每份数值个数相同）\n（二）、分类变量分有序分类变量和无序分类变量\n有序分类变量离散化方法包括Label-Encoding（有序分类变量数值之间存在一定的顺序关系，可直接使用划分后的数据进行数据建模。优点:解决了分类变量的编码问题。缺点:可解释性差）\n无序分类变量离散化方法包括独热编码（使用M位状态寄存器对M个状态进行编码，每个状态都有独立的寄存器位，这些特征互斥，所以在任意时候只有一位有效，也就是说这M位状态中只有一个状态位值为1，其他都是0，换句话说就是M个变量用M维表示，每个维度的数值为1或为0。；优点:解决了分类器不好处理分类变量的问题。；缺点:分类变量不宜过多，可能会造成稀疏矩阵）哑编码（哑编码和独热编码类似，唯一区别就是哑编码采用M-1位状态寄存器对M个状态进行编码。；优点:解决了分类器不好处理分类变量的问题。；缺点:分类变量不宜过多，可能会造成稀疏矩阵）"
    },
    {
        "title": "特征处理-特征编码",
        "content": "数据挖掘中，一些算法可以直接计算分类变量，比如决策树模型，但许多机器学习算法不能直接处理分类变量，他们的输入和输出都是数值型数据，因此把分类变量转换成数值型数据是必要的，可用独热编码和哑编码实现\n应用场景1、对逻辑回归中的连续变量做离散化处理，然后对离散特征进行独热编码或者哑编码，这样会使模型具有较强的非线性能力\n应用场景2、对于不能处理分类变量的模型，必须要先使用独热编码或哑编码，将变量转换成数值型；但若模型可以处理分类变量，那就无须转换数据，如树模型"
    },
    {
        "title": "用户画像",
        "content": "用户画像：即用户信息标签化，就是收集这个用户的各种行为和数据，从而分析得到这个用户的一些基本的信息和典型特征，最后形成一个人物原型，从中挖掘用户价值，从而提供业务推荐、精准营销等服务\n用户画像构建流程：1、收集用户相关数据：可以来源于网站上用户交易数据，用户日志数据，用户行为数据等；。\n2、数据预处理：对收集到的用户数据做一些预处理工作，包括检查数据是否完整，即数据是否含有缺失值，数据是否含有异常值以及数据是否存在不均衡现象，从而决定是否对缺失值填充，或者删除异常值，对不均衡数据进行重采样等数据清洗工作。最终将杂乱无章的数据转化为结构化数据，即我们后续机器学习模型可以识别的数据格式。；。\n3、用户行为数据建模：对于不携带标签的用户数据，也就是我们事先并不知道该用户属于哪一类别，或者说他和哪些用户行为数据比较相似，此时我们可以采用无监督学习中的聚类算法来对它们进行聚类，相似相近、关联性大的用户数据放在一起，不相似不相近，关联性不大的用户不放在一起，可以用这些聚类算法，针对不同形状的数据进行相应的聚类，此时聚类结束后，我们可以用相应的聚类评价指标来评价我们聚类模型性能的优劣，或者将聚类结果与专家给出的标准做对比，将聚类结果回归到真实商业逻辑上。对聚类后不同类别的用户进行打标签，从而得到带有标签的用户数据，对于携带标签的数据，我们接着选择机器学习中有监督学习的相关算法来对这些有标签的数据进行统计分析。\n4、举例：例如某银行推行一种信用卡，该银行的目标就是想知道哪些客户群体会办理信用卡，哪些客户群体不会办理信用卡，所以我们可以通过收集关于用户办理信用卡相关的数据，比如年龄、性别、婚姻状况、名下是否有房产、月收入年收入、之前是否办理过信用卡等等信息，通过聚类算法将用户分为三类群体，即会办理、不会办理以及未知三种客户群体，银行可据此向会办理信用卡的客户群体推送办卡相关业务以及活动福利，实现业务推荐和精准营销。最终我们可以通过现有的携带标签的用户数据，构建机器学习分类模型，例如LR、SVM等算法模型，然后通过该模型对未知用户进行预测，预测该用户是否会办理信用卡"
    },
    {
        "title": "无监督-聚类",
        "content": "无监督：是指在未加入标签的数据中，根据数据之间的属性特征和关联性对数据进行区分，相似相近、关联性大的数据放在一起，不相似不相近的、关联性不大的数据不放在一起；无监督本质：利用无标签的数据去学习数据的分布和数据与数据之间的关系；无监督算法包括如聚类算法、关联算法。\n分类：基于原型（K-means算法、K-means++算法、k-mediods算法）基于层次（HierarchicalClustring算法、Birch算法）基于密度（DBSCAN算法）\n（1）、K-means算法：思想：输入聚类个数K，已经包含n个数据样本的数据集，输出标准的K个聚类的算法，然后将n个数据样本划分为K个聚类，最终结果所满足：同一聚类中数据相似度较高，而不同聚类的数据相似度低；步骤：1、随机选取K个对象作为初始质心  2、计算样本到K个质心的欧氏距离，按就近原则将它们划分到距离最近的质心所对应的类中   3、计算各类别中所有样本对应的均值，将均值作为新的质心，计算目标函数  4、判断聚类中心或目标函数是否改变，若不变则输出，若改变，则返回2；；；；；\nK-means算法核心问题：K值如何选取：1、人工指定：多次选取K值，选择聚类效果最好的K值。2、均方根：假设我们有m个样本，该方法认为K=根号下m/2。3、枚举法：计算类内距离均值与类间距离均值之比，选择最小的K值，将所有K值进行二次聚类，选择两次聚类结果最相似的K值。4、手肘法：随着K值得增大，样本划分越来越精细，聚类得程度越来越高，误差平方和SSE便逐渐减小(误差平方和是所有样本的聚类误差，代表聚类效果的好坏，SSE越小越好)，当K小于真实聚类数时，随着K值的增大会大幅增加每个簇的聚合程度，SSE的下降幅度也会很大，当K等于真实聚类数时，随着K值的继续增大，聚合程度也会迅速减小，SSE的下降幅度便会骤减，然后随着K的继续增大而趋于平缓，所以K值和SSE的关系图就像一个手肘的图形，肘部对应的值便是数据真实聚类的个数\n（2）、K-means++：背景：为了解决K-means初始质心敏感的问题。；技术原理：不同于K-means算法是第一次随机选取K个样本作为初始质心，K-menas++是假设已经选取了p个初始质心，只有在选取第p+1个质心时，距离这p个质心越远的点会有更高的概率当选为第p+1个聚类中心(为了避免异常点的存在，第二个点的选择会从距离较远的几个点中通过加权选取第二个点)，只有在选取第一个聚类中心时是随机选取(p=1)，该方法的改进符合一般直觉：聚类中心之间距离的越远越好\n（3）、K-mediods：能够避免数据中异常值的影响；算法步骤：1、随机选取一组样本点作为中心点集  2、每个中心点对应一个簇 3、计算各样本点到各个中心点的距离，将样本点放入距离最近的中心点的类中。  4、计算各簇距簇内各样本点的距离绝对误差最小的点，作为新的聚类中心    5、如果新的聚类中心与原中心点相同，则过程结束，如果不同，则返回2\nK-mediods与k-means算法对比：1、算法流程基本一致。；2、质心的计算方式不同:k-means算法是将所有样本点对应的均值作为新的中心点，可能是样本中不存在的点 K-mediods是计算簇内每一个点到簇内其他点的距离之和，将绝对误差最小的点作为新的聚类中心，质心必须是某个样本点的值。；3、k-mediods可以避免数据中异常值带来的影响。；4、质心的计算复杂度更高:k-means直接将均值点作为新的聚类中心，而k-mediods需要计算簇内任意两点之间的距离，在对每个距离进行比较狭取新的质心，计算复杂度增加，速度变慢。；5、稳定性高，执行速度变慢:在具有异常值的小样本数据集中，k-mediods算法比k-means算法效果好，但是随着数据集规模的增加，k-mediods算法执行的速度会很慢，所以如果数据集本身不存在很多的异常值的话，就不用k-mediods代者k-means。；\n（4）、Hierarchical Clustering算法：思想:确保距离近的样本落在同一个族中\n步骤:1、每个样本点都作为一个簇，形成族的集合C 。2、将距离最近的两个簇合并，形成一个簇3、从C中去除这对簇。 4、最终形成层次树形的聚类结构树形图（判断两个簇之间的距离方法：1单链接 -- 不同两个簇之间最近的两个点的距离；2全链接-- 不同两个簇之间最远的两个点的距离；3均链接 -- 不同两个簇中所有点两两之间的平均距离）\n优点:可排除噪声点的干扰，但有可能和噪声点分为一簇 2、适合形状不规则，不要求聚类完全的情况 3、不必确定K值，可根聚类结果不同有不同的结果 4、原埋简单，易于理解\n缺点:计算量很大，耗费的存储空间相对于其他几种方法要高。 2、合并操作不能撤销 3、合并操作必须有一个合并限制比例，否则可能发生过度合并导致所有分类中心聚集，造成聚类失败\n适用场景:适合形状不规则，不要求聚类完全的情况\n（5）、Birch：使用聚类特征三元组表示一个簇的有关信息，而不用且体的一组点来表示该簇，通过构造满足分支因子和簇直径限制的聚类特征树来进行聚类三元组(数据点样本个数，数据点样本特征之和，数据点样本特征平方和)，分支因子:树的每个节点的样本个数，簇直径:一类点的距离范国\n算法步骤:1、扫描数据，建立聚类特征树 2.使用某种算法对聚类特征树的叶节点进行聚类\n优点:一次扫描就能进行很好的聚类\n缺点:要求是球形聚类，因为CF树存储的都是半径类的数据，都是球形才适合\n适用场景:因为Birch算法通过一次扫描就可以进行比较好的聚类，所以适用于大数据集，而且数据的分布呈凸型以及球形的情况，并且由于Birch算法需要提供正确的器类个数和簇直径限制，对不可视的高维数据不可行\n（6）、DBSCAN：一个聚类可以由其中任何核心对象唯一确定，该算法利用基于密度的概念，要求聚类空间中某一区域内的样本个数不小于某一给定闻值，该方法能够在具有噪声的空间数据库中发现任意形状的簇，可将密度足够大的相邻区域连接，能够有效处理异常数据，主要用于对空间教据的聚类。\n步骤:1、DBSCAN通过检查数据中每个样本的eps邻域来搜索簇，如果点p的eps邻域内包含的样本个数大于给定的闽值，那么就建立一个以点P为核心对象的簇。\n2、然后DBSCAN送代的聚集这些核心对象直接密度可达的对象，这个过程可能还会涉及密度可达簇的合并\n3、当没有新的样本点加入到簇中时，过程结束"
    },
    {
        "title": "数据属性",
        "content": "标称属性：标称，意味着与名称有关，标称属性的值是一些符号或事物的名称，每个值代表事物的某种类别、状态或编码。因此标称属性又被看做是分类的。标称属性的值不具有有意义的顺序，而且是不定量的。也就是说给定一个数据集，找出这种属性的均值没有意义;举例：头发颜色和婚姻状况，头发的颜色={黑色、棕色、红色、白色}；婚姻状况={单身、已婚、离异、丧偶}；其次还有职业，具有教师、程序员、农民等\n二元属性:二元属性是一种标称属性，只有两种状态，用来描述事物的两种状态，用值0或1来体现，0表示该属性不出现，1表示出现。二元属性又称布尔属性，如果两种状态对应的是True和False;对称的二元属性(两种状态具有相等的价值，携带相同的权重，例如性别，具有男女两种状态)非对称的二元属性(其状态的结果不是同等重要的，例如艾滋病化验结果的阳性和阴性，用1对最重要的结果(HIV阳性)编码，而另一个用0编码(HIV阴性))\n序数属性:其可能的值之间具有有意义的序或秩评定，但是相继值之间的差是未知的，也就是对应的值有先后顺序。序数属性可以把数量值的值域划分成有限个有序类别。（如0-很不满意，1-不满意，2-满意，3-很满意），把数值属性离散化得到。可以用众数和中位数表示序数属性的中心趋势，但不能定义均值.;举例：成绩={优、良、中、差}；    drink_size表示饮料杯的大小：大、小、中。\n数值属性:是定量的，即它是可以度量的量，用整数或实数值表示;(1)区间标度属性:用相等的单位尺度度量，区间标度属性的值有序，可以评估值之间的差，但不能评估倍数，没有固有的零点(也就是说0°C并不是指现在没有温度)。举例：摄氏温度、华氏温度，日历日期。不能说2020年是1010年的两倍，两者的差有意义，但是比值没有意义；(2)比率标度属性:具有固有零点的数值属性。比率标度属性的值有序，可以评估值之间的差，也可以评估倍数.举例：开氏温度、重量、高度、速度、货币量。拿货币来说，可以说200元比100元多100元，也可以说200元是100元的两倍\n离散属性：具有有限或无限可数个值，可以用或不用整数表示。例如，属性customer_ID是无限可数的。顾客数量是无限增长的，但事实上实际的值集合是可数的（可以建立这些值与整数集合的一一对应）举例：邮编、省份数目\n连续属性:如果属性不是离散的，则它是连续的。属性值为实数，一般用浮点变量表示。\n总结：（1）标称、二元、序数属性都是定性的，他们只描述对象的特征，而不给出实际大小和数值；（2）标称、二元属性的中心趋势可以用众数度量。序数属性的中心趋势可以用它的众数和中位数度量，但不能定义均值（3）所有属性都能用中心趋势来表述。标称、二元属性用众数度量；序数属性用众数、中位数度量。均值是数值属性的中心趋势描述"
    },
    {
        "title": "降维-PCA，奇异值、主成份分析",
        "content": "思想：降维就是将事物的特征进行压缩和筛选，将原始高维空间中的数据映射到低维空间中，该项任务比较抽象，如果没有特定领域的知识，很难事先决定采用哪些数据。比如在人脸识别任务中，如果直接采用图片的原始像素信息，那么数据的维度是非常大的，所以此时就要用到降维的方法，对图片信息进行处理，选出区分度最大的像素组合\n1、SVD（奇异值分解）：1、奇异值分解可以适用于任意矩阵的一种分解方法 2、奇异值分解可以发现数据中隐藏的特征来建立矩阵行列之间的关系 3、奇异值分解能够发现矩阵中的几余，并提供用于消除它的格式\n优点:原理简单，仅涉及简单的矩阵线性变换知识。可以有效处理数据噪音，矩阵处理过程中得到的三个短阵也具有物理意义\n缺点:分解出的矩阵可解释性差，计算量大\n应用场景:广泛的用来求解线性最小平方、最小二乘问题，低秩通近问题，数据压缩问题，应用于推荐系统(找到用户没有评分的物品，经过SVD压缩后低维空间中，计算未评分物品与其他物品的相似性，得到一个预测打分，再对这些物品评分从高到低排序，返回前N个物品推荐给用户)\n2、PCA（主成份分析法）：思想:寻找表示数据分布的最优子空间(降维，去掉线性相关性)，就是将n维特征映射到k维上(k<n)，k维是全新的正交特征(k维特征称为主成份，是重新构造出来的k维特征，而不是简单的从n维特征中去除n-k维特征)，PCA目的是在高维数据中寻找最大方差的方向，然后将原始数据映射到维数小的新空间中。数学原理:根据协方差矩阵选取前s个最大特征值所对应的特征向量构建映射矩阵，进行降维\n优点:1、方差衡量的无监督学习，不受样本标签的限制 2、各主成份之间正交，可消除原始数据中各特征之间的影响 3、计算方法简单，主要运算是奇异值分解，容易在计算机上实现\n缺点:主成份解释某含义具有一定的模糊性，不如原始样本特征解释性强 2、方差小的非主成份也可能含有重要信息，因降维丢弃后可能对后续数据处理产生影响\n算法流程：1、对所有样本构成的矩阵X去中心化2、求X的协方差矩阵C   3、利用特征值分解，求出协方差矩阵C的特征值和特征向量      4、取前s个最大特征值对应的特征向量构成变换矩阵W    5、用原数据集和变换矩阵W相乘，得到降维后的新数据集（矩阵）\n追问:详细介绍PCA：在PCA中，数据从原始坐标系转换到新的坐标系中，转换坐标系时，选择数据方差最大的方向作为坐标轴的方向，（方差最大的方向给出了数据最重要的信息），第一个坐标轴的方向是方差最大的方向，第二个新坐标轴的方向是与第一个新坐标轴正交且方差次大的方向，重复过程N次，N是数据原始维度，通过这种方式可以获得新的坐标系，其中大部分方差包含在前几个坐标轴中，而后几个坐标轴中方差基本为0，这时可以忽略后面的坐标轴，只保留前面几个包含大部分方差的坐标轴；通过计算协方差矩阵，可以得到协方差矩阵的特征值和特征向量，选取前几个最大特征值所对应的特征向量组成变换矩阵，就可以将矩阵转换到新的坐标系中，实现数据的降维\n3、LDA（线性判别分析）：思想:寻找可分性判据最大的子空间，即投影后类内间距最小，类间距离最大 2、用到了fisher的思想，即寻找一个向量，可以使得类内散度最小，类间散度最大，其实也就是选取特征向量构建映射矩阵，然后对数据进行处理，该方法能使投影后样本的类间散步矩阵最大，类内散步矩阵最小\n优点:降维过程中可以使用类别的先验经验 2、LDA在样本分类信息依赖均值而不是方差的时候，比PCA之类的算法更优\n缺点:LDA不适合对非高斯分布的样本进行降维，PCA也有这个问题 2、LDA在样本分类信息依赖方差而不是均值的时候，降维效果不好 3、LDA可能过度拟合数据 4、LDA降维最多降到类别数k-1的维数，如果我们降维的维度大于k-1，则不能使用LDA\n4、LLE（局部线性嵌入）：相比较于传统的PCA、LDA这种只关注样本方差的降维方法，LLE在保持降维效果的同时也保留了样本局部的线性特征，因为其保留了局部的线性特征，所以常被应用在高维数据可视化、图像识别等领域\n5、LDA VS PCA：共同点：1、都属于线性方法。2、在降维时都采用矩阵分解的方法。3、假设数据符合正态分布。；不同点：1、LDA是有监督的，不能保证投影到新空间的坐标系是正交的，选择分类性能最好的投影方向。2、LDA可以用于降维也可以用于分类。3、LDA降维保留个数与其对应的类别数有关，与数据本身维度无关\n奇异值分解定义：将矩阵分解为奇异向量和奇异值，可以将矩阵A分解为三个矩阵的乘积，即A=UDV^t，其中矩阵U和V为正交矩阵，U的列向量为左奇异向量，V的列向量为右奇异向量，D矩阵的对角线上的值为矩阵A的奇异值，D矩阵为对角矩阵，奇异值按大小进行排序，对角线之外的值为0\n奇异值分解应用：1、在机器学习和数据挖掘领域，奇异值的应用很广泛，比如应用在用于降维的PCA(主成份分析法)和LDA(线性判别分析法)，数据压缩（以图像压缩为代表）算法，还有做搜索引擎语义层次检索的LSI。；2、求矩阵伪逆：奇异值分解可以被应用于求矩阵的伪逆，若矩阵M的奇异值分解为M=UDV^t，那么M的伪逆为M=VD^+U^t，其中D+为D的伪逆，并将其主对角线上非零元素求倒数在转置得到的。求伪逆通常可以用来求解线性最小平方、最小二乘法问题。；3、矩阵近似值：奇异值分解在统计中的主要应用为PCA（主成分分析），一种数据分析的方法，用来发现大量数据中的隐含模式，可以用于模式识别，数据压缩等方面.PCA方法的作用是将数据集映射到低维空间中的坐标系中，数据集的特征值(可以用奇异值来表征)按照重要性排列，降维的过程其实就是舍弃不重要的特征向量，而剩下的特征向量组成的空间就是降维后的空间。；4、平行奇异值：用于频率选择性的衰落信道的分解"
    },
    {
        "title": "数据挖掘流程",
        "content": "数据挖掘是通过对大量的数据进行分析，以发现和提取隐含在其中的具有价值的信息和知识的过程。跨行业数据挖掘标准流程，是当今数据挖掘业界通用流行的标准之一，它强调数据挖掘技术在商业中的应用，是用以管理并指导Data Miner有效、准确开展数据挖掘工作获得最佳挖掘成果的一系列工作步骤的规范标准\n1、商业理解：从商业角度理解项目的目标和要求，然后把理解转化为数据挖问题的定义和一个旨在实现目标的初步计划。确定业务目标：分析项目背录，从业务角度分析项目的需求和目标，确定业务角度成功标准。；项目可行性分析：分析现有资源、条件和限制，风险估计、成本和效益估计。；确定数据控掘目标：明确数据挖狂的目标和成功标隹。；提出项目计划：对整个项目做出计划，初步估计用到的工具和技术。\n2、数据理解：开始于原始数据的收集，然后是熟悉数据，表明数据质量问题，探索数据进而对数据初步理解，发掘有趣的子集以形成对隐藏信息的假设。收集原始数据：收集项目所涉及到的数据，如有必要，将数据传入数据处理工具中，并做一些初步数据集成的工作，生成相应报告。；描述数据：对数据做一些大致描述，例如属性数、记录数等，生成报告。；探索数据：对数据做简单的统计分析，例如关键属性的分布。；检查数据质量：包括数据是否完整、是否有错误，是否会有缺失值等\n3、数据准备：从原始未加工的数据构造最终数据集的过程(这些数据指将要嵌入建模工具中的数据)，数据准备任务可能要实施多次，并且没有任何规定的顺序。这些任务包括表格、属性和记录的选择以及按照建模工具要求，对数据进行转换、清洗等。；数据选择：根据数据挖掘目标和数据质量选择合适数据，包括表格选择、属性选择和记是选择。；数据清洁：对选择出的教据进行清洁，提高数据质量，例去除噪音、填充缺失值等。；数据创建：在原始数据上生成新的记录或属性。；数据合并：利用表连接等方式将几个数据集合并在一起。；教据格式化：将数据转化为数据挖掘处理的格式。\n4、建立模型：选择和应用各种建模技术，同时对他们的参数进行调整，以达到最优值。选择建模技术 -- 确定数据挖症算法模型和参数；测试方案设计：设计某测试模型的质是和有效性机制模型训练：在准备好的数据集上运行数据挖掘算法，得出一个或多个模型模型测试评估：根据测试方案，从数据挖狂技术角度确定数据挖掘目标是否成功\n5、模型评估：更为彻底的评估模型和检查建立模型的各个步骤，从而确保它真正的达到了商业目标。结果评估：从商业角度评估得到的模型，甚至实际试用该模型测试其效果；过程回顾：回顾项目的所有流程，确定每一个阶段都没有错误；确定下一步工作：根据结果评估和过程回顾得出结论，确定部晋该控掘模型还是从某个阶段重新开始\n6、模型实施：实施计划 ：在业务运作中部署模型做出计划；监控和维护计划：如何监控模型在实际业务中的使用情况，如何维护该模型；作出最终报告 ：项目总结，项目经验和项目结果；项目回顾：回顾项目的实施过程，总结经验教训;对数据挖握的运行效果做一个预测"
    },
    {
        "title": "数据采集抽样方法",
        "content": "简单随机抽样：先将调查总体进行编号，然后根据抽签法或者随机数字表抽取部分所观察到的数据组成样本数据，分为有放回抽样和无放回抽样。常被应用于压缩数据量以减少费用和时间开销\n系统抽样：又被称为等距抽样，首先设定抽样间距为n，然后从前n个数据样本中抽取初始数据，然后按照顺序每隔n个单位抽取一个数据组成样本数据\n分层抽样：将总体数据按照特征划分为若干层次或类型，然后从各个类型和层次的数据中采用简单随机抽样或者系统抽样抽取子样本，最终将这些子样本组合为总体数据样本，常被应用于离网预警模型和金融欺诈模型等严重有偏的数据\n整群抽样：将总体数据按照属性拆分为互不相交、互不重复的群，这些群中的数据尽可能具有不同属性，尽量能代表总体数据的信息，然后以群为单位进行抽样"
    },
    {
        "title": "数据清洗",
        "content": "定义：数据清洗是指通过删除、转换、组合等方法或策略清洗数据中的异常样本，为数据建模提供优质数据的过程；场景：不均衡数据处理、缺失值处理、异常值处理\n1.不均衡数据处理：类别数据不平衡是分类任务中出现的经典问题，一般在数据清洗环节进行处理，不均衡简单来讲就是数据集中一个类别的数据远超其他类别数据的数据量，比如在1000条用户数据中，男性数据占950条，女性数据只占50条；处理办法1重采样数据（过采样：对少的一类进行重复选择，欠采样：对多的一类进行少量随机选择）2.K-fold交叉验证3.一分类4.组合不同的重采样数据集（核心原理：建立N个模型。    假设稀有样本有100个，然后从丰富样本中抽取1000(100*10)个数据，将这1000个分为N份，分别和100个稀有数据合并建立模型）\n2.异常值处理：异常值指偏离正常范围的值，不是错误值，异常值出现频率较低，但又会对实际项目分析造成影响；检测方式：异常值一般通过箱型图或分布图来判断；处理办法：采取盖帽法或者数据离散化   2、删除异常值、使用与异常值较少的时候  3、将异常值视为缺失值，按照缺失值的处理方法处理   4、估算异常值，mean/mode/median\n3.缺失值处理：数据缺失产生原因：1、人为疏忽、机器故障等客观原因造成数据缺失   2、人为故意隐瞒部分数据，比如在数据表中有意将一列属性视为空值，此时缺失值可以被看作为特殊的特征值   3、数数据本身不存在，例如银行在做用户信息收集时，对于学生群体来说，薪资这一列就不存在，所以在数据集中显示为空值   4、系统实时性要求较高  5、历史局限性导致数据收集不完整；；；.缺失值处理方法包括1.删除（适用场景：数据量大，缺失值少的数据集，完全随机缺失时可以直接使用删除操作）2.填充。3.不处理（补齐得缺失值毕竟不是原始数据，所以不一定符合客观事实，数据填充在一定程度上改变了数据的原始分布，也不排除加入噪音点的可能，因此对于一些无法容忍缺失值的模型可以进行填充，但有些模型本身可以容忍一定的数据缺失，此时选用不处理的方式，比如XGboost模型）\n缺失值填充方法包括如下4种：（1）.数值填充：众数填充-以类别数据量较多的类别填充，适用于数据倾斜时。；均值、中位数填充：以所有非缺失值的Mean或Median填充缺失值(广义插补)   2、分类别计算非缺失值的Mean或Median填充各类别的缺失值(相似填充)；均值填充：适用于数据中没有极端值的场景；中位数填充：适用于数据中有奇异值的场景\n（2）KNN：通过KNN算法将所有样本进行划分，通过计算欧氏距离，选取与缺失数据样本最近的K个样本，然后通过投票法或者K个值加权平均来估计该缺失样本的缺失数据；优点：不需要为含有缺失值的每个属性都建立预测模型  2、一个属性有多个缺失值时也可以很好解决   3、缺失值处理时把数据结构之间的相关性考虑在内；缺点：面对大数据集，KNN时间开销大  2、K值得选择是关键，过大过小都会影响结果。\n（3）回归：把数据中不含缺失值得部分当作训练集，建立回归模型，将此回归模型用来预测缺失值；适用场景：只适用缺失值是连续得情况；优缺点：预测填充理论上比值填充效果好，但如果缺失值与其他变量没有关系，那么预测出得缺失值没有意义\n（4）变量映射：把变量映射到高维空间中，优点：可以保留数据得完整性，无需考虑缺失值，缺点：1、计算开销增加  2、可能会出现稀疏矩阵，影响模型质量。"
    },
    {
        "title": "总结-机器学习，有监督，无监督",
        "content": "（一）、有监督学习：\n1.逻辑回归:\n优点:速度快,适合二分类问题;简单易懂,直接看到各个特征的权重;能容易地更新模型吸收新的数据.\n缺点:容易欠拟合,一般准确度不太高;不能很好地处理大量多类特征或变量;只能处理两分类问题,且必须线性可分.\n适用场景: 用于二分类领域,可以得出概率值.\n2.KNN(K最近邻算法):\n三要素: K值的选取、距离度量方式(欧氏距离、曼哈顿距离)、分类决策规则(多数表决法、平均法).\n优点:理论成熟,思想简单,既可以用来做分类也可以用来做回归;可用于非线性分类;\n对数据没有假设,准确度高,对噪声不敏感.\n缺点:计算量大;样本不平衡问题;需要大量的内存.\n适用场景: 可用于回归,分类,对于类域的交叉或重叠较多的待分样本集来说,KNN方法较其他方法更为适合.\n3.朴素贝叶斯:\n公式: P(Ci | X) = P(X | Ci)P(Ci)/P(X)\n优点:容易实现;对小规模的数据表现好;对缺失数据不大敏感.\n缺点:算法成立的前提是假设各属性之间互相独立.当数据集满足这种独立性假设时,分类度较高.而实际领域中,数据集可能并不完全满足独立性假设；需要计算先验概率；\n适用场景: 数据的各个维度对target的影响不相关或者相关度较小、适合向懂数学的客户解释、联合处理高维的数据,效果不太好.\n4.支持向量机(SVM):\n分类:线性可分:计算分类超平面;线性不可分:通过核函数将数据映射到更高的维度,然后再计算分类超平面.\n核函数: 线性核函数、多项式核函数、高斯核函数、Sigmoid核函数.\n优点:分类效果好;原理简单,有良好的解释性.\n缺点:不适用于大规模数据集;只能用于二分类场景;对缺失值敏感,对核函数的选择敏感.\n适用场景: 一般会应用于数据量较小的二分类场景.\n5.决策树:\n优点：概念简单，计算复杂度不高；可解释性强；能同时处理数据型和常规型属性；对中间值缺失不敏感；应用范围广；可以扩展性很强。\n缺点：完全生长的决策树容易导致过拟合；信息增益来度量会偏向于取值较多的属性；剪枝过程较难控制。\n适用场景：算法可解释性好，算法速度快，对硬件性能要求不高。\n6.随机森林:\n优点：相对于决策树算法具有更好的准确率；构建树的过程中采用并行的方式提高了算法运行效率；能够直接处理高维度数据；不需要提前降维；使用有放回的抽样方式使模型方差小，泛化能力强。\n缺点：取值比较多的特征对随机森林的决策会产生更大的影响；Bagging改进了预测的准确率但损失了解释性；在某些噪音比较大的特征上RF模型还是容易陷入过拟合。\n适用场景：数据维度相对较低，几十维；客户要求高准确性，无需调参就可以达到好的分类效果。\n7.自适应提升(Adaboost):\n优点：作为分类器时，分类精度很高；在AdaBoost的框架下可以使用各种回归分类模型来构建弱学习器非常灵活；构造简单，结果可理解；不容易发生过拟合。\n缺点：对异常样本敏感，异常样本在迭代中可能会获得较高的权重，影响最终的强学习器的预测准确性。\n适用场景：适用于二分类问题。\n8.梯度提升树(GBDT):\n优点：可以灵活处理各种类型的数据；在相对少的调参时间情况下，预测的准备事也可以比较高；使用一些健壮的损失函数，对异常值的鲁棒性非常强；很好的利用了弱分类器进行级联；充分考虑了每个分类器的权重。\n缺点：对异常样本敏感，异常样本在迭代中可能会获得较高的权重，影响最终的强学习器的预测准确性。\n适用场景：几乎可用于所有回归问题（线性/非线性）；也可用于二分类问题（设定阈值，大于阈值为正例，反之为负例）。\n9.朴素贝叶斯(Naive Bayes):\n优点：对缺失值不敏感，可以给缺失值自动划分方向；引入阔值限制树分裂，控制树的规模，防止过拟合问题；分裂数据分开后与未分割前的差值增益，不用每个节点排序算增益，减少计算量，可以并行计算。\n（二）、无监督学习（关联规则和聚类）：\nK-Means算法优点：1.简单、易于理解、运算速度快；2.对处理大数据集，该算法保持可伸缩性和高效性；3.当簇接近高斯分布时，它的效果较好。缺点：1.在 K-Means算法是局部最优的，容易受到初始质心的影Ⅱ向；2.在 K-Means算法中K值需要事先给定的，有时候K值的选定非常难以估计；3.在簇的平均值可被定义的情况下才能使用，只能应用连续型数据；4.大数据情况算法开销是非常大的；5.对噪声和孤立点数据敏感。\nK-mediods算法优点：1.K-mediods算法具有能够处理大型数据集 2.结果能相当紧凑，并且簇与簇之间明显分明 3.相比于K-means对噪声点不敏感。缺点：1.只适用于连续性数据；2.只适用于聚类结果为凸形的数据集等；3.必须事先确定K值；4.一般在获得一个局部最优的解后就停止了。\nK-means与k-mediods的区别：1、与K-means相比，K-mediods算法对于噪声不那么敏感，这样对于离群点就不会造成划分的结果偏差过大，少数数据不会造成重大影响。2、K-mediods由于上述原因被认为是对K-means的改进，但由于按照中心点选择的方式进行计算，算法的时间复杂度也比K-means上升了O(n)。\nDBScan优点：1.可以解决数据分布特殊（非凸，互相包络，长条形等）的情况。2.对于噪声不敏感，速度较快，不需要指定簇的个数；可适用于较大的数据集。3.在邻域参数给定的情况下结果是确定的，只要数据进入算法的顺序不变，与初始值无关。缺点：1.因为对整个数据集我们使用的是一组领域参数，簇之间密度差距过大时效果不好。2.数据集较大的时候很消耗内存。3.对于高维数据距离的计算会比较麻烦，造成维数灾难。\nApriori优点：1.使用先验原理，大大提高了频繁项集逐层产生的效率；2.简单易理解；数据集要求低。缺点：1.每二步产生候选项目集时循环产生的组合过多，没有排除不应该参与组合的元素；2.面对大数据集是，每次扫描比较数据和阈值会大大增加计算机系统的I/O开销。\nFP-growth优点：1.能适应海量数据场景。缺点：1.FP-Growth实现比较困难，在某些数据集上性能会下降；2.FP-Growth适用数据类型：离散型数据。\nApriori与Fp-growth算法对比：ariori算法多次扫描交易数据库，每次利用候选频集集产生频集；而FP-growth则利用树形结构，无需产生候选项集而是直接得到频繁集，大大减少扫描交易数据库的次数，从而提高了算法的效率，但是apriori的算法扩展性好，可以用于并行计算等领域。"
    },
    {
        "title": "总结-PCA与LDA对比",
        "content": "共同点:\nPCA与LDA都属于线性方法。\n两者在降维时都采用矩阵分解的方法。\n假设数据符合正态分布。\n不同点:\nLDA是有监督的，不能保证投影到新空间的坐标系是正交的，选择分类性能最好的投影方向。\nLDA降维保留个数与其对应类别的个数有关，与数据本身的维度无关。（原始数据是n维的，有c个类别，降维后一般是到c-1维）\nLDA可以用于降维，还可用于分类。"
    },
    {
        "title": "大数据ppt",
        "content": "大数据架构是关于大数据平台系统整体结构与组件的抽象和全局描述，用于指导大数据平台系统各个方面的设计和实施。\n一个典型的大数据平台系统架构应包括以下层次：1数据平台层（数据采集、数据处理、数据分析）；2数据服务层（开放接口、开放流程、开放服务）；3数据应用层（针对企业业务特点的数据应用）；4数据管理层（应用管理、系统管理）。\n(1)数据平台层 是大数据体系中最基础和最根本的部分。数据平台层一般包含三个层次。1.数据采集层：包括传统的ETL离线采集、实时采集等。 2.数据处理层：根据数据处理场景要求不同，对采集回来的数据进行一些规范化的预处理。常用处理方式可以分为Hadoop离线处理、实时流处理等。 3.数据分析层：包括传统的数据挖掘和进一步的机器学习、 深度学习等。\n(2)数据服务层是基于数据平台层，以开放接口、开放流程为基础，采用基于云计算的大数据存储和处理架构、分布式数据挖掘算法和基于互联网的大数据存储、处理和挖掘大数据服务模式。\n构建基于服务的大数据分析模式，提供大数据处理和分析的服务功能。\n基于互联网和云计算的大数据存储、处理和挖掘的数据中心系统架构，提供多用户、多任务的大数据分析服务。　\n(3)数据应用层 是各个企业根据自身的具体业务及应用所规划和实施的大数据应用和服务。主要将大数据应用到行业领域，实现基于行业的应用。\n根据企业的特点不同划分不同类别的应用，比如针对运营商，对内有精准营销、客服投诉、基站分析等，对外有基于位置的客流、基于标签的广告应用等等。\n主流的应用层面的技术包括大数据统计、分析、挖掘、展现等等。\n(4)数据管理层包括应用管理和系统管理。\n应用管理主要是从数据设计、开发到数据销毁的全生命周期管理，建立数据标准、质量规则和安全策略等，从而实现从事前管理、事中控制和事后稽核、审计的全方位的数据质量管理，元数据管理和安全管理。\n系统管理主要是将大数据平台纳入统一的云管理平台管理，云管理平台包括支持一键部署、增量部署的可视化运维工具、面向多租户的计算资源管控体系(多租户管理、安全管理、资源管理、负载管理、配额管理以及计量管理)和完善的用户权限管理体系，提供企业级的大数据平台运维管理能力支撑。\n主数据是描述数据产品特征的任何信息，如名字、位置、可感知的、重要性、质量、对企业的价值，以及与企业认为值得管理的其他数据产品的关系等。主数据决定信息架构的如何满足业务需求，因此主数据是数据治理计划的关键。\n\n大数据平台构建完成并投入使用时，可能会面临的问题：\n数据标准缺乏结构化管理；\n源数据变化造成数据平台数据混乱；\n来自组织不同部分的数据在多个报告上的不一致性；\n从数据生命周期角度经常存在的数据存储库、策略、标准和计算流程中的风险。\n\n 数据标准规范化--规范化管理构成数据平台的业务和技术基础设施，包括数据管控制度与流程规范文档、信息项定义等。\n 数据关系脉络化--实现对数据间流转、依赖关系的影响和血缘分析。\n 数据质量度量化--全方位管理数据平台的数据质量，实现可定义的数据质量检核和维度分析，以及问题跟踪。\n组织可通过治理其数据而达到以下目标：\n改进用户对报告的信任级别。\n确保数据在来自组织不同部分的多个报告上的一致性。\n确保恰当地保护企业信息，以满足审计者和监管者的需求。\n改进客户的洞察水平，推动营销计划的实施。\n直接影响组织最关注的 3 个因素：提高收入、降低成本和减少风险。\n将数据视为战略性企业资产，意味着组织需要建立其现有数据的清单，就像建立物理资产的清单一样。\n典型的组织拥有与其客户、供应商和产品相关的过量的信息。这样的组织甚至可能不知道所有这些数据位于何处。\n组织需要防御其财务、企业资源规划和人力资源应用程序中的关键业务数据受到未授权更改，因为这可能影响到其财务报告的完整性，以及日常业务决策的质量和可靠性。\n数据治理是一门将数据视为一项企业资产的学科。它涉及到以企业资产的形式对数据进行优化、保护和利用的决策权利。它涉及到对组织内的人员、流程、技术和策略的编排，以从企业数据获取最优的价值。\n\n\nDUGP(Unified Data Governance Platform)华为大数据统一数据治理平台，为运营商提供全面高效的数据资产管控环境，实现了数据的集中、统一和共享。包括统一的数据采集和整合，统一的安全、标准、生命周期和质量管理, 以及多维度数据云图功能。提供开箱即用的可以实现全生命周期的主数据管理，包括主数据的集中存储、主数据合并、主数据清洗、主数据监管和主数据的共享，满足集团对于企业级别主数据管理平台的需求。\n"
    }
]
    results = []
    for entry in json_data:
        if key.lower() in entry['content'].lower():
            results.append({'title': entry['title'], 'content': entry['content']})
    df = pd.DataFrame(results)
    df.style.set_properties(**{'white-space': 'pre-wrap', 'text-align': 'left'})
def lunshu_all():
    json_data=[
    {
        "title": "关联规则",
        "content": "反映了一个事物与其他事物之间相互依存性和关联性，如果两个事物或多个事物之间存在关联规则，那么其中的一个事物就可以由其他事物来预测到。关联规则必须在频繁项集中诞生且满足一定的置信度阈值\n整体步骤：1、生成频繁项集和生成规则2、找到强关联规则3、找出所有满足强关联规则的项集\n涉及的概念：项集（项的集合，包含k项的集合称为k项集）频繁项集（满足最小支持度阈值的项集）支持度（如果有两个不相交的非空集合X、Y(物品集)，N为数据记录总数，X集合和Y集合共同出现的次数占记录总数的比例：support(X->Y)=|X交Y|/N）置信度（集合X和集合Y共同出现的次数占集合X出现次数的比例，confidence(X->Y)=|X交Y|/|X|）提升度（表示置信度与Y总体发生的概率之比，lift(X->Y)=confidence(X->Y)/P(Y)）强关联规则（满足最小支持度阈值和最小置信度阈值的规则被称为强关联规则）"
    },
    {
        "title": "关联规则-Apriori算法",
        "content": "工作原理：1、首先扫描整个数据集，然后产生一个大的候选项集，计算每个候选项的次数，然后基于预先设定的最小支持度阈值，找出频繁一项集集合2、然后基于频繁一项集和原数据集找到频繁二项集3、同样的办法直到生成频繁N项集，其中已不可在生成满足最小支持度的N+1项集，也就是极大频繁项集4、根据Apriori定律1：频繁项集的子集一定是频繁项集，所以到此就找到了所有的频繁项集，然后又可以根据Apriori定律2：非频繁项集的超集一定是非频繁项集来帮助算法构建频繁项集树，加快收敛速度5、然后为每一个频繁项集创建一颗置信树（并不只为极大频繁项集创建置信树，还要为极大频繁项集的所有子集都创建置信树）6、最后可以得出数据之间的关联规则，且该关联规则满足支持度和置信度的阈值\n优点：1、使用先验原理，大大提高了频繁项集逐层产生的效率    2、简单易理解，数据集要求低\n缺点：1、每一步产生的候选项集时循环产生的组合过多，没有排除不该参与组合的元素    2、每次计算项集的支持度时，都需要将数据库中的记录全部扫描一遍，如果是一个大型数据库的话，这种扫描会大大增加计算机I/O的开销，而这种代价是伴随着数据库记录的增加呈几何级增长的，因此人们开始追求更好的算法\n应用：推荐系统：用关联算法做协同过滤，Apriori不适于非重复项集数元素较多的案例，建议分析的商品种类为10类左右。"
    },
    {
        "title": "关联规则-FP-growth算法",
        "content": "定义：该算法建立在Apriori算法概念之上，不同之处是它采用了更高级的数据结构FP-tree减少数据扫描次数，只需要扫描两次数据库，相比于Apriori减少了I/O操作，克服了Apriori算法需要多次扫描数据库的问题\n为了减少I/O次数，FP-growth引入了一些数据结构来临时存储数据，数据结构包括三部分：1、一个项头表里面记录了所有的频繁一项集出现的次数，并且按照次数降序排序  2、FP-Tree将原始数据集映射到了内存中的一棵FP树  3、节点链表，所有项头表的频繁一项集都是一个节点链表的头，它依次指向FP树中的该频繁一项集出现的位置。这样做主要是方便项头表和FP-tree之间的联系查找和更新，也好理解。\n算法流程：1、扫描数据，得到所有频繁一项集的计数，然后删除低于支持度阈值的项集，将频繁一项集放入项头表，并按支持度降序排列。2、扫描数据，将读到的原始数据剔除非频繁一项集，并将每一条再按支持度降序排列  3、读入排序后的数据集，逐条插入FP树，插入时按照排序后的顺序插入FP树中，排序靠前的是祖先节点，靠后的是子孙节点，如果有共用的祖先，则对应的共用祖先节点计数加1，插入后如果有新的节点出现，则项头表对应的节点会通过节点链表连接上新节点，直到所有数据都插入到FP树上，FP树建立完成。  4、从项头表的底部依次向上找到项头表对应的条件模式基。从条件模式基递归挖掘得到项头表项的频繁项集 5、如果不限制频繁项集的项数，则返回步骤4所有的频繁项集，否则只返回满足项数要求的频繁项集\n优点：优点：FP-growth一般快于Apriori，因为只扫描两次数据库\n缺点：缺点：1、FP-growth实现比较困难，在某些数据集上性能可能会下降   2、适用数据类型：离散型数据"
    },
    {
        "title": "集成学习、随机森林",
        "content": "概述：在机器学习中，直接建立一个高性能的分类器是很困难的，但是如果构建一系列性能较差的弱分类器，再将这些弱分类器集成起来，也许就能得到一个性能较高的分类器。通常根据训练集的不同，会训练得到不同的基分类器，这时可以通过训练集的不同来构造不同的基分类器，最终把他们集成起来，形成一个组合分类器。  组合分类器是一个复合模型，由多个基分类器组合而成，基分类器通过投票，组合分类器基于投票的结果进行预测。组合分类器往往比基分类器更加准确，常用的组合方法：装袋、提升、随机森林\n构建分类器的过程一般有两种集成方法1、利用训练集的不同子集训练得到不同的基分类器2、利用同一个训练集的不同属性子集构建不同的基分类器\n随机森林是bagging的一个扩展变体，它是以决策树为基学习器构建bagging的基础上，进一步在决策树的训练过程中引入了随机属性选择。具体步骤：1、从样本集中用自助法选出n个样本组成子集  2、从所有属性n中随机选择K个属性，选择最佳分裂属性(ID3、C4.5、CART)作为节点建立决策树，每棵树都最大程度生长，不进行剪枝 3、重复以上两步m次，建立m棵决策树  4、这m个决策树形成随机森林，在分类问题中通过投票决定输出属于哪一类，在回归问题中输出所有决策树输出的平均值\n优点：1、准确率高。2、各个初级学习器可以并行计算。3、能处理高维持征的数据样本，不需要降维。4、能够评估各个特征在分类问题中的重要性。5、因为采用随机选择特征，对部分特征缺失不数感。6、由于采用有放回抽样，训练出来的模型方差小,泛化能力强\n缺点：1、取值较多的特征会对RF的决策树产生更大影响有可能影响模型的效果。2、bagging保证了预测准确率，但损失了解释率。3、在某些噪音比较大的特征上，RF还是容易陷入过拟合"
    },
    {
        "title": "Bagging",
        "content": "对训练集进行有放回的抽取训练样例，从而为每一个基学习器都构建一个与训练集相当大小但各不相同的训练集，从而训练出不同的基学习器。Bagging是并行计算来训练每个弱学习器，且每个弱学习器的权重相等，且训练集权重也相等\n算法流程:1、从大小为N的原始数据集中独立随机地抽取m个数据(m<=n),形成一个自助数据集 2、重复第一步K次，产生K个独立的自助数据集。3、利用K个数据集训练出K个最优模型(K次可以并行进行) 4、分类问题中的分警结果根据K个模型的结果投票决定，回归问题:对K个模型的值求平均得到结果\n特点：1、通过降低基学习器的方差改善了泛化误差。2、由于每一个样本被选中的概率相同，所以装袋并不侧重于训练数据集中的任何实例，因为对于噪声数据，装袋不太受过分拟合的影响。3、由于是多个决策树组成，所以装袋提升了准确率的同时损失了解释性，哪个变是起到重要作用未知"
    },
    {
        "title": "boosting",
        "content": "主要作用和bagging类似，都是将昔干基分类器整合为一个分类器的方法，boosting是个顺序的过程，每个后续模里都会尝试纠正先前模型的错误，后续的模型依赖之前的模型\n算法步骤:1、首先给每一个训练样例赋予相同的权重 2、然后训练第一个基分类器并用它对训练集进行测试，对于那些分类错误的测试样本提高权重(实际算法中是降低分类正确的样本的权重)3、随后用调整后的带权训练集训练第二个基分类器 4、最后重复这个过程直到最后得到一个足够好的学习器\n提升是一个选代的过程，用于自适应地改变训练样本的分布，使得基分类警聚焦在那些很难分的样本上，不像bagging，提升给每一个训练样本赋予一个权值，而且可以在每一轮提升过程结束时自动地调整权值"
    },
    {
        "title": "adaboost",
        "content": "自适应在于前一个基分类器分错的样本会得到加权，加权后的全体样本再次被用来训练下一个基分类器，采用选代的思想，继承boosting，每次选代只训练一个弱学习器，训练好的弱学习器将参与下一次选代。\n步骤:1、初始化训练数据的权值分布，如果有N个样本，则每一个训练样本最开始时都被赋予相同的权值:1/N 2、通过训练集训练得到弱分类器，将分类正确的样本降低权重，同时提升分类错误样本的权重，将权重更新后的训练集用于训练下一个分类器，如此选代下去 3、将这些得到的弱分类器泪合成强分类器，加大那些分类误差率小的分类器的权重，使其在最终的分类函数中起较大的决定作用，降低分类误差率大的弱分类器的权重，使其在最终的分类函数中起着较小的作用，总之就是误差率低的弱分类器在最终分类器中权重较大，起较大的决定作用，否则较小。\n优点：1、很好地利用了弱分类器进行级联。2、提供的是框架，可以使用各种方法构建弱分类器，很灵活。3、不容易发生过拟合。4、具有很高的精度。5、相对于bagging和随机森林，adaboost充分考虑了每个分类器的权重\n缺点：对异常样本敏感，异常样本可能会在迭代过程中获得较高的权重值，最终影响模型效果。"
    },
    {
        "title": "XGboost",
        "content": "（1）xgboost本质上还是GBDT,但是把速和效率做到了极致，不同于传统的GBDT，只利用了一阶导数信息，xGboost对损失函数做了二阶求导泰勒展开，并在目标函数之外加入了正则项整体求最优解，用以权衡目标函数的下降和模型的复杂程度，避免过拟合，传统GBDTI以CART作为基学习器，XGboost还支持线性分类器，也就是xgboost相当于带L1和L2正则化项的逻辑回归(分类问题)或者线性回归(回归问题)\n（2）表勒展开的一次项和二次项系数不依赖于损失函数，所以xgboost支持自定义损失函数\n（3）XGboost在损失区数里加入正则项，用于控制模型复杂度，正则项降低了模型的复杂度，使学习出来的模型更加简单，防止过拟合，这点优于GBDT\n（4）xGboost还借鉴了随机森林算法，支持列抽样不仅能够降低过拟合还能减少计算，优于GBDT\n（5）Xgboost进行完一次选代后，会将叶子节点的权重乘上一个缩减系数，相当于学习速率\n(XxGboost中的eta)，主要是为了削弱每棵树的影响，让后面有更大的学习空间，实际应用中一般把eta设置的小一点(小学习率可使得后面的学习更加仔细)，选代次数设置的大一点\n（6）应用场景：预测用户行为喜好，商品推荐"
    },
    {
        "title": "梯度提升树（GBDT）",
        "content": "（1）梯度提升决策树，在GBDT中基学习器都是分类回归树，也就是CART，且使用的都是CART中回树。\n（2）对于初级学习器的结果来说一般都是低方差，高混差，因此GBDT的训练过程就是通过降低差来不断提高精度\n（3）GBDT的核心是在于累加所有树的结果作为最终结果，例如对年龄的累加，10岁加5岁，分类树的结果显然是累加不了的，所以GBDT中都是回归树，不是分类树，尽管GBDT调整后也可用于分类，但是不代表GBDT的树是分类树\n（4）GBDT的核心在于每一颗树学习的是之前所有树的结论和残差，这个残差就是一个加预测值后能得到真实值的累加是\n（5）主要使用到的损失函数有:0-1损失函数，对数损失区数，平方损失函数等，目标是:希望损失函数能够不断减小，还有希望损失函数能够尽快减小。\n（6）应用场景：常用于大数据挖掘竟赛；可用于几乎所有的回归问题，也可用于二分类但不适合多分类问题；广告推荐；排序问题"
    },
    {
        "title": "ELT，ETL",
        "content": "ELT，概念：即extract、load 、transform(数据抽取，加载，转换)\n（1）抽取：（1全量抽取）当数据源中有新数据加入或发生数据更新操作时，系统此时采用全量抽取，类似于数据迁移或复制，将数据源中的数据原封不动的从数据库中抽取出来，转换成自己的ETL工具可以识别的格式，一般在系统初始化时使用，全量一次后，就要每天采用增量抽取（2增量抽取）当数据源中有新数据加入或数据更新操作时，系统可以识别出更新的数据，此时采用增量抽取，增量抽取只抽取自上次抽取以来，数据库中新增或修改的数据，在ETL中增量抽取使用更广泛；适合场景：1源数据数据量小2源数据不易发生变化3源数据规律性变化4目标数据量巨大（3更新提醒）当数据源中有新数据加入或数据更新操作时，系统会发出提醒，是最简单的一种抽取方式\n（2）转换：抽取完数据后，要根据具体业务对数据进行转换操作，数据转换一般包括清洗和转换两部分，清洗掉数据集中重复、不完整的数据以及错误的数据\n（3）加载：是将已按照业务需求清洗、转换、整理后的数据加载到各类数据仓库或数据库中，进而进行智能商业分析、数据挖掘等。（1全量加载）全表删除后在进行数据加载（2增量加载）目标表仅更新源表中变化的数据。。。。\nELT优势：1、相对于传统的ETL来说，ELT数据处理管道中无需单独的转换引擎，数据转换和数据消耗在同个地方。2、减少了数据预处理的时间开销，在实际应用中，下游各应用的目的各不相同，同一份数据可能有不同的应用，进而做不同的转换操作，面对这种情况，ETL需要多次对数据进行抽取、转换、加载，而ELT只需要进行一次数据的抽取加载，多次转换，从而实现一份数据的多次应用，大大降低了时间的开销。\n案例：电商用户收集用户信息进行商业分析，有两个核心目标：1、提高用户转化率  2、提高商品的精准营销效率  这两个项目都需要从同一数据源抽取数据，但是项目目标不同，所以需要的细节数据也就不同。\n对于ETL来说，相关人员需要从数据源抽取两次数据（一个项目各需要抽取一次），然后每个项目各需要进行一次ETL的完整流程；对于ELT来说，相关人员只需从数据源抽取一次数据，然后将数据加载到目的地(Hive、Hbase中)，最终转换成我们项目所需要的细节数据即可，无需重复的抽取和加载过程。"
    },
    {
        "title": "正则化。Lasso回归和岭回归",
        "content": "正则化：根据奥卡姆剃刀原理，在所有能解释数据的模型中，越简单的模型越靠谱。但是在实际问题中，为了拟合复杂的数据，不得不采用更复杂的模型，使用更复杂的模型通常会产生过拟合，而正则化就是防止过拟合的工具之一，通过限制参数过大或者过多来避免模型的复杂\nL1（Lasso Regression）、L2正则化（Ridge Regression）的目的都是为了防止过拟合，两者差别在于：岭回归中的L2正则项能够将一些特征变成很小的值，而Lasso回归中的L1正则项得到的特征是稀疏的。Lasso回归会趋向于减少特征的数量，相当于删除特征，类似于降维，而岭回归会把一些特征的权重调小，这些特征都是接近于0的，因此Lasso回归在特征选择的时候非常有用，而岭回归就是一种规则化而已。\n在所有特征中，如果只有少数特征起重要作用的话，选择Lasso回归比较合适，它能自动选择特征。而大部分特征能起到作用而且作用比较平均的话，选择岭回归更合适"
    },
    {
        "title": "spark mllib，rdd",
        "content": "数据对象：1Dataframe、2RDD、3Dataset(具有RDD和dataframe的优点，同时避免他们的缺点)。\n相同点：1、都有惰性机制，在进行转换操作时不会立即执行，只有遇到Active操作时才会执行2、都是spark的核心，只是弹性分布式数据集的不同体现3、都具有分区的概念4、都有相通的算子，比如map、filter等5、都会根据spark的内存情况自动做缓存计算即使数据量很大也不用担心内存的溢出，不用担心OOM-当请求的内存过大时，JVM无法满足而自杀\n不同点：1、RDD以person作为类型参数，但spark并不知道person的内部结构，而Dataframe提供了详细的结构信息，使得sparkSQL可以清楚的知道数据中包含了哪些列，以及列的名称和类型Dataframe提供了数据的结构信息，即schema2、RDD是分布式JAVA的对象集合，Dataframe是row对象的集合3、Dataframe除了提供比RDD更加丰富的算子以外，更重要的是提升执行效率、减少数据读取和执行计划的优化，比如filter下推、裁剪等。\nRDD操作：（优点：类型安全，面向对象；缺点：序列化和反序列化开销大，GC垃圾回收机制的性能开销，频繁的创建和销毁对象，势必增加GC）基本操作主要为Transformation算子（该算子是通过转换从一个或多个RDD生成新的RDD，该操作是惰性的，只有调用action算子时才发起job,典型算子如map、filter、flatMap,distinct等）和Action算子（当代码调用该类型算子的时候，立即启动job，典型算子包括：count、saveasTextfile、takeordered等）\nDataframe操作：（优点：自带schema信息，降低序列化和反序列化的开销；缺点：不是面向对象的，编译期不安全）"
    },
    {
        "title": "决策树",
        "content": "定义：决策树是一种分类算法，它通过对有标签的数据进行学习，学习数据的特征，得到一个模型，什么样的数据就打上什么样的标签\n算法思想：选择属性特征对训练集进行分类，使得各个子数据集有更好的分类，其中的关键点就是寻找分裂规则，因为它们决定了给定节点上的元组如何分裂\n决策树生成步骤：1、根据特征度是选择，从上至下递归地生成子节点，直到数据集不可分则停止决策树生长。2、剪枝:决策树容易过拟合，需要剪枝来缩小树的结构和规模(包括预剪枝和后萝枝)。3、先剪枝:通过提前停止树的构建而对树萝枝。4、后剪枝:由完全生长的树减去子树\n特征度量选择：ID3采用信息增益大的特征作为分裂属性；C4.5采用信息增益率大的属性分裂，同时避免了ID3信息增益偏向取值较多的属性的缺点，但是其实是偏向于取值较少的特征；CART使用基尼指数克服了C4.5计算复杂度的缺点，偏向取值较多的属性\n场景：ID3和C4.5都只能处理分类问题，CART可以用于分类和回归； ID3与C4.5是多叉树，速度慢，CART是二叉树，计算速度快；ID3没有剪枝策略，C4.5有预剪枝和后剪枝，CART采用CCP代价复杂度后剪枝；\n预剪枝：提前设置好树的深度；后剪枝：用完全生长的树减去子树，在测试集上定义损失函数C，目标是通过剪枝使得在测试集上的C值下降，就是说通过剪枝使在测试集上误差率降低\n后剪枝步骤：\n1.自底向上的遍历每一个非叶节点（除了根节点），将当前的非叶节点从树中减去，其下所有的叶节点合并成一个节点，代替原来被剪掉的节点。 \n2. 计算剪去节点前后的损失函数，如果剪去节点之后损失函数变小了，则说明该节点是可以剪去的，并将其剪去；如果发现损失函数并没有减少，说明该节点不可剪去，则将树还原成未剪去之前的状态。 \n3. 重复上述过程，直到所有的非叶节点（除了根节点）都被尝试了。"
    },
    {
        "title": "决策树-id3",
        "content": "核心思想：选择信息增益最大的属性进行分裂，求信息增益时要先求出信息熵和条件熵，信息增益=信息熵-条件熵 ，信息增益：信息的不确定性减少的程度      信息熵：信息的不纯度，信息熵越大，数据分布越混乱，也就是概率分布越均匀，信息熵也越大；\nID3算法建立在奥卡姆剃刀的思想上，越是小型的决策树越优于大的决策树；算法流程：1、初始化属性集合和数据集合  2、计算数据集合的信息熵，和所有属性的条件熵，然后得出信息增益，选择信息增益最大的属性作为当前决策树的分裂节点  3、更新数据集合合属性集合，也就是删除掉上一步使用的属性，并按照属性值来划分不同分支的数据集合   4、依次对每种取值情况下的子集重复第2步  5、若子集只包含单一属性，则为分支叶子节点，根据其属性值标记  6、完成所有属性集合的划分；\nID3优点：1、概念简单，计算复杂度不高，可解释强，易于理解\n2、数据的准备工作简单，能够同时处理数据型和常规型居性\n3、对中间值的缺失不敏感，比较适合处理有缺失属性值的样本，能够处理不相关特征\n4、可以对很多属性的数据集构造决策树，可扩展性强，可用于不熟悉的数据集，并从中提取一些属性规则\nID3缺点：1、没有剪枝策略，可能会产生过度匹配问题，决策树过深，容易导致过拟合，泛化能力差，因此希要简直\n2、信息增益会对取值较多的属性有所好，也就是作为分类属性\n应用场景：适用于特征取值字段不多的数据集，因为信息增益会对取值较多的属性偏好，更容易选择这种属性作为分类属性"
    },
    {
        "title": "决策树-C4.5算法",
        "content": "ID3算法容易选择那些取值较多的特征来划分，因为根据这些属性划分出的数据纯度较高，比如身份证的例子。而且ID3算法属性只能是离散的，当然属性值也可以是连续的数值型，但是需要对这些数据进行数据预处理，变为离散型的才可以用ID3算法，所以C4.5继承了ID3的优点，改进了它的缺点(信息增益会对取值较多的属性有所偏好，也就是作为分类属性)，并在此基础上做了改进的算法，能够处理属性是连续型的\n信息增益率=信息增益/属性a的一个固有值，当属性a\n改进优化的点：1、用信息增益率代替信息增益来选择特征，克眼了用信息增益选择特征时偏向取值较多的属性的不足\n2、能够完成对连续型数值属性的离散化处理\n3、能处理居性值缺失的情况\n4、在决策树构造完成之后剪枝\n"
    },
    {
        "title": "决策树-CART算法",
        "content": "分类回归树，在ID3的基础上，进行优化的决策树，CART既可以是分类树，也可以是回归树，当作为分类树时，采用基尼指数作为节点的分裂依据；当作为回归树时，采用最小方差作为节点的分裂依据。 CART只能用来建立二叉树\nCART在分类问题中采用基尼值来衡量节点的纯度，节点越不纯，基尼值越大，以二分类为例，如果节点的所有数据只有一个类别，则基尼值为0；此外，CART算法采用基于CCP(代价复杂度)的后剪枝方法"
    },
    {
        "title": "算法评估指标",
        "content": "在机器学习算法中，我们可以将算法分为分类算法、回归算法和聚类算法，当我们建立完模型以后，需要用一定的评估指标来判断模型的优劣\n回归算法评估指标（预测连续型数值问题）：最常用的评估指标是 MSE均方误差（mean squared error），均方误差是描述估计量与被估计量之间差异程度的一种评估指标，也就是预测值与实际值误差平方和的均值\n聚类算法评估：聚类的评价方式从大方向上分为两类，一种是分析外部信息，一种是分析内部信息。外部信息就是可看得见的直观信息，比如聚类完成后的类别号。内部信息就是聚类结束后通过一些模型生成这个聚类的相关信息，比如熵值、纯度这种数学评价指标，常见的聚类评估指标有：互信息法、兰德系数、轮毂系数等。\n轮毂系数，适用于实际类别信息未知的情况。对于单个样本来说，设a为其与类别内各样本的平均距离，设b为其与距离最近的类别内的样本的平均距离，轮毂系数公式：(b-a)/max(a,b)\n分类算法评估指标（预测离散型数值问题）：1、错误率，指预测错误的样本数占总样本数的比例，又被叫做汉明损失。表达式：(FP+FR)/P+R。2、精度，指预测正确的样本数占总样本数的比例，又叫做预测准确率。表达式：(TP+TR)/P+R。3、查准率，又叫做精确率，指在预测为正的样本数中真正正例的比例，表示预测是否分类为1中实际为0的误报率，表达式：TP/(TP+FP)。4、查全率，又叫做召回率，指在所有实际正例样本中预测为正的样本数的比例，表示漏掉了一该被分类为1的，却被分为0的漏报成分。5、f1-score，在理想状态中，模型的查准率和查全率越高越好，但是在现实状况下，查准率和查全率会出现一个升高，一个降低的情况。所以就需要一个能够综合考虑查准率和查全率的评估指标，因此引入F值，F值表达式为：(a^2+1)pr/a^2(p+r)，a为权重系数，当a=1时，F值便是F1值，代表查准率和查全率权重相等，是最常用的一种评估指标。表达式为：f1=2pr/p+r。6、PR曲线，是描述查准率和查全率变化的曲线，以查准率和查全率为纵、横坐标轴，根据学习器的预测结果对测试样本进行排序，将最可能是正例的样本排在前面，最不可能是正例的样本排在后面，按照此顺序，依次将每个样本当作正例进行预测，每次计算P值和R值。7、ROC曲线，与PR曲线类似，都是按照排序的顺序将样本当作正例进行预测，不同的是ROC曲线引进TPR、FPR的概念，FPR是假正例率，TPR是真正例率，ROC曲线以TPR(真正例率)为横轴、FPR(假正例率)为纵轴。ROC更加偏重于测试样本评估值的排序好坏。8、AUC面积，进行模型性能比较时，如果模型A的ROC曲线被模型B的ROC曲线完全包住，那么认为B模型的性能更好。若A和B的ROC曲线有交叉的地方，则比较两个模型ROC曲线与坐标轴围成的面积，AUC被定义为ROC曲线下的面积，面积越大,AUC值越大，模型分类的质量就越好。AUC为1时，所有正例排在负例前面，AUC为0时，所有负例排在正例前面\n备注1：查准率是宁愿漏掉，不可错杀。应用场景：一般应用于识别垃圾邮件的场景中。因为我们不希望很多的正常邮件被误杀，这样会造成严重的困扰。因此在这种场景下，查准率是一个很重要的指标\n2：查全率是宁愿错杀，不可漏掉。应用场景：一般用于金融风控领域，我们希望系统能够筛选出所有风险的行为或用户，然后进行人工鉴别，如果漏掉一个可能会造成灾难性后果。\n3：对于类别不平衡问题，ROC曲线的表现会比较稳定（不会受不均衡数据的影响），但如果我们希望看出模型在正类上的表现效果，还是用PR曲线更好\n4：ROC曲线由于兼顾正例与负例，适用于评估分类器的整体性能（通常是计算AUC，表示模型的排序性能）；PR曲线则完全聚焦于正例，因此如果我们主要关心的是正例，那么用PR曲线比较好"
    },
    {
        "title": "最优化方法-无约束-梯度下降",
        "content": "除了目标函数以外，对参与优化的各变量没有其他函数或变量约束，称之为无约束最优化问题；目标函数f(x)，当对其进行最小化时，也把它称作为代价函数、损失函数或误差函数\n求解方法1直接法:通常用于当目标函数表达式十分复杂或者写不出具体表达式时。通过数值计算，经过一系列迭代过程产生点列，在其中搜索最优点\n求解方法2解析法:根据无约束最优化问题的目标函数的解析式给出一种最优解的方法，主要有梯度下降、牛顿法、拟牛顿法、共轭梯度法和共轭方向法等\n概念：1、方向导数:函数沿任意方向的变化率，需要求得某一点在某一方向的导数即方向导数。2、梯度方向:函数在变量空间中的某一点沿着哪个方向有最大的变化率?最大方向导数方向，即梯度方向。3、梯度:函数在某一点的梯度是一个矢量，具有大小和方向。它的方向与取得最大方向导数的方向一致，而它的模为方向导数的最大值。4、正梯度向量指向上坡，负梯度向量指向下坡。我们在负梯度方向上移动可以减小f(x)，被称为最速下降法或梯度下降法\n一、批量梯度下降：每更新一次权重需要对所有的数据样本点进行遍历。在最小化损失函数的过程中，需要不断反复的更新权重使得误差函数减小；特点：每一次的参数更新都用到了所有的训练数据，因此批量梯度下降法会非常耗时，样本数据量越大，训练速度也变得越慢\n二、随机梯度下降：为了解决批量梯度下降训练速度过慢的问题。它是利用随机选取每个样本的损失函数对sita求偏导得到对应的梯度来更新sita；因为随机梯度下降是随机选择一个样本进行迭代更新一次，所以伴随的一个问题是噪音比批量梯度下降的多，使得随机梯度下降并不是每次都向着整体优化的方向\n三、小批量梯度下降：为了解决前两者的缺点，使得算法的训练过程较快，而且也要保证最终参数训练的准确率。它是通过增加每次送代个数来实现的；每次在一批数据上优化参数并不会比单个数据慢太多，但是每次使用一批数据可以大大减小收敛所需的迭代次数，同时可以使得收敛的结果更加接近批量梯度下降的效果"
    },
    {
        "title": "最优化方法-有约束",
        "content": "等式约束最优化：拉格朗日乘子法是解决等式约束最优化的问题的最常用方法，基本思想就是通过引入拉格朗日乘子来将含有n个变量和k个约束条件的约束优化问题转化为含有（n+k）个变量的无约束优化问题\n不等式约束最优化：大部分实际问题的约束都是不超过多少时间，不超过多少人力等等，因此对拉格朗日乘子法进行了扩展，增加了KKT条件，求解不等式约束的优化问题；使用拉格朗日函数对目标函数进行了处理，生成了一个新的目标函数。通过一些条件可以求出最优值的必要条件，这个条件就是KKT条件。经过拉格朗日函数处理之后的新目标函数，保证原目标函数与限制条件有交点，也就是必须要有解"
    },
    {
        "title": "异常值",
        "content": "定义：异常值是偏离整体样本的观察值，也是偏离正常范围的点\n影响：异常值会影响模型的精度，降低模型的准确性。增加了整体数据方差；异常值是随机分布的，可能会改变数据集的正态分布。增加因此异常值处理是数据预处理中重要的一步，异常检测场景：入侵检测、欺诈检测、安全监测等\n出现原因：1、数据输入错误，相关人员故意或者无意导致数据异常，比如客户年收入13万美元，数据登记为130万美元。2、数据测量，实验误差，比如测量仪器不精准导致数据异常。3、数据处理错误，比如ETL操作不当，发送数据异常。4、抽样错误，数据采集时包含了错误数据或无关数据。5、自然异常值，非人为因素导致数据异常，比如今年某月份的降水量远超前几年同月份降水量。\n异常值检测方法：1、散点图：将数据用散点图可视化出来，可以观测到异常值。2、基于分类模型的异常检测：根据现有数据建立模型，然后对新数据进行判断从而确定是否偏离，偏离则为异常值，比如贝叶斯模型，SVM模型等。3、3sita原则：若数据集服从期望为u，方差为σ^2的正态分布，异常值被定义为其值与平均值的偏差超过三倍标准差的值。4、箱型图分析：箱型图分为上界，下界，上四分位数，下四分位数，以及离群点   四分位数就是将所有数值按从小到大排列并分成四等份，处于三个分割点位置的数值就是四分位数。（上界:上限是非异常范围内的最大值；下界:下限是非异常范围内的最小值；上四分位数:数值排序后处于75%位置上的值；下四分位数:数值排序后处于25%位置上的值）\n异常值处理办法：1、删除异常值 -适用于异常值较少的情况。2、将异常值视为缺失值，按照缺失值的处理方法处理异常值。3、估算异常值，均值、中位数、众数填充异常值"
    },
    {
        "title": "训练集，测试集，验证集合",
        "content": "在训练过程中使用验证数据集来评估模型并更新模型超参数，训练结束后使用测试数据集评估训练好的模型的性能\n训练集：是用来构建机器学习模型，用于模型拟合的数据样本；测试集：用来评估训练好的模型的性能；验证集：辅助构建模型，用于构建过程中评估模型，为模型提供无偏估计，进而调整模型超参数和用于对模型的能力进行初步评估。通常用来在模型迭代训练时，用以验证当前模型的泛化能力（准确率，召回率等），以决定是否停止继续训练\n划分方法：（1留出法）直接将数据集划分为互斥的集合，一个用作训练集，一个用作测试集，且满足训练集U测试集=全集；训练集交测试集=空集  常见的划分为2/3-4/5的样本用作模型训练，剩下的用作测试。通常选择70%的数据作为训练集，30%的数据作为测试集；通常单次使用留出法得到的结果不够稳定可靠，一般采取分层抽样或者若干次随机划分作为留出法的评估结果\n（2自助法）给定包含M个样本的数据集，每次随机从中抽取一个样本，放入新的数据集中，然后从原始样本中有放回的抽取M次，就可以得到包含M个样本的数据集，平均情况下会有63.2%的原始样本出现在数据集中，而剩下的36.8%的原始样本不出现在数据集中，也称.632自助法\n（3交叉验证）将数据集划分为K个大小相同的互斥子集，同样保持数据分布的一致性，即采用分层抽样的方法获得这些子集。目的：在实际训练中，模型的结果通常对训练数据好，但是对训练数据以外的数据拟合程度较差，交叉验证的目的就是用作评价模型的泛化能力，从而进行模型选择\nK-折交叉验证：每次用K-1个子集的并集作为训练集，余下的那个子集作为测试集，这样就会产生K种数据集划分的情况，从而可以进行K次训练和测试，最终返回K次测试结果的均值\nK折交叉验证的K最常用的取值是10，此时为10折交叉验证，常用的还有5、20等\n股用于模型调优，找到使得模型泛化性能最优的超参数，在全部训练集上重新训练模型，并使用独立测试集对模型性能做出最终评价\n如果训练集相对较小，则增大K值：增大K的话，在每次选代过程中将会有更多的数据参与模型的训练，能够得到最小偏差，同时算法时间也会变长，且不同训练集间高度相似，会导致结果方差较高\n如果训练集相对较大，则减小K值：减小K的话，降低模型在数据集上重复拟合的时间和成本，在平均性能的基础上获得模型的准确评估"
    },
    {
        "title": "特征选择",
        "content": "为什么要进行特征选择：背景：现实中大数据挖掘任务，往往属性特征过多，而一个普遍存在的事实是，大数据集带来的关键信息之聚集在部分或少数特征上，因此需要从中选择出重要的特征使得后续建模过程只在一部分特征上建立，减少维数灾难出现的可能，同时去除不相关的特征，留下关键因素，降低学习的任务难度，更容易挖掘数据本身带有的规律，同时在特征选择的过程中，会对数据的特征有更充分的理解\n如何进行特征选择：当数据预处理完成后，需要选择有意义的特征进行模型训练，通常从三方面考虑：1、特征是否发散:如果一个特征不发散，例如方差接近于0，也就是说样本在这个特征上基本没有差异，这个特征对样本的区分作用不明显，区分度不高。2、待征之间的相关性:特征与待征之间的线性相关性，去除相关性高的特征。3、特征与目标之间的相关性:与目标相关性高的特征，应当优先选择\n三种常见特征选择方法对比：1、过去由于和特定的学习器无关，计算开销小，泛化能力强于后两种特征选择方法，因此，在实际应用中由于数据集很大，特征维度高，过港式特征选择应用的更广泛些，但它是通过一些统计指标对特征进行排字来选择，通常不能发现冗余。2、从模型性能的角度出发，包装法的性能要优于过考去，但时间开销较大。3、嵌入法中的L正则化方法对于特征理解和特征选择来说是非常强大的工具，它能够生成稀疏的模型，对于选择待征子集来说非常有用。4、嵌入法中的随机森林方法是当前比较主流的方法之一，它易于使用，一般不需要其他特征工程操作、调参等繁琐的步亲，有直接的工具包都提供平均不纯堂下降方法，它的两个主要问题：一是重要的特征有可能得分很低，二是这种方法对特征变量类别多的特征有利。"
    },
    {
        "title": "特征选择-Filter过滤法",
        "content": "按照特征发散性或者相关性对各个特征评分，设定阈值或待选择阈值的个数，选择特征；总的缺点是：若特征之间具有强关联，且非线性时，Filter方法不能避免选择的最优特征组合冗余。   Filter总结：利用不同的打分规则，对每个特征进行打分，相当于给每个特征赋予权重，按权重排序，对不达标的特征进行过滤\n1、方差选择法：方差越大的特征，对于分析目标影响越大，就越有用；如果方差较小，比如小于一，那么这个特征可能对算法的作用就比较小；如果某个特征方差为0，即所有的样本在这个特征上的取值都是一样的，那么对模型训练没有任何作用，可以直接舍弃（特征的方差越大越好）    ；实现方法：设定一个方差的阈值，当方差小于这个阈值的特征就会被删除；适用场景：只适用于连续变量。\n2、相关系数法（皮尔逊相关系数）：该方法衡量的是变量之间的线性相关性，两个变量之间的皮尔逊相关系数定义为两个变量之间的协方差与标准差的商，结果的取值区间为【-1，1】，-1表示完全的负相关，+1表示完全正相关，0表示没有线性相关    实现方法：R为相关性，P为显著性，首先看P值，P值用于判断R值，即相关系数有没有统计学意义，判断标准一般为0.05，当P值>0.05时，则相关性系数没有统计学意义，此时无论R值大小，都表明两者之间没有相关性；当P值<0.05，则表明两者之间有相关性，此时再看R值，R值越大相关性越大，正数则正相关，负数就是负相关   ；适用场景：适用于特征类型均为数值特征的情况；缺陷：只对线性关系敏感，如果特征与响应变量的关系是非线性的，即便两个变量具有一 一对应的关系，相关系数也可能会接近0。建议最好把数据可视化出来，以免得出错误的结论。\n3、卡方检验(𝜒^2检验)：它可以检验某个特征分布和输出值分布之间的相关性，𝝌^𝟐值描述了自变量与因变量之间的相关程度，𝜒^2值衡量实际值与理论值的差异程度，𝜒^2值越大，相关程度也就越大   实现方法：1、计算无关性假设(随机抽取一条实际值计算)   2、根据无关性假设生成新的理论值四格表   3、根据计算公式算出𝜒^2   4、计算该相依表的自由度，查询卡方分布的临界值表来判断𝜒^2值是否合理(需要用100%-卡方分布临界表的值才能得出相关性)\n4、互信息法：互信息表示两个变量是否有关系以及关系的强弱。互信息可以理解为，X引入导致Y的熵减小的量，从信息熵的角度分析特征和输出值之间的关系评分；互信息值越大，说明该特征和输出值之间的相关性越大，越需要保留；缺陷：它不属于度量方式，也没有办法归一化，在不同数据集上的结果无法做比较    2、对于连续变量通常需要先离散化，而互信息的结果对离散化的方式敏感\n"
    },
    {
        "title": "特征选择-wrapper包装法",
        "content": "根据目标函数(通常是预测效果评分)，每次选择若干特征，或者排除若干特征\n常用技术 -- 递归特征消除法RFE；\n使用一个基模型来进行多轮训练，每轮训练后，移除若干权值系数的特征，在基于新的特征集进行下一轮训练。  步骤：1、指定一个有N个特征的数据集 2、选择一个算法模型  3、指定保留特征的数量K(K<N)  4、第一轮对所有特征进行训练，算法会根据基模型的目标函数给出每个特征的评分或排名，将最小得分或排名的特征移除，这时候特征减少为N-1，对其进行第二轮训练，持续迭代，直到特征保留为K,这K个特征就是选择的特征\nRFE的稳定性很大程度上取决于在迭代时底层所采用的模型。例如，假如RFE采用的普通的回归，没有经过正则化的回归是不稳定的，那么RFE就是不稳定的；假如采用的是Ridge，而用Ridge正则化的回归是稳定的，那么RFE就是稳定的。"
    },
    {
        "title": "特征选择-Embedded嵌入法",
        "content": "先使用某些机器学习的算法或模型进行训练，得到各个特征的权值系数，根据系数从大到小选择特征\n（1基于L1正则化方法）：1、L1正则化将回归系数的L范数作为惩罚项加到损失函数上，由于正则化非0，这就使得那些弱的待征所对应的系数变为0，因此比L1正则化往往会使学习到的模型很保统(系数w经常为0)这个特性使得L1正则化成为一种很好的特征选择的方法。2、L1正则化像非正则化线性模型一样也是不稳走的，如果待征集合中具有相关联的持征，当数据发生轻微变化时，也有可能导致很大的差异。3、L1正则化能够生成稀疏的模型。4、L1正则化可以产生稀疏权值短阵，即产生一个稀疏模型，可以用于特征选择，也可以防止过拟合\n数据稀疏性说明:L1正则化本来就是为了降低过拟合风险，但是L1正则化的结果往往会得到稀疏数据，即L1方法选择后的数据拥有更多0分量，所以被当作特征选择的强大方法。稀疏数据对后续建模的好处:数据集表示的矩阵中有很多列与当前任务无关，通过特征选择去除这些列，如果数据備疏性比较突出，意味着去除了较多的无关列，模型训练过程实际上可以在较小的列上进行，降低学习任务的难度，计算和存储开销小，\n（2基于树模型方法）：基于树的预测模型能够用来计算特征的重要程度，因此能用来去除不相关的特征，随机森林具有准确率高，稳定性强、易于使用的优点，是目前最流行的机器学习算法之一，随机森林提供的特征选择方法：平均不纯度减少和平均精度下降。（一）、平均不纯度减少：1、随机森林由多个决策树构成，决策树的每一个节点都是关于某个特征的条件，目的时将数据集按照不同的取值一分为二。对于一个决策树森林来说，可以算出每个特征平均减少了多少不纯度，并把它平均减少的不纯度作为特征选择的值  2、若特征之间存在关联，一旦某个特征被选择后，其他特征的重要度就会急剧下降，而不纯度已经被选中的那个特征降下来，其他特征就很难在降低那么多不纯度。在理解数据时，容易错误的认为先被选中的特征是很重要的，而其余的特征是不重要的，但实际上这些特征对响应变量的作用可能非常接近；。（二）、平均精度下降：该方法直接度量每个特征对模型的精确率的影响，基本思路是打乱每个特征的特征值顺序，并且度量顺序变动对模型精确率的影响，因为对于不重要的变量，打乱其顺序对模型的准确率影响不会太大，对于重要的变量，打乱顺序就会降低模型的准确率；在特征选择上，需要注意：尽管在所有特征上进行了训练得到模型，然后才得到每个特征的重要性测试，这并不等于筛选掉某个或某几个重要特征后模型的性能就一定会下降很多，因为即便删掉某个特征后，其关联特征一样可以发挥作用，让模型性能不变"
    },
    {
        "title": "特征处理-特征缩放(数值归一化)",
        "content": "特征缩放可以提高模型的精度和模型的收敛速度，在实际业务中，当数据的量纲不同、数量级差距大时，会影响最终的模型，因此需要用到特征缩放\n1、标准化（常用）：概念:将训练集中的某一列特征缩放到均值为0，方差为1的状态；特点:标准化对数据进行规范化，去除数据的单位限制，将数据转换为无量纲的纯数值，便于不用单位的数据进行比较和加权，同时不改变数据的原始分布状态，标准化要求原始数据近似满足高斯分布，数据越接近高斯分布，标准化效果越佳；适用场景:数据存在异常值和较多的噪音值\n2、最小值-最大值归一化：概念:将训练集中某一列特征数值缩放到-1到1，或0到1之间；特点:受训练集中最大值和最小值影响大，存在数据集中最大值与最小值动态变化的可能；适用场景:数据较为稳定，不存在极端的最大值和最小值\n3、均值归一化：公式：(x-x均值)/max(x)-min(x)\n4、缩放成单位向量：公式：x/||x||\n5、基于树的方法是不需要特征归一化或标准化，比如随机森林，bagging 和 boosting等。基于参数的模型或基于距离的模型，需要特征归一化或标准化。\n"
    },
    {
        "title": "特征处理-数值离散化",
        "content": "概念:把无限空间中的有限个体映射到有限空间中去，提高算法的时空效率，简单讲就是在不改变数据相对大小的情况下，对数据进行相应缩小;离散化仅适用于只关注元素之间的大小关系而不关注元素数值本身的情况 在数据挖掘理论研究中，研究表明离散化数值也能在提高建模速度和提高模型精度上有显著作用；\n作用:离散化可以降低特征中的噪音节点，提升特征的表达能力但是离散化的过程都会带来一定的信息丢失\n（一）、数值变量分离散变量和连续变量\n连续变量中的有监督连续变量离散方法包括1R（把连续的区间分成小区间，然后根据类标签对区间内变量调整）、基于信息熵（自顶向下的方法，运用决策树理念进行离散化）、基于卡方（自底向上的方法，运用卡方检验的方法，自底向上合并数值进行有监督离散化，核心操作是merge）；\n连续变量中的无监督连续变量离散方法包括聚类划分（使用聚类算法将数据分为K类，需要制定K值的大小）；分箱-数据先排序（1等宽划分-把连续变量按照相同的区间间隔划分成几等份，也就是根据数值的最大值和最小值进行划分，分为N份，每份的数值间隔相同；2等频划分-把连续变量划分成几等份，保证每份数值个数相同）\n（二）、分类变量分有序分类变量和无序分类变量\n有序分类变量离散化方法包括Label-Encoding（有序分类变量数值之间存在一定的顺序关系，可直接使用划分后的数据进行数据建模。优点:解决了分类变量的编码问题。缺点:可解释性差）\n无序分类变量离散化方法包括独热编码（使用M位状态寄存器对M个状态进行编码，每个状态都有独立的寄存器位，这些特征互斥，所以在任意时候只有一位有效，也就是说这M位状态中只有一个状态位值为1，其他都是0，换句话说就是M个变量用M维表示，每个维度的数值为1或为0。；优点:解决了分类器不好处理分类变量的问题。；缺点:分类变量不宜过多，可能会造成稀疏矩阵）哑编码（哑编码和独热编码类似，唯一区别就是哑编码采用M-1位状态寄存器对M个状态进行编码。；优点:解决了分类器不好处理分类变量的问题。；缺点:分类变量不宜过多，可能会造成稀疏矩阵）"
    },
    {
        "title": "特征处理-特征编码",
        "content": "数据挖掘中，一些算法可以直接计算分类变量，比如决策树模型，但许多机器学习算法不能直接处理分类变量，他们的输入和输出都是数值型数据，因此把分类变量转换成数值型数据是必要的，可用独热编码和哑编码实现\n应用场景1、对逻辑回归中的连续变量做离散化处理，然后对离散特征进行独热编码或者哑编码，这样会使模型具有较强的非线性能力\n应用场景2、对于不能处理分类变量的模型，必须要先使用独热编码或哑编码，将变量转换成数值型；但若模型可以处理分类变量，那就无须转换数据，如树模型"
    },
    {
        "title": "用户画像",
        "content": "用户画像：即用户信息标签化，就是收集这个用户的各种行为和数据，从而分析得到这个用户的一些基本的信息和典型特征，最后形成一个人物原型，从中挖掘用户价值，从而提供业务推荐、精准营销等服务\n用户画像构建流程：1、收集用户相关数据：可以来源于网站上用户交易数据，用户日志数据，用户行为数据等；。\n2、数据预处理：对收集到的用户数据做一些预处理工作，包括检查数据是否完整，即数据是否含有缺失值，数据是否含有异常值以及数据是否存在不均衡现象，从而决定是否对缺失值填充，或者删除异常值，对不均衡数据进行重采样等数据清洗工作。最终将杂乱无章的数据转化为结构化数据，即我们后续机器学习模型可以识别的数据格式。；。\n3、用户行为数据建模：对于不携带标签的用户数据，也就是我们事先并不知道该用户属于哪一类别，或者说他和哪些用户行为数据比较相似，此时我们可以采用无监督学习中的聚类算法来对它们进行聚类，相似相近、关联性大的用户数据放在一起，不相似不相近，关联性不大的用户不放在一起，可以用这些聚类算法，针对不同形状的数据进行相应的聚类，此时聚类结束后，我们可以用相应的聚类评价指标来评价我们聚类模型性能的优劣，或者将聚类结果与专家给出的标准做对比，将聚类结果回归到真实商业逻辑上。对聚类后不同类别的用户进行打标签，从而得到带有标签的用户数据，对于携带标签的数据，我们接着选择机器学习中有监督学习的相关算法来对这些有标签的数据进行统计分析。\n4、举例：例如某银行推行一种信用卡，该银行的目标就是想知道哪些客户群体会办理信用卡，哪些客户群体不会办理信用卡，所以我们可以通过收集关于用户办理信用卡相关的数据，比如年龄、性别、婚姻状况、名下是否有房产、月收入年收入、之前是否办理过信用卡等等信息，通过聚类算法将用户分为三类群体，即会办理、不会办理以及未知三种客户群体，银行可据此向会办理信用卡的客户群体推送办卡相关业务以及活动福利，实现业务推荐和精准营销。最终我们可以通过现有的携带标签的用户数据，构建机器学习分类模型，例如LR、SVM等算法模型，然后通过该模型对未知用户进行预测，预测该用户是否会办理信用卡"
    },
    {
        "title": "无监督-聚类",
        "content": "无监督：是指在未加入标签的数据中，根据数据之间的属性特征和关联性对数据进行区分，相似相近、关联性大的数据放在一起，不相似不相近的、关联性不大的数据不放在一起；无监督本质：利用无标签的数据去学习数据的分布和数据与数据之间的关系；无监督算法包括如聚类算法、关联算法。\n分类：基于原型（K-means算法、K-means++算法、k-mediods算法）基于层次（HierarchicalClustring算法、Birch算法）基于密度（DBSCAN算法）\n（1）、K-means算法：思想：输入聚类个数K，已经包含n个数据样本的数据集，输出标准的K个聚类的算法，然后将n个数据样本划分为K个聚类，最终结果所满足：同一聚类中数据相似度较高，而不同聚类的数据相似度低；步骤：1、随机选取K个对象作为初始质心  2、计算样本到K个质心的欧氏距离，按就近原则将它们划分到距离最近的质心所对应的类中   3、计算各类别中所有样本对应的均值，将均值作为新的质心，计算目标函数  4、判断聚类中心或目标函数是否改变，若不变则输出，若改变，则返回2；；；；；\nK-means算法核心问题：K值如何选取：1、人工指定：多次选取K值，选择聚类效果最好的K值。2、均方根：假设我们有m个样本，该方法认为K=根号下m/2。3、枚举法：计算类内距离均值与类间距离均值之比，选择最小的K值，将所有K值进行二次聚类，选择两次聚类结果最相似的K值。4、手肘法：随着K值得增大，样本划分越来越精细，聚类得程度越来越高，误差平方和SSE便逐渐减小(误差平方和是所有样本的聚类误差，代表聚类效果的好坏，SSE越小越好)，当K小于真实聚类数时，随着K值的增大会大幅增加每个簇的聚合程度，SSE的下降幅度也会很大，当K等于真实聚类数时，随着K值的继续增大，聚合程度也会迅速减小，SSE的下降幅度便会骤减，然后随着K的继续增大而趋于平缓，所以K值和SSE的关系图就像一个手肘的图形，肘部对应的值便是数据真实聚类的个数\n（2）、K-means++：背景：为了解决K-means初始质心敏感的问题。；技术原理：不同于K-means算法是第一次随机选取K个样本作为初始质心，K-menas++是假设已经选取了p个初始质心，只有在选取第p+1个质心时，距离这p个质心越远的点会有更高的概率当选为第p+1个聚类中心(为了避免异常点的存在，第二个点的选择会从距离较远的几个点中通过加权选取第二个点)，只有在选取第一个聚类中心时是随机选取(p=1)，该方法的改进符合一般直觉：聚类中心之间距离的越远越好\n（3）、K-mediods：能够避免数据中异常值的影响；算法步骤：1、随机选取一组样本点作为中心点集  2、每个中心点对应一个簇 3、计算各样本点到各个中心点的距离，将样本点放入距离最近的中心点的类中。  4、计算各簇距簇内各样本点的距离绝对误差最小的点，作为新的聚类中心    5、如果新的聚类中心与原中心点相同，则过程结束，如果不同，则返回2\nK-mediods与k-means算法对比：1、算法流程基本一致。；2、质心的计算方式不同:k-means算法是将所有样本点对应的均值作为新的中心点，可能是样本中不存在的点 K-mediods是计算簇内每一个点到簇内其他点的距离之和，将绝对误差最小的点作为新的聚类中心，质心必须是某个样本点的值。；3、k-mediods可以避免数据中异常值带来的影响。；4、质心的计算复杂度更高:k-means直接将均值点作为新的聚类中心，而k-mediods需要计算簇内任意两点之间的距离，在对每个距离进行比较狭取新的质心，计算复杂度增加，速度变慢。；5、稳定性高，执行速度变慢:在具有异常值的小样本数据集中，k-mediods算法比k-means算法效果好，但是随着数据集规模的增加，k-mediods算法执行的速度会很慢，所以如果数据集本身不存在很多的异常值的话，就不用k-mediods代者k-means。；\n（4）、Hierarchical Clustering算法：思想:确保距离近的样本落在同一个族中\n步骤:1、每个样本点都作为一个簇，形成族的集合C 。2、将距离最近的两个簇合并，形成一个簇3、从C中去除这对簇。 4、最终形成层次树形的聚类结构树形图（判断两个簇之间的距离方法：1单链接 -- 不同两个簇之间最近的两个点的距离；2全链接-- 不同两个簇之间最远的两个点的距离；3均链接 -- 不同两个簇中所有点两两之间的平均距离）\n优点:可排除噪声点的干扰，但有可能和噪声点分为一簇 2、适合形状不规则，不要求聚类完全的情况 3、不必确定K值，可根聚类结果不同有不同的结果 4、原埋简单，易于理解\n缺点:计算量很大，耗费的存储空间相对于其他几种方法要高。 2、合并操作不能撤销 3、合并操作必须有一个合并限制比例，否则可能发生过度合并导致所有分类中心聚集，造成聚类失败\n适用场景:适合形状不规则，不要求聚类完全的情况\n（5）、Birch：使用聚类特征三元组表示一个簇的有关信息，而不用且体的一组点来表示该簇，通过构造满足分支因子和簇直径限制的聚类特征树来进行聚类三元组(数据点样本个数，数据点样本特征之和，数据点样本特征平方和)，分支因子:树的每个节点的样本个数，簇直径:一类点的距离范国\n算法步骤:1、扫描数据，建立聚类特征树 2.使用某种算法对聚类特征树的叶节点进行聚类\n优点:一次扫描就能进行很好的聚类\n缺点:要求是球形聚类，因为CF树存储的都是半径类的数据，都是球形才适合\n适用场景:因为Birch算法通过一次扫描就可以进行比较好的聚类，所以适用于大数据集，而且数据的分布呈凸型以及球形的情况，并且由于Birch算法需要提供正确的器类个数和簇直径限制，对不可视的高维数据不可行\n（6）、DBSCAN：一个聚类可以由其中任何核心对象唯一确定，该算法利用基于密度的概念，要求聚类空间中某一区域内的样本个数不小于某一给定闻值，该方法能够在具有噪声的空间数据库中发现任意形状的簇，可将密度足够大的相邻区域连接，能够有效处理异常数据，主要用于对空间教据的聚类。\n步骤:1、DBSCAN通过检查数据中每个样本的eps邻域来搜索簇，如果点p的eps邻域内包含的样本个数大于给定的闽值，那么就建立一个以点P为核心对象的簇。\n2、然后DBSCAN送代的聚集这些核心对象直接密度可达的对象，这个过程可能还会涉及密度可达簇的合并\n3、当没有新的样本点加入到簇中时，过程结束"
    },
    {
        "title": "数据属性",
        "content": "标称属性：标称，意味着与名称有关，标称属性的值是一些符号或事物的名称，每个值代表事物的某种类别、状态或编码。因此标称属性又被看做是分类的。标称属性的值不具有有意义的顺序，而且是不定量的。也就是说给定一个数据集，找出这种属性的均值没有意义;举例：头发颜色和婚姻状况，头发的颜色={黑色、棕色、红色、白色}；婚姻状况={单身、已婚、离异、丧偶}；其次还有职业，具有教师、程序员、农民等\n二元属性:二元属性是一种标称属性，只有两种状态，用来描述事物的两种状态，用值0或1来体现，0表示该属性不出现，1表示出现。二元属性又称布尔属性，如果两种状态对应的是True和False;对称的二元属性(两种状态具有相等的价值，携带相同的权重，例如性别，具有男女两种状态)非对称的二元属性(其状态的结果不是同等重要的，例如艾滋病化验结果的阳性和阴性，用1对最重要的结果(HIV阳性)编码，而另一个用0编码(HIV阴性))\n序数属性:其可能的值之间具有有意义的序或秩评定，但是相继值之间的差是未知的，也就是对应的值有先后顺序。序数属性可以把数量值的值域划分成有限个有序类别。（如0-很不满意，1-不满意，2-满意，3-很满意），把数值属性离散化得到。可以用众数和中位数表示序数属性的中心趋势，但不能定义均值.;举例：成绩={优、良、中、差}；    drink_size表示饮料杯的大小：大、小、中。\n数值属性:是定量的，即它是可以度量的量，用整数或实数值表示;(1)区间标度属性:用相等的单位尺度度量，区间标度属性的值有序，可以评估值之间的差，但不能评估倍数，没有固有的零点(也就是说0°C并不是指现在没有温度)。举例：摄氏温度、华氏温度，日历日期。不能说2020年是1010年的两倍，两者的差有意义，但是比值没有意义；(2)比率标度属性:具有固有零点的数值属性。比率标度属性的值有序，可以评估值之间的差，也可以评估倍数.举例：开氏温度、重量、高度、速度、货币量。拿货币来说，可以说200元比100元多100元，也可以说200元是100元的两倍\n离散属性：具有有限或无限可数个值，可以用或不用整数表示。例如，属性customer_ID是无限可数的。顾客数量是无限增长的，但事实上实际的值集合是可数的（可以建立这些值与整数集合的一一对应）举例：邮编、省份数目\n连续属性:如果属性不是离散的，则它是连续的。属性值为实数，一般用浮点变量表示。\n总结：（1）标称、二元、序数属性都是定性的，他们只描述对象的特征，而不给出实际大小和数值；（2）标称、二元属性的中心趋势可以用众数度量。序数属性的中心趋势可以用它的众数和中位数度量，但不能定义均值（3）所有属性都能用中心趋势来表述。标称、二元属性用众数度量；序数属性用众数、中位数度量。均值是数值属性的中心趋势描述"
    },
    {
        "title": "降维-PCA，奇异值、主成份分析",
        "content": "思想：降维就是将事物的特征进行压缩和筛选，将原始高维空间中的数据映射到低维空间中，该项任务比较抽象，如果没有特定领域的知识，很难事先决定采用哪些数据。比如在人脸识别任务中，如果直接采用图片的原始像素信息，那么数据的维度是非常大的，所以此时就要用到降维的方法，对图片信息进行处理，选出区分度最大的像素组合\n1、SVD（奇异值分解）：1、奇异值分解可以适用于任意矩阵的一种分解方法 2、奇异值分解可以发现数据中隐藏的特征来建立矩阵行列之间的关系 3、奇异值分解能够发现矩阵中的几余，并提供用于消除它的格式\n优点:原理简单，仅涉及简单的矩阵线性变换知识。可以有效处理数据噪音，矩阵处理过程中得到的三个短阵也具有物理意义\n缺点:分解出的矩阵可解释性差，计算量大\n应用场景:广泛的用来求解线性最小平方、最小二乘问题，低秩通近问题，数据压缩问题，应用于推荐系统(找到用户没有评分的物品，经过SVD压缩后低维空间中，计算未评分物品与其他物品的相似性，得到一个预测打分，再对这些物品评分从高到低排序，返回前N个物品推荐给用户)\n2、PCA（主成份分析法）：思想:寻找表示数据分布的最优子空间(降维，去掉线性相关性)，就是将n维特征映射到k维上(k<n)，k维是全新的正交特征(k维特征称为主成份，是重新构造出来的k维特征，而不是简单的从n维特征中去除n-k维特征)，PCA目的是在高维数据中寻找最大方差的方向，然后将原始数据映射到维数小的新空间中。数学原理:根据协方差矩阵选取前s个最大特征值所对应的特征向量构建映射矩阵，进行降维\n优点:1、方差衡量的无监督学习，不受样本标签的限制 2、各主成份之间正交，可消除原始数据中各特征之间的影响 3、计算方法简单，主要运算是奇异值分解，容易在计算机上实现\n缺点:主成份解释某含义具有一定的模糊性，不如原始样本特征解释性强 2、方差小的非主成份也可能含有重要信息，因降维丢弃后可能对后续数据处理产生影响\n算法流程：1、对所有样本构成的矩阵X去中心化2、求X的协方差矩阵C   3、利用特征值分解，求出协方差矩阵C的特征值和特征向量      4、取前s个最大特征值对应的特征向量构成变换矩阵W    5、用原数据集和变换矩阵W相乘，得到降维后的新数据集（矩阵）\n追问:详细介绍PCA：在PCA中，数据从原始坐标系转换到新的坐标系中，转换坐标系时，选择数据方差最大的方向作为坐标轴的方向，（方差最大的方向给出了数据最重要的信息），第一个坐标轴的方向是方差最大的方向，第二个新坐标轴的方向是与第一个新坐标轴正交且方差次大的方向，重复过程N次，N是数据原始维度，通过这种方式可以获得新的坐标系，其中大部分方差包含在前几个坐标轴中，而后几个坐标轴中方差基本为0，这时可以忽略后面的坐标轴，只保留前面几个包含大部分方差的坐标轴；通过计算协方差矩阵，可以得到协方差矩阵的特征值和特征向量，选取前几个最大特征值所对应的特征向量组成变换矩阵，就可以将矩阵转换到新的坐标系中，实现数据的降维\n3、LDA（线性判别分析）：思想:寻找可分性判据最大的子空间，即投影后类内间距最小，类间距离最大 2、用到了fisher的思想，即寻找一个向量，可以使得类内散度最小，类间散度最大，其实也就是选取特征向量构建映射矩阵，然后对数据进行处理，该方法能使投影后样本的类间散步矩阵最大，类内散步矩阵最小\n优点:降维过程中可以使用类别的先验经验 2、LDA在样本分类信息依赖均值而不是方差的时候，比PCA之类的算法更优\n缺点:LDA不适合对非高斯分布的样本进行降维，PCA也有这个问题 2、LDA在样本分类信息依赖方差而不是均值的时候，降维效果不好 3、LDA可能过度拟合数据 4、LDA降维最多降到类别数k-1的维数，如果我们降维的维度大于k-1，则不能使用LDA\n4、LLE（局部线性嵌入）：相比较于传统的PCA、LDA这种只关注样本方差的降维方法，LLE在保持降维效果的同时也保留了样本局部的线性特征，因为其保留了局部的线性特征，所以常被应用在高维数据可视化、图像识别等领域\n5、LDA VS PCA：共同点：1、都属于线性方法。2、在降维时都采用矩阵分解的方法。3、假设数据符合正态分布。；不同点：1、LDA是有监督的，不能保证投影到新空间的坐标系是正交的，选择分类性能最好的投影方向。2、LDA可以用于降维也可以用于分类。3、LDA降维保留个数与其对应的类别数有关，与数据本身维度无关\n奇异值分解定义：将矩阵分解为奇异向量和奇异值，可以将矩阵A分解为三个矩阵的乘积，即A=UDV^t，其中矩阵U和V为正交矩阵，U的列向量为左奇异向量，V的列向量为右奇异向量，D矩阵的对角线上的值为矩阵A的奇异值，D矩阵为对角矩阵，奇异值按大小进行排序，对角线之外的值为0\n奇异值分解应用：1、在机器学习和数据挖掘领域，奇异值的应用很广泛，比如应用在用于降维的PCA(主成份分析法)和LDA(线性判别分析法)，数据压缩（以图像压缩为代表）算法，还有做搜索引擎语义层次检索的LSI。；2、求矩阵伪逆：奇异值分解可以被应用于求矩阵的伪逆，若矩阵M的奇异值分解为M=UDV^t，那么M的伪逆为M=VD^+U^t，其中D+为D的伪逆，并将其主对角线上非零元素求倒数在转置得到的。求伪逆通常可以用来求解线性最小平方、最小二乘法问题。；3、矩阵近似值：奇异值分解在统计中的主要应用为PCA（主成分分析），一种数据分析的方法，用来发现大量数据中的隐含模式，可以用于模式识别，数据压缩等方面.PCA方法的作用是将数据集映射到低维空间中的坐标系中，数据集的特征值(可以用奇异值来表征)按照重要性排列，降维的过程其实就是舍弃不重要的特征向量，而剩下的特征向量组成的空间就是降维后的空间。；4、平行奇异值：用于频率选择性的衰落信道的分解"
    },
    {
        "title": "数据挖掘流程",
        "content": "数据挖掘是通过对大量的数据进行分析，以发现和提取隐含在其中的具有价值的信息和知识的过程。跨行业数据挖掘标准流程，是当今数据挖掘业界通用流行的标准之一，它强调数据挖掘技术在商业中的应用，是用以管理并指导Data Miner有效、准确开展数据挖掘工作获得最佳挖掘成果的一系列工作步骤的规范标准\n1、商业理解：从商业角度理解项目的目标和要求，然后把理解转化为数据挖问题的定义和一个旨在实现目标的初步计划。确定业务目标：分析项目背录，从业务角度分析项目的需求和目标，确定业务角度成功标准。；项目可行性分析：分析现有资源、条件和限制，风险估计、成本和效益估计。；确定数据控掘目标：明确数据挖狂的目标和成功标隹。；提出项目计划：对整个项目做出计划，初步估计用到的工具和技术。\n2、数据理解：开始于原始数据的收集，然后是熟悉数据，表明数据质量问题，探索数据进而对数据初步理解，发掘有趣的子集以形成对隐藏信息的假设。收集原始数据：收集项目所涉及到的数据，如有必要，将数据传入数据处理工具中，并做一些初步数据集成的工作，生成相应报告。；描述数据：对数据做一些大致描述，例如属性数、记录数等，生成报告。；探索数据：对数据做简单的统计分析，例如关键属性的分布。；检查数据质量：包括数据是否完整、是否有错误，是否会有缺失值等\n3、数据准备：从原始未加工的数据构造最终数据集的过程(这些数据指将要嵌入建模工具中的数据)，数据准备任务可能要实施多次，并且没有任何规定的顺序。这些任务包括表格、属性和记录的选择以及按照建模工具要求，对数据进行转换、清洗等。；数据选择：根据数据挖掘目标和数据质量选择合适数据，包括表格选择、属性选择和记是选择。；数据清洁：对选择出的教据进行清洁，提高数据质量，例去除噪音、填充缺失值等。；数据创建：在原始数据上生成新的记录或属性。；数据合并：利用表连接等方式将几个数据集合并在一起。；教据格式化：将数据转化为数据挖掘处理的格式。\n4、建立模型：选择和应用各种建模技术，同时对他们的参数进行调整，以达到最优值。选择建模技术 -- 确定数据挖症算法模型和参数；测试方案设计：设计某测试模型的质是和有效性机制模型训练：在准备好的数据集上运行数据挖掘算法，得出一个或多个模型模型测试评估：根据测试方案，从数据挖狂技术角度确定数据挖掘目标是否成功\n5、模型评估：更为彻底的评估模型和检查建立模型的各个步骤，从而确保它真正的达到了商业目标。结果评估：从商业角度评估得到的模型，甚至实际试用该模型测试其效果；过程回顾：回顾项目的所有流程，确定每一个阶段都没有错误；确定下一步工作：根据结果评估和过程回顾得出结论，确定部晋该控掘模型还是从某个阶段重新开始\n6、模型实施：实施计划 ：在业务运作中部署模型做出计划；监控和维护计划：如何监控模型在实际业务中的使用情况，如何维护该模型；作出最终报告 ：项目总结，项目经验和项目结果；项目回顾：回顾项目的实施过程，总结经验教训;对数据挖握的运行效果做一个预测"
    },
    {
        "title": "数据采集抽样方法",
        "content": "简单随机抽样：先将调查总体进行编号，然后根据抽签法或者随机数字表抽取部分所观察到的数据组成样本数据，分为有放回抽样和无放回抽样。常被应用于压缩数据量以减少费用和时间开销\n系统抽样：又被称为等距抽样，首先设定抽样间距为n，然后从前n个数据样本中抽取初始数据，然后按照顺序每隔n个单位抽取一个数据组成样本数据\n分层抽样：将总体数据按照特征划分为若干层次或类型，然后从各个类型和层次的数据中采用简单随机抽样或者系统抽样抽取子样本，最终将这些子样本组合为总体数据样本，常被应用于离网预警模型和金融欺诈模型等严重有偏的数据\n整群抽样：将总体数据按照属性拆分为互不相交、互不重复的群，这些群中的数据尽可能具有不同属性，尽量能代表总体数据的信息，然后以群为单位进行抽样"
    },
    {
        "title": "数据清洗",
        "content": "定义：数据清洗是指通过删除、转换、组合等方法或策略清洗数据中的异常样本，为数据建模提供优质数据的过程；场景：不均衡数据处理、缺失值处理、异常值处理\n1.不均衡数据处理：类别数据不平衡是分类任务中出现的经典问题，一般在数据清洗环节进行处理，不均衡简单来讲就是数据集中一个类别的数据远超其他类别数据的数据量，比如在1000条用户数据中，男性数据占950条，女性数据只占50条；处理办法1重采样数据（过采样：对少的一类进行重复选择，欠采样：对多的一类进行少量随机选择）2.K-fold交叉验证3.一分类4.组合不同的重采样数据集（核心原理：建立N个模型。    假设稀有样本有100个，然后从丰富样本中抽取1000(100*10)个数据，将这1000个分为N份，分别和100个稀有数据合并建立模型）\n2.异常值处理：异常值指偏离正常范围的值，不是错误值，异常值出现频率较低，但又会对实际项目分析造成影响；检测方式：异常值一般通过箱型图或分布图来判断；处理办法：采取盖帽法或者数据离散化   2、删除异常值、使用与异常值较少的时候  3、将异常值视为缺失值，按照缺失值的处理方法处理   4、估算异常值，mean/mode/median\n3.缺失值处理：数据缺失产生原因：1、人为疏忽、机器故障等客观原因造成数据缺失   2、人为故意隐瞒部分数据，比如在数据表中有意将一列属性视为空值，此时缺失值可以被看作为特殊的特征值   3、数数据本身不存在，例如银行在做用户信息收集时，对于学生群体来说，薪资这一列就不存在，所以在数据集中显示为空值   4、系统实时性要求较高  5、历史局限性导致数据收集不完整；；；.缺失值处理方法包括1.删除（适用场景：数据量大，缺失值少的数据集，完全随机缺失时可以直接使用删除操作）2.填充。3.不处理（补齐得缺失值毕竟不是原始数据，所以不一定符合客观事实，数据填充在一定程度上改变了数据的原始分布，也不排除加入噪音点的可能，因此对于一些无法容忍缺失值的模型可以进行填充，但有些模型本身可以容忍一定的数据缺失，此时选用不处理的方式，比如XGboost模型）\n缺失值填充方法包括如下4种：（1）.数值填充：众数填充-以类别数据量较多的类别填充，适用于数据倾斜时。；均值、中位数填充：以所有非缺失值的Mean或Median填充缺失值(广义插补)   2、分类别计算非缺失值的Mean或Median填充各类别的缺失值(相似填充)；均值填充：适用于数据中没有极端值的场景；中位数填充：适用于数据中有奇异值的场景\n（2）KNN：通过KNN算法将所有样本进行划分，通过计算欧氏距离，选取与缺失数据样本最近的K个样本，然后通过投票法或者K个值加权平均来估计该缺失样本的缺失数据；优点：不需要为含有缺失值的每个属性都建立预测模型  2、一个属性有多个缺失值时也可以很好解决   3、缺失值处理时把数据结构之间的相关性考虑在内；缺点：面对大数据集，KNN时间开销大  2、K值得选择是关键，过大过小都会影响结果。\n（3）回归：把数据中不含缺失值得部分当作训练集，建立回归模型，将此回归模型用来预测缺失值；适用场景：只适用缺失值是连续得情况；优缺点：预测填充理论上比值填充效果好，但如果缺失值与其他变量没有关系，那么预测出得缺失值没有意义\n（4）变量映射：把变量映射到高维空间中，优点：可以保留数据得完整性，无需考虑缺失值，缺点：1、计算开销增加  2、可能会出现稀疏矩阵，影响模型质量。"
    },
    {
        "title": "总结-机器学习，有监督，无监督",
        "content": "（一）、有监督学习：\n1.逻辑回归:\n优点:速度快,适合二分类问题;简单易懂,直接看到各个特征的权重;能容易地更新模型吸收新的数据.\n缺点:容易欠拟合,一般准确度不太高;不能很好地处理大量多类特征或变量;只能处理两分类问题,且必须线性可分.\n适用场景: 用于二分类领域,可以得出概率值.\n2.KNN(K最近邻算法):\n三要素: K值的选取、距离度量方式(欧氏距离、曼哈顿距离)、分类决策规则(多数表决法、平均法).\n优点:理论成熟,思想简单,既可以用来做分类也可以用来做回归;可用于非线性分类;\n对数据没有假设,准确度高,对噪声不敏感.\n缺点:计算量大;样本不平衡问题;需要大量的内存.\n适用场景: 可用于回归,分类,对于类域的交叉或重叠较多的待分样本集来说,KNN方法较其他方法更为适合.\n3.朴素贝叶斯:\n公式: P(Ci | X) = P(X | Ci)P(Ci)/P(X)\n优点:容易实现;对小规模的数据表现好;对缺失数据不大敏感.\n缺点:算法成立的前提是假设各属性之间互相独立.当数据集满足这种独立性假设时,分类度较高.而实际领域中,数据集可能并不完全满足独立性假设；需要计算先验概率；\n适用场景: 数据的各个维度对target的影响不相关或者相关度较小、适合向懂数学的客户解释、联合处理高维的数据,效果不太好.\n4.支持向量机(SVM):\n分类:线性可分:计算分类超平面;线性不可分:通过核函数将数据映射到更高的维度,然后再计算分类超平面.\n核函数: 线性核函数、多项式核函数、高斯核函数、Sigmoid核函数.\n优点:分类效果好;原理简单,有良好的解释性.\n缺点:不适用于大规模数据集;只能用于二分类场景;对缺失值敏感,对核函数的选择敏感.\n适用场景: 一般会应用于数据量较小的二分类场景.\n5.决策树:\n优点：概念简单，计算复杂度不高；可解释性强；能同时处理数据型和常规型属性；对中间值缺失不敏感；应用范围广；可以扩展性很强。\n缺点：完全生长的决策树容易导致过拟合；信息增益来度量会偏向于取值较多的属性；剪枝过程较难控制。\n适用场景：算法可解释性好，算法速度快，对硬件性能要求不高。\n6.随机森林:\n优点：相对于决策树算法具有更好的准确率；构建树的过程中采用并行的方式提高了算法运行效率；能够直接处理高维度数据；不需要提前降维；使用有放回的抽样方式使模型方差小，泛化能力强。\n缺点：取值比较多的特征对随机森林的决策会产生更大的影响；Bagging改进了预测的准确率但损失了解释性；在某些噪音比较大的特征上RF模型还是容易陷入过拟合。\n适用场景：数据维度相对较低，几十维；客户要求高准确性，无需调参就可以达到好的分类效果。\n7.自适应提升(Adaboost):\n优点：作为分类器时，分类精度很高；在AdaBoost的框架下可以使用各种回归分类模型来构建弱学习器非常灵活；构造简单，结果可理解；不容易发生过拟合。\n缺点：对异常样本敏感，异常样本在迭代中可能会获得较高的权重，影响最终的强学习器的预测准确性。\n适用场景：适用于二分类问题。\n8.梯度提升树(GBDT):\n优点：可以灵活处理各种类型的数据；在相对少的调参时间情况下，预测的准备事也可以比较高；使用一些健壮的损失函数，对异常值的鲁棒性非常强；很好的利用了弱分类器进行级联；充分考虑了每个分类器的权重。\n缺点：对异常样本敏感，异常样本在迭代中可能会获得较高的权重，影响最终的强学习器的预测准确性。\n适用场景：几乎可用于所有回归问题（线性/非线性）；也可用于二分类问题（设定阈值，大于阈值为正例，反之为负例）。\n9.朴素贝叶斯(Naive Bayes):\n优点：对缺失值不敏感，可以给缺失值自动划分方向；引入阔值限制树分裂，控制树的规模，防止过拟合问题；分裂数据分开后与未分割前的差值增益，不用每个节点排序算增益，减少计算量，可以并行计算。\n（二）、无监督学习（关联规则和聚类）：\nK-Means算法优点：1.简单、易于理解、运算速度快；2.对处理大数据集，该算法保持可伸缩性和高效性；3.当簇接近高斯分布时，它的效果较好。缺点：1.在 K-Means算法是局部最优的，容易受到初始质心的影Ⅱ向；2.在 K-Means算法中K值需要事先给定的，有时候K值的选定非常难以估计；3.在簇的平均值可被定义的情况下才能使用，只能应用连续型数据；4.大数据情况算法开销是非常大的；5.对噪声和孤立点数据敏感。\nK-mediods算法优点：1.K-mediods算法具有能够处理大型数据集 2.结果能相当紧凑，并且簇与簇之间明显分明 3.相比于K-means对噪声点不敏感。缺点：1.只适用于连续性数据；2.只适用于聚类结果为凸形的数据集等；3.必须事先确定K值；4.一般在获得一个局部最优的解后就停止了。\nK-means与k-mediods的区别：1、与K-means相比，K-mediods算法对于噪声不那么敏感，这样对于离群点就不会造成划分的结果偏差过大，少数数据不会造成重大影响。2、K-mediods由于上述原因被认为是对K-means的改进，但由于按照中心点选择的方式进行计算，算法的时间复杂度也比K-means上升了O(n)。\nDBScan优点：1.可以解决数据分布特殊（非凸，互相包络，长条形等）的情况。2.对于噪声不敏感，速度较快，不需要指定簇的个数；可适用于较大的数据集。3.在邻域参数给定的情况下结果是确定的，只要数据进入算法的顺序不变，与初始值无关。缺点：1.因为对整个数据集我们使用的是一组领域参数，簇之间密度差距过大时效果不好。2.数据集较大的时候很消耗内存。3.对于高维数据距离的计算会比较麻烦，造成维数灾难。\nApriori优点：1.使用先验原理，大大提高了频繁项集逐层产生的效率；2.简单易理解；数据集要求低。缺点：1.每二步产生候选项目集时循环产生的组合过多，没有排除不应该参与组合的元素；2.面对大数据集是，每次扫描比较数据和阈值会大大增加计算机系统的I/O开销。\nFP-growth优点：1.能适应海量数据场景。缺点：1.FP-Growth实现比较困难，在某些数据集上性能会下降；2.FP-Growth适用数据类型：离散型数据。\nApriori与Fp-growth算法对比：ariori算法多次扫描交易数据库，每次利用候选频集集产生频集；而FP-growth则利用树形结构，无需产生候选项集而是直接得到频繁集，大大减少扫描交易数据库的次数，从而提高了算法的效率，但是apriori的算法扩展性好，可以用于并行计算等领域。"
    },
    {
        "title": "总结-PCA与LDA对比",
        "content": "共同点:\nPCA与LDA都属于线性方法。\n两者在降维时都采用矩阵分解的方法。\n假设数据符合正态分布。\n不同点:\nLDA是有监督的，不能保证投影到新空间的坐标系是正交的，选择分类性能最好的投影方向。\nLDA降维保留个数与其对应类别的个数有关，与数据本身的维度无关。（原始数据是n维的，有c个类别，降维后一般是到c-1维）\nLDA可以用于降维，还可用于分类。"
    },
    {
        "title": "大数据ppt",
        "content": "大数据架构是关于大数据平台系统整体结构与组件的抽象和全局描述，用于指导大数据平台系统各个方面的设计和实施。\n一个典型的大数据平台系统架构应包括以下层次：1数据平台层（数据采集、数据处理、数据分析）；2数据服务层（开放接口、开放流程、开放服务）；3数据应用层（针对企业业务特点的数据应用）；4数据管理层（应用管理、系统管理）。\n(1)数据平台层 是大数据体系中最基础和最根本的部分。数据平台层一般包含三个层次。1.数据采集层：包括传统的ETL离线采集、实时采集等。 2.数据处理层：根据数据处理场景要求不同，对采集回来的数据进行一些规范化的预处理。常用处理方式可以分为Hadoop离线处理、实时流处理等。 3.数据分析层：包括传统的数据挖掘和进一步的机器学习、 深度学习等。\n(2)数据服务层是基于数据平台层，以开放接口、开放流程为基础，采用基于云计算的大数据存储和处理架构、分布式数据挖掘算法和基于互联网的大数据存储、处理和挖掘大数据服务模式。\n构建基于服务的大数据分析模式，提供大数据处理和分析的服务功能。\n基于互联网和云计算的大数据存储、处理和挖掘的数据中心系统架构，提供多用户、多任务的大数据分析服务。　\n(3)数据应用层 是各个企业根据自身的具体业务及应用所规划和实施的大数据应用和服务。主要将大数据应用到行业领域，实现基于行业的应用。\n根据企业的特点不同划分不同类别的应用，比如针对运营商，对内有精准营销、客服投诉、基站分析等，对外有基于位置的客流、基于标签的广告应用等等。\n主流的应用层面的技术包括大数据统计、分析、挖掘、展现等等。\n(4)数据管理层包括应用管理和系统管理。\n应用管理主要是从数据设计、开发到数据销毁的全生命周期管理，建立数据标准、质量规则和安全策略等，从而实现从事前管理、事中控制和事后稽核、审计的全方位的数据质量管理，元数据管理和安全管理。\n系统管理主要是将大数据平台纳入统一的云管理平台管理，云管理平台包括支持一键部署、增量部署的可视化运维工具、面向多租户的计算资源管控体系(多租户管理、安全管理、资源管理、负载管理、配额管理以及计量管理)和完善的用户权限管理体系，提供企业级的大数据平台运维管理能力支撑。\n主数据是描述数据产品特征的任何信息，如名字、位置、可感知的、重要性、质量、对企业的价值，以及与企业认为值得管理的其他数据产品的关系等。主数据决定信息架构的如何满足业务需求，因此主数据是数据治理计划的关键。\n\n大数据平台构建完成并投入使用时，可能会面临的问题：\n数据标准缺乏结构化管理；\n源数据变化造成数据平台数据混乱；\n来自组织不同部分的数据在多个报告上的不一致性；\n从数据生命周期角度经常存在的数据存储库、策略、标准和计算流程中的风险。\n\n 数据标准规范化--规范化管理构成数据平台的业务和技术基础设施，包括数据管控制度与流程规范文档、信息项定义等。\n 数据关系脉络化--实现对数据间流转、依赖关系的影响和血缘分析。\n 数据质量度量化--全方位管理数据平台的数据质量，实现可定义的数据质量检核和维度分析，以及问题跟踪。\n组织可通过治理其数据而达到以下目标：\n改进用户对报告的信任级别。\n确保数据在来自组织不同部分的多个报告上的一致性。\n确保恰当地保护企业信息，以满足审计者和监管者的需求。\n改进客户的洞察水平，推动营销计划的实施。\n直接影响组织最关注的 3 个因素：提高收入、降低成本和减少风险。\n将数据视为战略性企业资产，意味着组织需要建立其现有数据的清单，就像建立物理资产的清单一样。\n典型的组织拥有与其客户、供应商和产品相关的过量的信息。这样的组织甚至可能不知道所有这些数据位于何处。\n组织需要防御其财务、企业资源规划和人力资源应用程序中的关键业务数据受到未授权更改，因为这可能影响到其财务报告的完整性，以及日常业务决策的质量和可靠性。\n数据治理是一门将数据视为一项企业资产的学科。它涉及到以企业资产的形式对数据进行优化、保护和利用的决策权利。它涉及到对组织内的人员、流程、技术和策略的编排，以从企业数据获取最优的价值。\n\n\nDUGP(Unified Data Governance Platform)华为大数据统一数据治理平台，为运营商提供全面高效的数据资产管控环境，实现了数据的集中、统一和共享。包括统一的数据采集和整合，统一的安全、标准、生命周期和质量管理, 以及多维度数据云图功能。提供开箱即用的可以实现全生命周期的主数据管理，包括主数据的集中存储、主数据合并、主数据清洗、主数据监管和主数据的共享，满足集团对于企业级别主数据管理平台的需求。\n"
    }
]
    results = []
    for dict_item in json_data:
        results.append({'title': dict_item['title']})
    df = pd.DataFrame(results)
    df.style.set_properties(**{'white-space': 'pre-wrap', 'text-align': 'left'})
    
            

def l_by_t(key):
    json_data=[
    {
        "title": "关联规则",
        "content": "# 读取数据并展示前五行\ndf = pd.read_csv('./data/online_retail_II.csv')\ndf.head()\n1.清理df数据：删除缺失值与重复值。\ndf.dropna(inplace=True)\ndf.drop_duplicates(inplace=True)\nfrom mlxtend.frequent_patterns import apriori, fpmax, fpgrowth\nfrom mlxtend.frequent_patterns import association_rules\n\n# 找到所有支持度超过0.03的项集\nbasket = df.groupby(['Invoice', 'StockCode'])['Quantity'].sum().unstack().fillna(0)\nbasket_sets = basket.applymap(lambda x: 1 if x > 0 else 0)\n使用 fpgrowth算法从数据basket_sets中计算频繁项集，并将最小支持度设置为 0.1。返回频繁项集frequent_itemsets\n\nfrequent_itemsets = fpgrowth(basket_sets, min_support=0.1, use_colnames=True)\nfrequent_itemsets\n使用association_rules从频繁项集frequent_itemsets中构建关联规则，metric为'confidence'， min_threshold为0.4。并将结果存储在rules中\n\nrules = association_rules(frequent_itemsets, metric='confidence', min_threshold=0.4)\n# 展示前五条关联规则\nrules.head()\n4.打印出同时满足置信度< 0.8，置信度>0.5，提升度>3.0的rules数据。\n/*此处由考生进行代码填写*/\nfiltered_rules = rules[(rules['confidence'] < 0.8) & (rules['confidence'] > 0.5) & (rules['lift'] > 3.0)]\n\n# 打印结果\nprint(filtered_rules)\n筛选规则rules，将规则中同时满足antecedent_sele为{'22111', '22271'}，consequent_sele为{'22272'}的规则选出来，并存储在final_sele中\n\n# 筛选 antecedents 和 consequents 满足条件的规则\nfinal_sele = (rules['antecedents'] == {'22111', '22271'}) & (rules['consequents'] == {'22272'})\n# 使用 final_sele 筛选出相应的规则\nrules.loc[final_sele ]"
    },
    {
        "title": "线性代数",
        "content": "生成一个包含整数0-11的向量\nx = np.arange(12)\n查看数组大小\nx.shape\n将x转换成二维矩阵，其中矩阵的第一个维度为1\nx = x.reshape(1,12)\n将x转换3x4的矩阵\nx = x.reshape(3,4)\n生成3*4的矩阵\nA = np.arange(12).reshape(3,4)\n转置\nA.T\n矩阵相乘：两个矩阵能够相乘的条件为第一个矩阵的列数等于第二个矩阵的行数。\nnp.matmul(A,B)\n矩阵对应运算：针对形状相同矩阵的运算统称，包括元素对应相乘、相加等，即对两个矩阵相同位置的元素进行加减乘除等运算。\n矩阵相乘：A*A\n矩阵相加：A + A\n逆矩阵实现：只有方阵才有逆矩阵\nA = np.arange(4).reshape(2,2)\nnp.linalg.inv(A)\n矩阵的特征值与特征向量\nA = [[1, 2],[2, 1]] #生成一个2*2的矩阵\nfrom scipy.linalg import eig\nevals, evecs = eig(A) #求A的特征值（evals）和特征向量(evecs)\n行列式\nnp.linalg.det(A)\n解线性方程组\nfrom scipy.linalg import solve\nx = solve(a, b)\n奇异值分解\n"
    },
    {
        "title": "概率论",
        "content": "ll = [[1,2,3,4,5,6],[3,4,5,6,7,8]]\nnp.mean(ll)  #全部元素求均值\nnp.mean(ll,0) #按列求均值，0代表列向量，1表示行向量\n求方差：\nnp.var(b)\nnp.var(ll,1)) #第二个参数为1，表示按行求方差\n标准差\nnp.std(ll)\n相关系数\nnp.corrcoef(a,b)\n二项分布\nfrom scipy.stats import binom, norm, beta, expon\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n#n,p对应二项式公式中的事件成功次数及其概率，size表示采样次数\nbinom_sim = binom.rvs(n=10, p=0.3, size=10000)\nprint('Data:',binom_sim)\nprint('Mean: %g' % np.mean(binom_sim))\nprint('SD: %g' % np.std(binom_sim, ddof=1))\n#生成直方图，x指定每个bin(箱子)分布的数据,对应x轴，binx是总共有几条条状图，normed值密度,也就是每个条状图的占比例比,默认为1\nplt.hist(binom_sim, bins=10, normed=True)\nplt.xlabel(('x'))\nplt.ylabel('density')\nplt.show()\n泊松分布\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n#产生10000个符合lambda=2的泊松分布的数\nX= np.random.poisson(lam=2, size=10000)  \n\na = plt.hist(X, bins=15, normed=True, range=[0, 15])\n#生成网格\nplt.grid()\nplt.show()\n正态分布\nimport matplotlib.pyplot as plt\nmu = 0\nsigma = 1\n#分布采样点\nx = np.arange(-5, 5, 0.1)\n#生成符合mu,sigma的正态分布\ny = norm.pdf(x, mu, sigma)\nplt.plot(x, y)\nplt.xlabel('x')\nplt.ylabel('density')\nplt.show()\n指数分布\nfrom scipy.stats import expon\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nlam = 0.5\n#分布采样点\nx = np.arange(0, 15, 0.1)\n#生成符合lambda为0.5的指数分布\ny = expon.pdf(x, lam)\nplt.plot(x, y)\nplt.title('Exponential: lam=%.2f' % lam)\nplt.xlabel('x')\nplt.ylabel('density')\nplt.show()\n\n中心极限定理\nimport numpy as np\nimport matplotlib.pyplot as plt\n#随机产生10000个范围为(1,6)的数\nramdon_data = np.random.randint(1,7,10000)\nprint(ramdon_data.mean())\nprint(ramdon_data.std())\n生成直方图\nplt.figure()\nplt.hist(ramdon_data,bins=6,facecolor='blue')\nplt.xlabel('x')\nplt.ylabel('n')\nplt.show()\n随机抽取1000组数据，每组50个\nsamples = []\nsamples_mean =[]\nsamples_std = []\n\n#从生成的1000个数中随机抽取1000组\nfor i in range(0,1000):\nsample = []\n#每组随机抽取50个数\n    for j in range(0,50):\n        sample.append(ramdon_data[int(np.random.random() * len(ramdon_data))])\n  #将这50个数组成一个array放入samples列表中\n    sample_ar = np.array(sample)\nsamples.append(sample_ar)\n#保存每50个数的均值和标准差\n    samples_mean.append(sample_ar.mean())\nsamples_std.append(sample_ar.std())\n#samples_std_ar = np.array(samples_std)\n#samples_mean_ar = np.array(samples_mean)\n# print(samples_mean_ar)\n梯度下降法\n训练集(x,y)共5个样本,每个样本点有3个分量 (x0,x1,x2)  \nx = [(1, 0., 3), (1, 1., 3), (1, 2., 3), (1, 3., 2), (1, 4., 4)]  \ny = [95.364, 97.217205, 75.195834, 60.105519, 49.342380]  y[i] 样本点对应的输出  \nepsilon = 0.0001  #迭代阀值，当两次迭代损失函数之差小于该阀值时停止迭代  \nalpha = 0.01  #学习率\ndiff = [0, 0]  \nmax_itor = 1000  \nerror1 = 0  \nerror0 = 0  \ncnt = 0  \nm = len(x)  \n#初始化参数  \ntheta0 = 0  \ntheta1 = 0  \ntheta2 = 0  \nwhile True:  \n    cnt += 1  \n\n    # 参数迭代计算  \n    for i in range(m):  \n        # 拟合函数为 \ny = theta0 * x[0] + theta1 * x[1] +theta2 * x[2]  \n        # 计算残差，即拟合函数值-真实值  \n        diff[0] = (theta0** x[i][0] + theta1 * x[i][1] + theta2 * x[i][2]) - y[i]  \n  \n        # 梯度 = diff[0] * x[i][j]。根据步长*梯度更新参数 \n        theta0 -= alpha * diff[0] * x[i][0]  \n        theta1 -= alpha * diff[0] * x[i][1]  \n        theta2 -= alpha * diff[0] * x[i][2]  \n  \n    # 计算损失函数  \n    error1 = 0  \n    for lp in range(len(x)):  \n        error1 += (y[lp]-(theta0 + theta1 * x[lp][1] + theta2 * x[lp][2]))**2/2  \n    #若当两次迭代损失函数之差小于该阀值时停止迭代，跳出循环；\n    if abs(error1-error0) < epsilon:  \n        break  \n    else:  \n        error0 = error1  \n  \n    print(' theta0 : %f, theta1 : %f, theta2 : %f, error1 : %f' % (theta0, theta1, theta2, error1) )  \n\nprint('Done: theta0 : %f, theta1 : %f, theta2 : %f' % (theta0, theta1, theta2)  )\nprint('迭代次数: %d' % cnt  )\n"
    },
    {
        "title": "斐波那契数列",
        "content": "def fibonacci(n):\nif n==1:\nreturn [0]\nif n==2:\nreturn [0,1]\n    fib = [0, 1]\n    for i in range(2, n):\n        next_value = fib[i-1] + fib[i-2]\n        fib.append(next_value)\n    return fib[:n]\n\n# 生成前10个斐波那契数列的值\nfibonacci_10 = fibonacci(10)"
    },
    {
        "title": "字典",
        "content": "# 创建字典并存储元素\ndict_data = {'name':'lee', 'age':22, 'gender':'male'}\n\n查看字典items\ndict_data.items()\n插入一个键值对\nd[\"mark\"]=99\n# 分别将字典中的键和值全部输出\nkeys = list(dict_data.keys())\nvalues = list(dict_data.values())\n\n# 获取字典中键'name'对应的值\nname_value = dict_data['name']\n# 1. 新建字典 Dict1\nDict1 = {'name': 'lee', 'age': 89, 'num': [1, 2, 8]}\n\n# 2. 浅拷贝 Dict1，副本命名为 Dict_copy\nDict_copy = Dict1.copy()\n\n# 3. 创建集合，命名为 sample_set，包含2个元素 \"Prince\", \"Techs\"\nsample_set = {\"Prince\", \"Techs\"}\n\n# 4. 检查集合 sample_set 中是否存在某一元素 \"Data\" 并打印结论\nexists_data = \"Data\" in sample_set\ncheck_conclusion = \"存在\" if exists_data else \"不存在\"\n\n# 5. 向集合 sample_set 中增加元素 \"Data\"\nsample_set.add(\"Data\")\n\n# 6. 将元素 'Techs' 从集合 sample_set 中移除\nsample_set.remove(\"Techs\")\n\n# 7. 将集合 sample_set 分别转换为元组和列表结构，并打印输出\nsample_set_tuple = tuple(sample_set)\nsample_set_list = list(sample_set)"
    },
    {
        "title": "列表，冒泡排序",
        "content": "# 创建列表并存储元素 [310, 7]\nlst = [310, 7]\n\n# 打印输出列表\nprint(lst)\n#在末尾添加一个元素3\nlst.append(3)\n# 在列表元素下标为2的位置插入元素5\nlst.insert(2, 5)\n#列表从小到大排序\nlst.sort()\n从大到小\nlst.sort(reverse=True)\n# 冒泡排序，从大到小\nfor i in range(len(lst)):\n    for j in range(0, len(lst)-i-1):\n        if lst[j] < lst[j+1]:\n            lst[j], lst[j+1] = lst[j+1], lst[j]\n\n# 打印排序后的列表\nprint(lst)\n\n# 在列表末尾位置添加元素 'fish'\nlst.append('fish')\n将最后一个元素替换为100\nlst[-1]=100\n\n\n# 将列表中的字符串元素 'fish' 转换为全部大写并替换原本的 'fish'\nlst[lst.index('fish')] = 'FISH'\n字符串a=\"Victory\"\na.upper()全改为大写\na.lower()全改为小写\na.title()字符串改为首字母大写\n"
    },
    {
        "title": "集合",
        "content": "#创建集合\ns=set([1,2])\ns={1,2}\ns.add(3)\n# 删除元素（如果元素不存在，会引发KeyError）\ns.remove(2)\n清空集合\nset1.clear()\nset1.update([6, 7, 8])  # 添加元素，可以是列表、元组、字典等\n# 2. 检查集合中是否在元素'numpy'\ncontains_numpy = 'numpy' in sample\n# 3. 删除集合中的元素'pandas'\nsample.discard('pandas')\n# 4. 将集合分别转化为列表和元组并输出\nlist_from_set = list(sample)\ntuple_from_set = tuple(sample)\n\n# 5. 使用 copy()对集合进行浅拷贝\nsample_copy = sample.copy()\n"
    },
    {
        "title": "总-代码总结",
        "content": "一、算法\n1、线性回归\nfrom sklearn.linear_model import LinearRegression\nmodel.coef_ w 系数 model.intercept_ 截距\n2、逻辑回归\nfrom sklearn.linear_model import LogisticRegression\n3、KNN\nfrom sklearn.neighbors import KneiborsClassifier\nKneiborsClassifier(n_neighbors =4, algorithm = “ball_tree”)\n4、朴素贝叶斯\nfrom sklearn.naive_bayes import BernoulliNB\n5、SVM\nfrom sklearn.svm import svc\nsvc(c = 1.0, kernel = “rbf”)\nc：惩罚系数，默认是 0，C 越小，泛化能力越强。\nKernel：核函数 rbf 是径向基（高斯）此外还有线性 linear、多项式、poly、sigmoid\n6、决策树\nfrom sklearn.tree import DecisionTreeClassifier\n7、集成算法\nfrom sklearn.ensemble import RandomForestClassifier,  BaggingClassifier，\nAdaBoostClassifier，GradientBosstingClassifier\n\n通用 参数 n_estimators = 10 ,基类学习器个数。\nGradientBosstingClassifier(max_depth = 5)\nmax_depth = 5 树最大深度\nfrom xgboost.sklearn import XGBClassifier\n8、Kmeans\nfrom sklearn.cluster import Kmeans\nKmeans(n_clusters = k, init=’k_means++’,max_iter = 300)\nmodel.labels_ 类标签\nmodel.cluster_centers_ 簇中心\n\nestimator = KMeans(n_clusters=3)\nestimator.fit(X)\nlabel_pred = estimator.labels_\nx0 = X[label_pred == 0]\nx1 = X[label_pred == 1]\nx2 = X[label_pred == 2]\n\n\n\n\n9、AgglomerativeClustering\nfrom sklearn.cluster import AgglomerativeClustering\nAgglomerativeClustering(n_clusters=2, affinity=“euclidean”, compute_full_tree,linkage = “ward”）\ncompute_full_tree = False 时，训练 n_cluesters 后，训练停止；True 则训练整颗树。\nlinkage 的参数为\n{“ward”, “complete”,“average”, “single”}\n\n10、Birch\nfrom sklearn.cluster import Birch\nBirch(threshold=0.5, branching_factor=50, n_clusters=3)\nthreshold：float，表示设定的半径阈值，默认 0.5。\nbranching_factor：int，默认=50，每个节点最大特征树子集群数。\nn_clusters：int，默认=3，最终聚类数目。\n11、DBSCAN\nfrom sklearn.cluster import DBSCAN\nDBSCAN(eps=0.5, min_samples=5, metric=’euclidean)\neps：两个样本被看作邻居节点的最大距离。\nmin_samples：最小簇的样本数。\nmetric：距离计算方式，euclidean 为欧氏距离计算。\n12、ariori\nfrom mlxtend.frequent_patterns import apriori\nApriori(df, min_support=0.5, use_colnames=False, max_len=None,\nn_jobs=1)\n其中 df 代表数据框数据集，min_support 表示指定的最小支持度，\nuse_colnames=True 表示使用元素名字，默认的 False 使用列名代表元素，max_len 表\n示生成的项目集的最大长度。如果为 None，则评估所有可能的项集长度。\n二、画图\nplt.xlabel('pca1')\nplt.ylabel('pca2')\nplt.title(\"PCA\")\nplt.legend(loc='lower left')\n1、条形图\n数据为：\ngrouped = data.groupby(['是否高质用户', '网络类型'])['用户标识符'].count().unstack()\n\n网络类型 2G 3G 4G\n是否高质用户   \n0 1630.0 1668.0 1699.0\n1 NaN 2530.0 2473.0\ndf.plot.bar(rot = 0) //直接 df 画图 rot 是下标志是否字体是立着的还是卧着的。\ngrouped.plot(kind='bar', alpha=1.0, rot=0)\n2、散点图\ndata.plot.scatter(x = '类目 2 消费金额', y = 6, c = 'br')\nx = 为列名 列名可以用字符串 或如 y 的序列序数\nc = 'br'是颜色， 如蓝色和红色。\nplt.scatter(data.loc[:, '类目 2 消费金额'], data.loc[:, '类目 3 消费金额'], c=y,alpha=0.5)\n# 生成数据可视化\ny = data.loc[:, '是否高质用户']\nplt.scatter(data.loc[:, '类目 2 消费金额'], data.loc[:, '类目 3 消费金额'], c=y,alpha=0.5)\n这种图的颜色点由 y 决定。\n3、饼图\ngrouped.plot.pie(autopct = '%0.01f', subplots = True)\nautopct 为是否在饼图画百分比；Subplots = True 为是否为每个列画单独子图。\n4、3D 画图\nfrom mpl_toolkits.mplot3d import Axes3D\nfig = plt.figure()\n方法一\nax = fig.gca(projection='3d')\n方法二\nax = Axes3D(fig)\nax.scatter(X[y==i ,0], X[y==i, 1], X[y==i,2], c=c,marker=m,\nlabel=l)\n#做三维曲面图，rstride 和 cstride 分别代表行和列的跨度，cmap:曲面颜色\nax.plot_surface(X, Y, Z1, rstride=1, cstride=1, cmap='rainbow')\n5、热力图\nfig = plt.figure(figsize = (25.10))\nplt.subplot(1,2,1)\n# 关系热力图\nmask_corr = np.zeros_like(df.corr())\nmast_corr[np.triu_indices_from(mask_corr)] = True\nsns.heatmap(df.corr(), mask = mask_corr)\n6、直方图\n#x 指定每个 bin(箱子)分布的数据,对应 x 轴，binx 是总共有几条条状图，\nnormed 值密度,也就是每个条状图的占比例比,默认为 1\nplt.hist(binom_sim, bins=10, normed=True)\n\ncond = data['是否高质用户'] == 1\ndata[cond]['类目1消费金额'].hist(alpha=0.5, label='高质用户')\ndata[~cond]['类目1消费金额'].hist(color='r', alpha=0.5, label='非高质用户')\nplt.legend()\n三、缺失值处理\n1、检测\nmissing_sum = df.isnull().sum().sorted_values(ascending=False) //检测缺失数量\nmissing_rate = missing_sum/df.shape[0].sort_values(ascending=False)\nmissing_stat = pd.concat([missing_sum,missing_rate], keys=['missing_sum','missing_rate', axis = ‘columns’])\n2、删除\ndf.drop(columns = cols_to_drop,inplace=True)\ndel df[cols_to_drop]\n3、填充\n3.1 非监督填充方法一\nfrom sklearn.preprocessing import Imputer\ndata = Imputer(missing_values='NaN', strategy='most_frequent',\naxis=0) //strategy =mean/median/most_frequent\ndataMode = data.fit_transform(df)\n方法二、\nvalues = {'A': 0, 'B': 1, 'C': 2, 'D': 3}\ndf.fillna(value=values)\n3.2 算法填充\n参见具体算法\n\n四、异常值处理\n1、散点图检测\n具体见绘图\n2、利用决策树等算法预测\n具体见算法\n3、 3σ原则\nmin_mask = df[\"weight\"] < (df[\"weight\"].mean() - 3 *  df[\"weight\"].std())\nmax_mask = df[\"weight\"] > (df[\"weight\"].mean() + 3 * df[\"weight\"].std())\n# 只要满足上诉表达式的任一个就为异常值，所以这里使用位与运算\nmask = min_mask | max_mask\nprint(df.loc[mask,\"weight\"])\n\n4、IQR\niqr = Ser.quantile(0.75)-Ser.quantile(0.25)\nLow = Ser.quantile(0.25)-1.5*iqr\nUp = Ser.quantile(0.75)+1.5*iqr\nindex = (Ser< Low) | (Ser>Up)\nreturn index\n\n五、特征缩放\n1、标准化\n方法一、\nfrom sklearn.preprocessing import scale\nX =  scale(X)\n方法二、\nfrom sklearn.preprocessing import StandardScaler\nX = StandardScaler().fit_transform(X)\n2、最小值-最大值归一化\nfrom sklearn.preprocessing import MinMaxScaler\nX = MinMaxScaler(feature_range=(0, 1), copy=True).fit_transform(X)\n3、均值归一化\nmean=np.mean(x)\nmin=np.min(x)\nmax=np.max(x)\nMeanNormalization=(x-mean)/(max-min)\n4、缩放成单位向量\nlinalg = np.linalg.norm(x, ord=1)\nX=x/linalg\n\n六、数值离散化\n1、聚类算法\n详细见算法\n2、等宽划分\nx=pd.cut(X,5)\n3、等频划分\nx=pd.qcut(X,5)\n\n七、特征编码\n1、独热编码\nfrom sklearn.preprocessing import OneHotEncoder\nenc = OneHotEncoder(sparse = False)\nx = enc.fit_transform(iris.data)\n2、哑编码\npd.get_dummies(X) //转完后全部记得变为 int\nX = X.astype(np.int)\nX.info()\n\n\n\n\n3、LabelEncoder\n方法一、\nle = preprocessing.LabelEncoder()\nle.fit([\"paris\", \"paris\", \"tokyo\", \"amsterdam\"])\nlist(le.classes_) #['amsterdam', 'paris', 'tokyo']\nle.transform([\"tokyo\", \"tokyo\", \"paris\"]) # array([2, 2, 1]...)\nlist(le.inverse_transform([2, 2, 1]))#['tokyo', 'tokyo', 'paris']\n方法二、\ndf['Sex'] = df['Sex'].map({'female': 1, 'male': 0})\n\n八、特征选择\n1、过滤法\n方法一、方差\nfrom sklearn.feature_selection import VarianceThreshold\nX_var=VarianceThreshold(threshold=0.5).fit_transform(X, y) \n #使用阈值0.5 进行选择，特征方差小于 0.5 的特征会被删除\n方法二、卡方\nfrom sklearn.feature_selection chi2,SelectKBest\nSelectKBest(chi2, k=2).fit_transform(iris.data, iris.target)\n方法三、互信息素\nfrom sklearn.feature_selection import mutual_info_classif\nX_mut = mutual_info_classif(X, y)\n\n2、包装法 RFE\nfrom sklearn.feature_selection import RFE\nfrom sklearn.linear_model import LogisticRegression\nx_rfe=RFE(estimator=LogisticRegression(), n_features_to_select=3).fit(X, y)\nprint(x_rfe.n_features_ ) # 所选特征的数量\nprint(x_rfe.support_ ) # 按特征对应位置展示所选特征，True 表示保留，False 表示剔除。\nprint(x_rfe.ranking_ ) # 特征排名，使得 ranking_[i]对应于第 i 个特征的排名位置。\nprint(x_rfe.estimator_ ) # 递归方法选择的基模型\n\n3、嵌入法\n方法一、逻辑回归\nprint(lr.coef_)\n方法二、L1 Lasso\nfrom sklearn.linear_model import Lasso\nls = Lasso()\nls.fit(X, Y)\nls.coef_\n方法三、随机森林\nrf = RandomForestRegressor(n_estimators=15, max_depth=6)\nboston_rf=rf.fit(X, y)\nboston_rf.feature_importances_\n\n九、降维\n1、PCA\nfrom sklearn.decomposition import PCA\nX_std = preprocessing.scale(X)\npca = PCA(n_components=2)\nX_pca =pca.fit(X_std).transform(X_std)\nprint(pca.explained_variance_ratio_)# 观测降维后特征信息量大小。\n2、LDA\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis\nas LDA\nlda = LDA(n_components=2)\nX_lda =lda.fit(X,y).transform(X)\nprint(lda.explained_variance_ratio_)\n\n十、模型评估调优\n1、常见普通\nfrom sklearn.metrics import accuracy_score, confusion_matrix, recall_score, f1_score,  fbeta_score\nprint(accuracy_score(y_true, y_pred))\nprint(fbeta_score(y_true, y_pred, beta=0.5))\n2、分类结果统计报告\nfrom sklearn.metrics import classification_report\nprint(classification_report(y_test, y_pred))\n3、样本划分\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y_labels, test_size=0.2)\n4、交叉验证\nfrom sklearn.model_selection import cross_val_score\ncv_scores = cross_val_score(rf_model, X_train,\ny_train,scoring=make_scorer(recall_score) ,cv=5)\nprint('mean f1_score socre of raw model{}'.format(np.mean(cv_scores)))\n\n\n\n5、网格调优\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.metrics import make_scorer\nmodel_gbr = GradientBoostingRegressor()\nparameters = {'loss': ['ls','lad','huber','quantile'],'min_samples_leaf':\n[1,2,3,4,5],'alpha': [0.1,0.3,0.6,0.9]}\nmodel_gs = GridSearchCV(estimator=model_gbr, param_grid=parameters, cv=5, scoring=make_scorer(f1_score))\nmodel_gs.fit(X_train,y_train)\nprint('Best score is:', model_gs.best_score_)\nprint('Best parameter is:', model_gs.best_params_)\n6、过采样\nfrom imblearn.over_sampling import SMOTE\nsmote_model = SMOTE(random_state=7, ratio=0.1)\nX_train_res_rf,y_train_res_rf = smote_model.fit_resample(X_train,y_train)\nprint('Resampled dataset shape {}'.format(y_train_res_rf))\n#过采样比例为 0.1，即目标的正负样本比例为 1:10\n\n十一、其他小知识点\n1、文件操作：\n写文件：\n# 使用 write 方法写文件\nwith open(\"f.txt\", \"w\") as f:\nf.write( \"www.huawei.com\")\n读取文件：\n# 使用 read 方法读取\nwith open(\"f.txt\", \"r\") as f:\nprint(f.read())\n2、实验模型的导出以及导入\nimport pickle\nfrom sklearn.externals import joblib\njoblib.dump(svm, 'svm.pkl')\nsvm = joblib.load('svm.pkl')\n3、间隔取值\nlist[::2 ] #list 间隔取值\ndf = df[[i%2==0 for i in range(len(df.index))]] #df 按行取样本\ndf=df.iloc[:,[i%2==0 for i in range(len(df.columns))]]#df按列取样本\n\n\n\n\n4、取序号值\narr = np.array([3, 22, 4, 11, 2, 44, 9])\nprint(np.max(arr))  # 44\nprint(np.argmax(arr))  # 5\nprint(np.argmin(arr))  # 4\nprint(np.where(arr > 4, arr - 10, arr * 10))  # [10 20 30 40 -5 -4 -3 -2]\n\ndef modthree(x):\n    return x % 3 ==0\nprint(np.where(modthree(arr), arr - 10, arr * 10))  # [ -7 220  40 110  20 440  -1]\n\narr = np.array([3, 4, 5, 6, 7])\nprint(np.argwhere(arr % 2 != 0))\nprint(np.argwhere(arr % 2 != 0).flatten())  # [0 2 4]  可以用来取序号\n3、浮点数两位\ndf.round(2)\n5、斐波那契数列\n# 位置参数\ndef fibs(num):\nresult = [0,1]# 新建列表存储数列的值\nfor i in range(2,num):# 循环 num-2 次\na = result[i-1] + result[i-2]\n# 将值追加至列表\nresult.append(a)\n# 返回列表\nreturn result\nfibs(5)\n# 输出：[0, 1, 1, 2, 3]\n\n十二、概率论：\n1、二项分布贝努力\nfrom scipy.stats import binom\nbinom_sim = binom.rvs(n=10, p=0.3, size=10000) #\n2、泊松分布\nX= np.random.poisson(lam=2, size=10000)\n3、正态分布\nfrom scipy.stats import norm\nx = np.arange(-5, 5, 0.1)\ny = norm.pdf(x, mu, sigma)\n4、指数分布\nfrom scipy.stats import expon\nx = np.arange(0, 15, 0.1)\ny = expon.pdf(x, lam)\n"
    },
    {
        "title": "电信用户分析,聚类",
        "content": "import pandas as pd\nimport numpy as np\nimport matplotlib.pylab as plt\n%matplotlib inline\n\nX = pd.read_csv('./telecom.csv', encoding='utf-8')\nprint(X.shape)\nX.head()\n\n# 数据预处理\nfrom sklearn import preprocessing\nX_scaled = preprocessing.scale(X)  # scale操作之后的数据零均值，单位方差（方差为1）\nX_scaled[0:5]\n\n# 进行PCA数据降维\nfrom sklearn.decomposition import PCA\n \n# 生成PCA实例\npca = PCA(n_components=3)  # 把维度降至3维\n# 进行PCA降维\nX_pca = pca.fit_transform(X_scaled)\n# 生成降维后的dataframe\nX_pca_frame = pd.DataFrame(X_pca, columns=['pca_1', 'pca_2', 'pca_3'])  # 原始数据由(30000, 7)降维至(30000, 3)\nX_pca_frame.head()\n\n# 训练简单模型\nfrom sklearn.cluster import KMeans\n \n# KMeans算法实例化，将其设置为K=10\nest = KMeans(n_clusters=10)\n \n# 作用到降维后的数据上\nest.fit(X_pca)\n\n# 取出聚类后的标签\nkmeans_clustering_labels = pd.DataFrame(est.labels_, columns=['cluster'])  # 0-9,一共10个标签\n \n# 生成有聚类后的dataframe\nX_pca_frame = pd.concat([X_pca_frame, kmeans_clustering_labels], axis=1)\n \nX_pca_frame.head()\n\n# 对不同的k值进行计算，筛选出最优的K值\nfrom mpl_toolkits.mplot3d import Axes3D  # 绘制3D图形\nfrom sklearn import metrics\n \n# KMeans算法实例化，将其设置为K=range(2, 14)\nd = {}\nfig_reduced_data = plt.figure(figsize=(12, 12))  #画图之前首先设置figure对象，此函数相当于设置一块自定义大小的画布，使得后面的图形输出在这块规定了大小的画布上，其中参数figsize设置画布大小\nfor k in range(2, 14):\n    est = KMeans(n_clusters=k, random_state=111)\n    # 作用到降维后的数据上\n    y_pred = est.fit_predict(X_pca)\n    # 评估不同k值聚类算法效果\n    calinski_harabaz_score = metrics.calinski_harabasz_score(X_pca_frame, y_pred)  # X_pca_frame：表示要聚类的样本数据，一般形如（samples，features）的格式。y_pred：即聚类之后得到的label标签，形如（samples，）的格式\n    d.update({k: calinski_harabaz_score})\n    print('calinski_harabaz_score with k={0} is {1}'.format(k, calinski_harabaz_score))  # CH score的数值越大越好\n    # 生成三维图形，每个样本点的坐标分别是三个主成分的值\n    ax = plt.subplot(4, 3, k - 1, projection='3d') #将figure设置的画布大小分成几个部分，表示4(row)x3(colu),即将画布分成4x3，四行三列的12块区域，k-1表示选择图形输出的区域在第k-1块，图形输出区域参数必须在“行x列”范围\n    ax.scatter(X_pca_frame.pca_1, X_pca_frame.pca_2, X_pca_frame.pca_3, c=y_pred)  # pca_1、pca_2、pca_3为输入数据，c表示颜色序列\n    ax.set_xlabel('pca_1')\n    ax.set_ylabel('pca_2')\n    ax.set_zlabel('pca_3')\n\n# 绘制不同k值对应的score，找到最优的k值\nx = []\ny = []\nfor k, score in d.items():\n    x.append(k)\n    y.append(score)\n \nplt.plot(x, y)\nplt.xlabel('k value')\nplt.ylabel('calinski_harabaz_score')\n\nX.index = X_pca_frame.index  # 返回：RangeIndex(start=0, stop=30000, step=1)\n \n# 合并原数据和三个主成分的数据\nX_full = pd.concat([X, X_pca_frame], axis=1)\nX_full.head()\n\n# 按每个聚类分组\ngrouped = X_full.groupby('cluster')\n \nresult_data = pd.DataFrame()\n# 对分组做循环，分别对每组进行去除异常值处理\nfor name, group in grouped:\n    # 每组去除异常值前的个数\n    print('Group:{0}, Samples before:{1}'.format(name, group['pca_1'].count()))\n\n    desp = group[['pca_1', 'pca_2', 'pca_3']].describe() # 返回每组的数量、均值、标准差、最小值、最大值等数据\n    for att in ['pca_1', 'pca_2', 'pca_3']:\n        # 去异常值：箱形图\n        lower25 = desp.loc['25%', att]\n        upper75 = desp.loc['75%', att]\n        IQR = upper75 - lower25\n        min_value = lower25 - 1.5 * IQR\n        max_value = upper75 + 1.5 * IQR\n        # 使用统计中的1.5*IQR法则，删除每个聚类中的噪音和异常点\n        group = group[(group[att] > min_value) & (group[att] < max_value)]\n    result_data = pd.concat([result_data, group], axis=0)\n    # 每组去除异常值后的个数\n    print('Group:{0}, Samples after:{1}'.format(name, group['pca_1'].count()))\nprint('Remain sample:', result_data['pca_1'].count())\n\n# 设置每个簇对应的颜色\ncluster_2_color = {0: 'red', 1: 'green', 2: 'blue', 3: 'yellow', 4: 'cyan', 5: 'black', 6: 'magenta', 7: '#fff0f5',\n                   8: '#ffdab9', 9: '#ffa500'}\n \ncolors_clustered_data = X_pca_frame.cluster.map(cluster_2_color)  # 簇名和颜色映射\nfig_reduced_data = plt.figure()\nax_clustered_data = plt.subplot(111, projection='3d')\n \n# 聚类算法之后的不同簇数据的映射为不同颜色\nax_clustered_data.scatter(X_pca_frame.pca_1.values, X_pca_frame.pca_2.values, X_pca_frame.pca_3.values,\n                          c=colors_clustered_data)\nax_clustered_data.set_xlabel('Component_1')\nax_clustered_data.set_ylabel('Component_2')\nax_clustered_data.set_zlabel('Component_3')\n\n# 筛选后的数据聚类可视化\ncolors_filtered_data = result_data.cluster.map(cluster_2_color)\nfig = plt.figure(figsize=(12,12))\nax = plt.subplot(111, projection='3d')\nax.scatter(result_data.pca_1.values, result_data.pca_2.values, result_data.pca_3.values, c=colors_filtered_data)\nax.set_xlabel('Component_1')\nax.set_ylabel('Component_2')\nax.set_zlabel('Component_3')\n\n# 查看各族中的每月话费情况\nmonthly_Fare = result_data.groupby('cluster').describe().loc[:, u'每月话费']\nmonthly_Fare\n\n# mean：均值；std：标准差\nmonthly_Fare[['mean', 'std']].plot(kind='bar', rot=0, legend=True)  # rot可以控制轴标签的旋转度数。legend是否在图上显示图例\n\n# 查看各族中的入网时间情况\naccess_time = result_data.groupby('cluster').describe().loc[:, u'入网时间']\naccess_time\naccess_time[['mean', 'std']].plot(kind='bar', rot=0, legend=True, title='Access Time')\n\n# 查看各族中的欠费金额情况\narrearage = result_data.groupby('cluster').describe().loc[:, u'欠费金额']\narrearage[['mean', 'std']].plot(kind='bar', rot=0, legend=True, title='Arrearage')\n\n# 综合描述\nnew_column = ['Access_time', u'套餐价格', u'每月流量', 'Monthly_Fare', u'每月通话时长', 'Arrearage', u'欠费月份数', u'pca_1', u'pca_2',\n              u'pca_3', u'cluster']\nresult_data.columns = new_column\nresult_data.groupby('cluster')[['Monthly_Fare', 'Access_time', 'Arrearage']].mean().plot(kind='bar')  # 每个簇的Monthly_Fare、Access_time、Arrearag的均值放在一块比较"
    },
    {
        "title": "手写线性回归",
        "content": "import numpy as np\n\ndef simple_linear_regression(x, y):\n    # 计算x和y的平均值\n    x_mean = np.mean(x)\n    y_mean = np.mean(y)\n    \n    # 计算权重w\n    w = np.sum((x - x_mean) * (y - y_mean)) / np.sum((x - x_mean) ** 2)\n    \n    # 计算偏置b\n    b = y_mean - w * x_mean\n    \n    return w, b\n\ndef predict(x, w, b):\n    # 使用模型参数进行预测\n    return w * x + b\n\n# 示例数据\nx = np.array([10, 4, 6])\ny = np.array([8, 2, 5])\n\n# 训练模型并获取参数\nw, b = simple_linear_regression(x, y)\n\n# 使用模型进行预测\nx_new = np.array([12])\ny_pred = predict(x_new, w, b)\n\nprint(f\"预测结果: y = {w:.2f} * x + {b:.2f}\")\nprint(f\"对于x = {x_new[0]}, 预测的y值是: {y_pred[0]:.2f}\")\n\n\n*基于线性回归算法实现\nfrom sklearn.linear_model import LinearRegression\nmodel=LinearRegression()\nmodel.fit(data,y)\nprint('基于线性逻辑回归算法',model.coef_,model.intercept_)"
    },
    {
        "title": "方差选择模型（VarianceThreshold）",
        "content": "* 题目说阈值为1，但是hreshold=填空，有如下的备注，我就调整了hreshold，用了另外的判断方法，输出的结果为True False。要看True的值。\nfrom sklearn.feature_selection import VarianceThreshold\n\n# 定义方差选择模型，定义方差系数\nmodel_vt = VarianceThreshold(threshold=0.7) #，输出超过3个小于6个。"
    },
    {
        "title": "SMOTE过采样",
        "content": "pip install imbalanced-learn\nfrom imblearn.over_sampling import SMOTE\nfrom sklearn.datasets import make_classification\nfrom sklearn.model_selection import train_test_split\n\n# 创建一个不平衡的数据集\nX, y = make_classification(n_classes=2, class_sep=2,\n                           weights=[0.1, 0.9], n_informative=3, n_redundant=1, flip_y=0,\n                           n_features=20, n_clusters_per_class=1, n_samples=1000, random_state=10)\n\n# 划分数据集\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=0)\n\n# 实例化 SMOTE\nsm = SMOTE(random_state=42)\n\n# 应用 SMOTE\nX_res, y_res = sm.fit_resample(X_train, y_train)\n\n# 现在 X_res 和 y_res 包含了过采样后的训练数据\n"
    },
    {
        "title": "最小二乘法",
        "content": "import numpy as np  \nimport scipy as sp  \nimport pylab as pl  \nfrom scipy.optimize import leastsq  # 引入最小二乘函数  \n\nn = 9  # 多项式次数  \n定义目标函数：  \ndef real_func(x):  \n  #目标函数：sin(2*pi*x)\n    return np.sin(2 * np.pi * x)  \n定义多项式函数，用多项式去拟合数据:  \ndef fit_func(p, x):  \n    f = np.poly1d(p)  \n    return f(x)  \n定义残差函数，残差函数值为多项式拟合结果与真实值的差值：  \ndef residuals_func(p, y, x):  \n    ret = fit_func(p, x) - y  \n    return ret  \n\nx = np.linspace(0, 1, 9)  # 随机选择9个点作为x  \nx_points = np.linspace(0, 1, 1000)  # 画图时需要的连续点  \ny0 = real_func(x)  # 目标函数  \ny1 = [np.random.normal(0, 0.1) + y for y in y0]  # 在目标函数上添加符合正态分布噪声后的函数  \np_init = np.random.randn(n)  # 随机初始化多项式参数  \n# 调用scipy.optimize中的leastsq函数，通过最小化误差的平方和来寻找最佳的匹配函数\n#func 是一个残差函数，x0 是计算的初始参数值，把残差函数中除了初始化以外的参数打包到args中\nplsq = leastsq(func=residuals_func, x0=p_init, args=(y1, x))  \n\nprint('Fitting Parameters: ', plsq[0])  # 输出拟合参数  \n\npl.plot(x_points, real_func(x_points), label='real')  \npl.plot(x_points, fit_func(plsq[0], x_points), label='fitted curve')  \npl.plot(x, y1, 'bo', label='with noise')  \npl.legend()  \npl.show()"
    },
    {
        "title": "美国人口收入分析",
        "content": "import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import LabelEncoder, StandardScaler\nfrom sklearn.neighbors import KNN\nfrom sklearn.metrics import classification_report, confusion_matrix\nfrom sklearn.model_selection import GridSearchCV\n\n# 1、加载数据，查看数据行列分布情况\ndata = pd.read_csv('adult_inconn.csv')\ndata.shape\n\n# 2、查看数据特征，最大值，最小值，中位数，平均数以及四分位数\ndata.describe()\n\n# 3、查看数据缺失值分布，并用柱形图表示\nmissing_values = data.isnull().sum()\nmissing_values.plot(kind='bar')\nmissing_values[missing_values > 0].plot(kind='bar', color='skyblue')\nplt.title(\"缺失值分布\")\nplt.xlabel(\"特征\")\nplt.ylabel(\"缺失值数量\")\nplt.show()\n# 4、创建新数据集、对Predclass 标签进行转化\n# 这里假设Predclass是目标变量，我们需要将其转换为数值型\ndata['Predclass'] = data['Predclass'].map({'<=50K': 0, '>50K': 1})\n\n# 5、使用cut方法对数据进行分箱，分10个箱\n# 这里以年龄为例进行分箱\ndata['age_bins'] = pd.cut(data['age'], bins=10, right=False)\n\n# 6、绘制分箱后的数据分布图\nage_distribution = data['age_bins'].value_counts().sort_index().plot(kind='bar')\n\n# 7、属性衍生 - 这里我们以年龄和收入为例创建一个新属性\ndata['age_income'] = data['age'] * data['Predclass']\n创建新的“年龄组”或“收入水平”特征。\n# 举例，创建“年龄组”特征\ndata['age_group'] = pd.cut(data['age'], bins=[0, 30, 60, 90], labels=['青年', '中年', '老年'])\n\n# 8、查看sex-marital 分布图\nsex_marital_distribution = sns.countplot(data=data, x='sex', hue='marital_status')\n# 性别和婚姻状况分布图\nsns.countplot(data=data, x='sex', hue='marital-status')\nplt.title(\"性别与婚姻状况分布\")\nplt.show()\n# 9、数据特征值处理，属性编码\n# 对类别特征进行编码\nlabel_encoders = {}\nfor column in data.select_dtypes(include=['object']).columns:\n    le = LabelEncoder()\n    data[column] = le.fit_transform(data[column])\n    label_encoders[column] = le\n\n# 10、将df 转为str - 这一步似乎没有必要，可能是有误解\ndata = data.astype(str)\n\n# 11、引入KNN 对数据进行预测\n# 首先划分数据集\nX = data.drop('Predclass', axis=1)\ny = data['Predclass']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n# 标准化特征\nscaler = StandardScaler()\nX_train_scaled = scaler.fit_transform(X_train)\nX_test_scaled = scaler.transform(X_test)\n\n# 12、选择模型最优参数\nknn = KNN()\nparam_grid = {'n_neighbors': np.arange(1, 30)}\nknn_gscv = GridSearchCV(knn, param_grid, cv=5)\nknn_gscv.fit(X_train_scaled, y_train)\nbest_params = knn_gscv.best_params_\n\n# 13、重新生成模型knn_final\nknn_final = KNN(n_neighbors=best_params['n_neighbors'])\n\n# 14、对数据进行拟合\nknn_final.fit(X_train_scaled, y_train)\n\n# 15、重新对X_test 进行预测\ny_pred = knn_final.predict(X_test_scaled)\n\n# 16、导入分类模块，画出分布图\nconf_matrix = confusion_matrix(y_test, y_pred)\nsns.heatmap(conf_matrix, annot=True, fmt='d')\n\n# 输出相关结果\ndata_shape, data_description, missing_values, best_params, conf_matrix\n绘制预测结果分布图\nsns.countplot(x=y_pred)\nplt.title(\"预测结果分布\")\nplt.show()\n"
    },
 
    results = []
    for entry in json_data:
        if key.lower() in entry['title'].lower():
            results.append({'title': entry['title'], 'content': entry['content']})

    df = pd.DataFrame(results)
    df.style.set_properties(**{'white-space': 'pre-wrap', 'text-align': 'left'})
def l_by_c(key):
    json_data=[
    {
        "title": "关联规则",
        "content": "# 读取数据并展示前五行\ndf = pd.read_csv('./data/online_retail_II.csv')\ndf.head()\n1.清理df数据：删除缺失值与重复值。\ndf.dropna(inplace=True)\ndf.drop_duplicates(inplace=True)\nfrom mlxtend.frequent_patterns import apriori, fpmax, fpgrowth\nfrom mlxtend.frequent_patterns import association_rules\n\n# 找到所有支持度超过0.03的项集\nbasket = df.groupby(['Invoice', 'StockCode'])['Quantity'].sum().unstack().fillna(0)\nbasket_sets = basket.applymap(lambda x: 1 if x > 0 else 0)\n使用 fpgrowth算法从数据basket_sets中计算频繁项集，并将最小支持度设置为 0.1。返回频繁项集frequent_itemsets\n\nfrequent_itemsets = fpgrowth(basket_sets, min_support=0.1, use_colnames=True)\nfrequent_itemsets\n使用association_rules从频繁项集frequent_itemsets中构建关联规则，metric为'confidence'， min_threshold为0.4。并将结果存储在rules中\n\nrules = association_rules(frequent_itemsets, metric='confidence', min_threshold=0.4)\n# 展示前五条关联规则\nrules.head()\n4.打印出同时满足置信度< 0.8，置信度>0.5，提升度>3.0的rules数据。\n/*此处由考生进行代码填写*/\nfiltered_rules = rules[(rules['confidence'] < 0.8) & (rules['confidence'] > 0.5) & (rules['lift'] > 3.0)]\n\n# 打印结果\nprint(filtered_rules)\n筛选规则rules，将规则中同时满足antecedent_sele为{'22111', '22271'}，consequent_sele为{'22272'}的规则选出来，并存储在final_sele中\n\n# 筛选 antecedents 和 consequents 满足条件的规则\nfinal_sele = (rules['antecedents'] == {'22111', '22271'}) & (rules['consequents'] == {'22272'})\n# 使用 final_sele 筛选出相应的规则\nrules.loc[final_sele ]"
    },
    {
        "title": "线性代数",
        "content": "生成一个包含整数0-11的向量\nx = np.arange(12)\n查看数组大小\nx.shape\n将x转换成二维矩阵，其中矩阵的第一个维度为1\nx = x.reshape(1,12)\n将x转换3x4的矩阵\nx = x.reshape(3,4)\n生成3*4的矩阵\nA = np.arange(12).reshape(3,4)\n转置\nA.T\n矩阵相乘：两个矩阵能够相乘的条件为第一个矩阵的列数等于第二个矩阵的行数。\nnp.matmul(A,B)\n矩阵对应运算：针对形状相同矩阵的运算统称，包括元素对应相乘、相加等，即对两个矩阵相同位置的元素进行加减乘除等运算。\n矩阵相乘：A*A\n矩阵相加：A + A\n逆矩阵实现：只有方阵才有逆矩阵\nA = np.arange(4).reshape(2,2)\nnp.linalg.inv(A)\n矩阵的特征值与特征向量\nA = [[1, 2],[2, 1]] #生成一个2*2的矩阵\nfrom scipy.linalg import eig\nevals, evecs = eig(A) #求A的特征值（evals）和特征向量(evecs)\n行列式\nnp.linalg.det(A)\n解线性方程组\nfrom scipy.linalg import solve\nx = solve(a, b)\n奇异值分解\n"
    },
    {
        "title": "概率论",
        "content": "ll = [[1,2,3,4,5,6],[3,4,5,6,7,8]]\nnp.mean(ll)  #全部元素求均值\nnp.mean(ll,0) #按列求均值，0代表列向量，1表示行向量\n求方差：\nnp.var(b)\nnp.var(ll,1)) #第二个参数为1，表示按行求方差\n标准差\nnp.std(ll)\n相关系数\nnp.corrcoef(a,b)\n二项分布\nfrom scipy.stats import binom, norm, beta, expon\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n#n,p对应二项式公式中的事件成功次数及其概率，size表示采样次数\nbinom_sim = binom.rvs(n=10, p=0.3, size=10000)\nprint('Data:',binom_sim)\nprint('Mean: %g' % np.mean(binom_sim))\nprint('SD: %g' % np.std(binom_sim, ddof=1))\n#生成直方图，x指定每个bin(箱子)分布的数据,对应x轴，binx是总共有几条条状图，normed值密度,也就是每个条状图的占比例比,默认为1\nplt.hist(binom_sim, bins=10, normed=True)\nplt.xlabel(('x'))\nplt.ylabel('density')\nplt.show()\n泊松分布\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n#产生10000个符合lambda=2的泊松分布的数\nX= np.random.poisson(lam=2, size=10000)  \n\na = plt.hist(X, bins=15, normed=True, range=[0, 15])\n#生成网格\nplt.grid()\nplt.show()\n正态分布\nimport matplotlib.pyplot as plt\nmu = 0\nsigma = 1\n#分布采样点\nx = np.arange(-5, 5, 0.1)\n#生成符合mu,sigma的正态分布\ny = norm.pdf(x, mu, sigma)\nplt.plot(x, y)\nplt.xlabel('x')\nplt.ylabel('density')\nplt.show()\n指数分布\nfrom scipy.stats import expon\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nlam = 0.5\n#分布采样点\nx = np.arange(0, 15, 0.1)\n#生成符合lambda为0.5的指数分布\ny = expon.pdf(x, lam)\nplt.plot(x, y)\nplt.title('Exponential: lam=%.2f' % lam)\nplt.xlabel('x')\nplt.ylabel('density')\nplt.show()\n\n中心极限定理\nimport numpy as np\nimport matplotlib.pyplot as plt\n#随机产生10000个范围为(1,6)的数\nramdon_data = np.random.randint(1,7,10000)\nprint(ramdon_data.mean())\nprint(ramdon_data.std())\n生成直方图\nplt.figure()\nplt.hist(ramdon_data,bins=6,facecolor='blue')\nplt.xlabel('x')\nplt.ylabel('n')\nplt.show()\n随机抽取1000组数据，每组50个\nsamples = []\nsamples_mean =[]\nsamples_std = []\n\n#从生成的1000个数中随机抽取1000组\nfor i in range(0,1000):\nsample = []\n#每组随机抽取50个数\n    for j in range(0,50):\n        sample.append(ramdon_data[int(np.random.random() * len(ramdon_data))])\n  #将这50个数组成一个array放入samples列表中\n    sample_ar = np.array(sample)\nsamples.append(sample_ar)\n#保存每50个数的均值和标准差\n    samples_mean.append(sample_ar.mean())\nsamples_std.append(sample_ar.std())\n#samples_std_ar = np.array(samples_std)\n#samples_mean_ar = np.array(samples_mean)\n# print(samples_mean_ar)\n梯度下降法\n训练集(x,y)共5个样本,每个样本点有3个分量 (x0,x1,x2)  \nx = [(1, 0., 3), (1, 1., 3), (1, 2., 3), (1, 3., 2), (1, 4., 4)]  \ny = [95.364, 97.217205, 75.195834, 60.105519, 49.342380]  y[i] 样本点对应的输出  \nepsilon = 0.0001  #迭代阀值，当两次迭代损失函数之差小于该阀值时停止迭代  \nalpha = 0.01  #学习率\ndiff = [0, 0]  \nmax_itor = 1000  \nerror1 = 0  \nerror0 = 0  \ncnt = 0  \nm = len(x)  \n#初始化参数  \ntheta0 = 0  \ntheta1 = 0  \ntheta2 = 0  \nwhile True:  \n    cnt += 1  \n\n    # 参数迭代计算  \n    for i in range(m):  \n        # 拟合函数为 \ny = theta0 * x[0] + theta1 * x[1] +theta2 * x[2]  \n        # 计算残差，即拟合函数值-真实值  \n        diff[0] = (theta0** x[i][0] + theta1 * x[i][1] + theta2 * x[i][2]) - y[i]  \n  \n        # 梯度 = diff[0] * x[i][j]。根据步长*梯度更新参数 \n        theta0 -= alpha * diff[0] * x[i][0]  \n        theta1 -= alpha * diff[0] * x[i][1]  \n        theta2 -= alpha * diff[0] * x[i][2]  \n  \n    # 计算损失函数  \n    error1 = 0  \n    for lp in range(len(x)):  \n        error1 += (y[lp]-(theta0 + theta1 * x[lp][1] + theta2 * x[lp][2]))**2/2  \n    #若当两次迭代损失函数之差小于该阀值时停止迭代，跳出循环；\n    if abs(error1-error0) < epsilon:  \n        break  \n    else:  \n        error0 = error1  \n  \n    print(' theta0 : %f, theta1 : %f, theta2 : %f, error1 : %f' % (theta0, theta1, theta2, error1) )  \n\nprint('Done: theta0 : %f, theta1 : %f, theta2 : %f' % (theta0, theta1, theta2)  )\nprint('迭代次数: %d' % cnt  )\n"
    },
    {
        "title": "斐波那契数列",
        "content": "def fibonacci(n):\nif n==1:\nreturn [0]\nif n==2:\nreturn [0,1]\n    fib = [0, 1]\n    for i in range(2, n):\n        next_value = fib[i-1] + fib[i-2]\n        fib.append(next_value)\n    return fib[:n]\n\n# 生成前10个斐波那契数列的值\nfibonacci_10 = fibonacci(10)"
    },
    {
        "title": "字典",
        "content": "# 创建字典并存储元素\ndict_data = {'name':'lee', 'age':22, 'gender':'male'}\n\n查看字典items\ndict_data.items()\n插入一个键值对\nd[\"mark\"]=99\n# 分别将字典中的键和值全部输出\nkeys = list(dict_data.keys())\nvalues = list(dict_data.values())\n\n# 获取字典中键'name'对应的值\nname_value = dict_data['name']\n# 1. 新建字典 Dict1\nDict1 = {'name': 'lee', 'age': 89, 'num': [1, 2, 8]}\n\n# 2. 浅拷贝 Dict1，副本命名为 Dict_copy\nDict_copy = Dict1.copy()\n\n# 3. 创建集合，命名为 sample_set，包含2个元素 \"Prince\", \"Techs\"\nsample_set = {\"Prince\", \"Techs\"}\n\n# 4. 检查集合 sample_set 中是否存在某一元素 \"Data\" 并打印结论\nexists_data = \"Data\" in sample_set\ncheck_conclusion = \"存在\" if exists_data else \"不存在\"\n\n# 5. 向集合 sample_set 中增加元素 \"Data\"\nsample_set.add(\"Data\")\n\n# 6. 将元素 'Techs' 从集合 sample_set 中移除\nsample_set.remove(\"Techs\")\n\n# 7. 将集合 sample_set 分别转换为元组和列表结构，并打印输出\nsample_set_tuple = tuple(sample_set)\nsample_set_list = list(sample_set)"
    },
    {
        "title": "列表，冒泡排序",
        "content": "# 创建列表并存储元素 [310, 7]\nlst = [310, 7]\n\n# 打印输出列表\nprint(lst)\n#在末尾添加一个元素3\nlst.append(3)\n# 在列表元素下标为2的位置插入元素5\nlst.insert(2, 5)\n#列表从小到大排序\nlst.sort()\n从大到小\nlst.sort(reverse=True)\n# 冒泡排序，从大到小\nfor i in range(len(lst)):\n    for j in range(0, len(lst)-i-1):\n        if lst[j] < lst[j+1]:\n            lst[j], lst[j+1] = lst[j+1], lst[j]\n\n# 打印排序后的列表\nprint(lst)\n\n# 在列表末尾位置添加元素 'fish'\nlst.append('fish')\n将最后一个元素替换为100\nlst[-1]=100\n\n\n# 将列表中的字符串元素 'fish' 转换为全部大写并替换原本的 'fish'\nlst[lst.index('fish')] = 'FISH'\n字符串a=\"Victory\"\na.upper()全改为大写\na.lower()全改为小写\na.title()字符串改为首字母大写\n"
    },
    {
        "title": "集合",
        "content": "#创建集合\ns=set([1,2])\ns={1,2}\ns.add(3)\n# 删除元素（如果元素不存在，会引发KeyError）\ns.remove(2)\n清空集合\nset1.clear()\nset1.update([6, 7, 8])  # 添加元素，可以是列表、元组、字典等\n# 2. 检查集合中是否在元素'numpy'\ncontains_numpy = 'numpy' in sample\n# 3. 删除集合中的元素'pandas'\nsample.discard('pandas')\n# 4. 将集合分别转化为列表和元组并输出\nlist_from_set = list(sample)\ntuple_from_set = tuple(sample)\n\n# 5. 使用 copy()对集合进行浅拷贝\nsample_copy = sample.copy()\n"
    },
    {
        "title": "总-代码总结",
        "content": "一、算法\n1、线性回归\nfrom sklearn.linear_model import LinearRegression\nmodel.coef_ w 系数 model.intercept_ 截距\n2、逻辑回归\nfrom sklearn.linear_model import LogisticRegression\n3、KNN\nfrom sklearn.neighbors import KneiborsClassifier\nKneiborsClassifier(n_neighbors =4, algorithm = “ball_tree”)\n4、朴素贝叶斯\nfrom sklearn.naive_bayes import BernoulliNB\n5、SVM\nfrom sklearn.svm import svc\nsvc(c = 1.0, kernel = “rbf”)\nc：惩罚系数，默认是 0，C 越小，泛化能力越强。\nKernel：核函数 rbf 是径向基（高斯）此外还有线性 linear、多项式、poly、sigmoid\n6、决策树\nfrom sklearn.tree import DecisionTreeClassifier\n7、集成算法\nfrom sklearn.ensemble import RandomForestClassifier,  BaggingClassifier，\nAdaBoostClassifier，GradientBosstingClassifier\n\n通用 参数 n_estimators = 10 ,基类学习器个数。\nGradientBosstingClassifier(max_depth = 5)\nmax_depth = 5 树最大深度\nfrom xgboost.sklearn import XGBClassifier\n8、Kmeans\nfrom sklearn.cluster import Kmeans\nKmeans(n_clusters = k, init=’k_means++’,max_iter = 300)\nmodel.labels_ 类标签\nmodel.cluster_centers_ 簇中心\n\nestimator = KMeans(n_clusters=3)\nestimator.fit(X)\nlabel_pred = estimator.labels_\nx0 = X[label_pred == 0]\nx1 = X[label_pred == 1]\nx2 = X[label_pred == 2]\n\n\n\n\n9、AgglomerativeClustering\nfrom sklearn.cluster import AgglomerativeClustering\nAgglomerativeClustering(n_clusters=2, affinity=“euclidean”, compute_full_tree,linkage = “ward”）\ncompute_full_tree = False 时，训练 n_cluesters 后，训练停止；True 则训练整颗树。\nlinkage 的参数为\n{“ward”, “complete”,“average”, “single”}\n\n10、Birch\nfrom sklearn.cluster import Birch\nBirch(threshold=0.5, branching_factor=50, n_clusters=3)\nthreshold：float，表示设定的半径阈值，默认 0.5。\nbranching_factor：int，默认=50，每个节点最大特征树子集群数。\nn_clusters：int，默认=3，最终聚类数目。\n11、DBSCAN\nfrom sklearn.cluster import DBSCAN\nDBSCAN(eps=0.5, min_samples=5, metric=’euclidean)\neps：两个样本被看作邻居节点的最大距离。\nmin_samples：最小簇的样本数。\nmetric：距离计算方式，euclidean 为欧氏距离计算。\n12、ariori\nfrom mlxtend.frequent_patterns import apriori\nApriori(df, min_support=0.5, use_colnames=False, max_len=None,\nn_jobs=1)\n其中 df 代表数据框数据集，min_support 表示指定的最小支持度，\nuse_colnames=True 表示使用元素名字，默认的 False 使用列名代表元素，max_len 表\n示生成的项目集的最大长度。如果为 None，则评估所有可能的项集长度。\n二、画图\nplt.xlabel('pca1')\nplt.ylabel('pca2')\nplt.title(\"PCA\")\nplt.legend(loc='lower left')\n1、条形图\n数据为：\ngrouped = data.groupby(['是否高质用户', '网络类型'])['用户标识符'].count().unstack()\n\n网络类型 2G 3G 4G\n是否高质用户   \n0 1630.0 1668.0 1699.0\n1 NaN 2530.0 2473.0\ndf.plot.bar(rot = 0) //直接 df 画图 rot 是下标志是否字体是立着的还是卧着的。\ngrouped.plot(kind='bar', alpha=1.0, rot=0)\n2、散点图\ndata.plot.scatter(x = '类目 2 消费金额', y = 6, c = 'br')\nx = 为列名 列名可以用字符串 或如 y 的序列序数\nc = 'br'是颜色， 如蓝色和红色。\nplt.scatter(data.loc[:, '类目 2 消费金额'], data.loc[:, '类目 3 消费金额'], c=y,alpha=0.5)\n# 生成数据可视化\ny = data.loc[:, '是否高质用户']\nplt.scatter(data.loc[:, '类目 2 消费金额'], data.loc[:, '类目 3 消费金额'], c=y,alpha=0.5)\n这种图的颜色点由 y 决定。\n3、饼图\ngrouped.plot.pie(autopct = '%0.01f', subplots = True)\nautopct 为是否在饼图画百分比；Subplots = True 为是否为每个列画单独子图。\n4、3D 画图\nfrom mpl_toolkits.mplot3d import Axes3D\nfig = plt.figure()\n方法一\nax = fig.gca(projection='3d')\n方法二\nax = Axes3D(fig)\nax.scatter(X[y==i ,0], X[y==i, 1], X[y==i,2], c=c,marker=m,\nlabel=l)\n#做三维曲面图，rstride 和 cstride 分别代表行和列的跨度，cmap:曲面颜色\nax.plot_surface(X, Y, Z1, rstride=1, cstride=1, cmap='rainbow')\n5、热力图\nfig = plt.figure(figsize = (25.10))\nplt.subplot(1,2,1)\n# 关系热力图\nmask_corr = np.zeros_like(df.corr())\nmast_corr[np.triu_indices_from(mask_corr)] = True\nsns.heatmap(df.corr(), mask = mask_corr)\n6、直方图\n#x 指定每个 bin(箱子)分布的数据,对应 x 轴，binx 是总共有几条条状图，\nnormed 值密度,也就是每个条状图的占比例比,默认为 1\nplt.hist(binom_sim, bins=10, normed=True)\n\ncond = data['是否高质用户'] == 1\ndata[cond]['类目1消费金额'].hist(alpha=0.5, label='高质用户')\ndata[~cond]['类目1消费金额'].hist(color='r', alpha=0.5, label='非高质用户')\nplt.legend()\n三、缺失值处理\n1、检测\nmissing_sum = df.isnull().sum().sorted_values(ascending=False) //检测缺失数量\nmissing_rate = missing_sum/df.shape[0].sort_values(ascending=False)\nmissing_stat = pd.concat([missing_sum,missing_rate], keys=['missing_sum','missing_rate', axis = ‘columns’])\n2、删除\ndf.drop(columns = cols_to_drop,inplace=True)\ndel df[cols_to_drop]\n3、填充\n3.1 非监督填充方法一\nfrom sklearn.preprocessing import Imputer\ndata = Imputer(missing_values='NaN', strategy='most_frequent',\naxis=0) //strategy =mean/median/most_frequent\ndataMode = data.fit_transform(df)\n方法二、\nvalues = {'A': 0, 'B': 1, 'C': 2, 'D': 3}\ndf.fillna(value=values)\n3.2 算法填充\n参见具体算法\n\n四、异常值处理\n1、散点图检测\n具体见绘图\n2、利用决策树等算法预测\n具体见算法\n3、 3σ原则\nmin_mask = df[\"weight\"] < (df[\"weight\"].mean() - 3 *  df[\"weight\"].std())\nmax_mask = df[\"weight\"] > (df[\"weight\"].mean() + 3 * df[\"weight\"].std())\n# 只要满足上诉表达式的任一个就为异常值，所以这里使用位与运算\nmask = min_mask | max_mask\nprint(df.loc[mask,\"weight\"])\n\n4、IQR\niqr = Ser.quantile(0.75)-Ser.quantile(0.25)\nLow = Ser.quantile(0.25)-1.5*iqr\nUp = Ser.quantile(0.75)+1.5*iqr\nindex = (Ser< Low) | (Ser>Up)\nreturn index\n\n五、特征缩放\n1、标准化\n方法一、\nfrom sklearn.preprocessing import scale\nX =  scale(X)\n方法二、\nfrom sklearn.preprocessing import StandardScaler\nX = StandardScaler().fit_transform(X)\n2、最小值-最大值归一化\nfrom sklearn.preprocessing import MinMaxScaler\nX = MinMaxScaler(feature_range=(0, 1), copy=True).fit_transform(X)\n3、均值归一化\nmean=np.mean(x)\nmin=np.min(x)\nmax=np.max(x)\nMeanNormalization=(x-mean)/(max-min)\n4、缩放成单位向量\nlinalg = np.linalg.norm(x, ord=1)\nX=x/linalg\n\n六、数值离散化\n1、聚类算法\n详细见算法\n2、等宽划分\nx=pd.cut(X,5)\n3、等频划分\nx=pd.qcut(X,5)\n\n七、特征编码\n1、独热编码\nfrom sklearn.preprocessing import OneHotEncoder\nenc = OneHotEncoder(sparse = False)\nx = enc.fit_transform(iris.data)\n2、哑编码\npd.get_dummies(X) //转完后全部记得变为 int\nX = X.astype(np.int)\nX.info()\n\n\n\n\n3、LabelEncoder\n方法一、\nle = preprocessing.LabelEncoder()\nle.fit([\"paris\", \"paris\", \"tokyo\", \"amsterdam\"])\nlist(le.classes_) #['amsterdam', 'paris', 'tokyo']\nle.transform([\"tokyo\", \"tokyo\", \"paris\"]) # array([2, 2, 1]...)\nlist(le.inverse_transform([2, 2, 1]))#['tokyo', 'tokyo', 'paris']\n方法二、\ndf['Sex'] = df['Sex'].map({'female': 1, 'male': 0})\n\n八、特征选择\n1、过滤法\n方法一、方差\nfrom sklearn.feature_selection import VarianceThreshold\nX_var=VarianceThreshold(threshold=0.5).fit_transform(X, y) \n #使用阈值0.5 进行选择，特征方差小于 0.5 的特征会被删除\n方法二、卡方\nfrom sklearn.feature_selection chi2,SelectKBest\nSelectKBest(chi2, k=2).fit_transform(iris.data, iris.target)\n方法三、互信息素\nfrom sklearn.feature_selection import mutual_info_classif\nX_mut = mutual_info_classif(X, y)\n\n2、包装法 RFE\nfrom sklearn.feature_selection import RFE\nfrom sklearn.linear_model import LogisticRegression\nx_rfe=RFE(estimator=LogisticRegression(), n_features_to_select=3).fit(X, y)\nprint(x_rfe.n_features_ ) # 所选特征的数量\nprint(x_rfe.support_ ) # 按特征对应位置展示所选特征，True 表示保留，False 表示剔除。\nprint(x_rfe.ranking_ ) # 特征排名，使得 ranking_[i]对应于第 i 个特征的排名位置。\nprint(x_rfe.estimator_ ) # 递归方法选择的基模型\n\n3、嵌入法\n方法一、逻辑回归\nprint(lr.coef_)\n方法二、L1 Lasso\nfrom sklearn.linear_model import Lasso\nls = Lasso()\nls.fit(X, Y)\nls.coef_\n方法三、随机森林\nrf = RandomForestRegressor(n_estimators=15, max_depth=6)\nboston_rf=rf.fit(X, y)\nboston_rf.feature_importances_\n\n九、降维\n1、PCA\nfrom sklearn.decomposition import PCA\nX_std = preprocessing.scale(X)\npca = PCA(n_components=2)\nX_pca =pca.fit(X_std).transform(X_std)\nprint(pca.explained_variance_ratio_)# 观测降维后特征信息量大小。\n2、LDA\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis\nas LDA\nlda = LDA(n_components=2)\nX_lda =lda.fit(X,y).transform(X)\nprint(lda.explained_variance_ratio_)\n\n十、模型评估调优\n1、常见普通\nfrom sklearn.metrics import accuracy_score, confusion_matrix, recall_score, f1_score,  fbeta_score\nprint(accuracy_score(y_true, y_pred))\nprint(fbeta_score(y_true, y_pred, beta=0.5))\n2、分类结果统计报告\nfrom sklearn.metrics import classification_report\nprint(classification_report(y_test, y_pred))\n3、样本划分\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y_labels, test_size=0.2)\n4、交叉验证\nfrom sklearn.model_selection import cross_val_score\ncv_scores = cross_val_score(rf_model, X_train,\ny_train,scoring=make_scorer(recall_score) ,cv=5)\nprint('mean f1_score socre of raw model{}'.format(np.mean(cv_scores)))\n\n\n\n5、网格调优\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.metrics import make_scorer\nmodel_gbr = GradientBoostingRegressor()\nparameters = {'loss': ['ls','lad','huber','quantile'],'min_samples_leaf':\n[1,2,3,4,5],'alpha': [0.1,0.3,0.6,0.9]}\nmodel_gs = GridSearchCV(estimator=model_gbr, param_grid=parameters, cv=5, scoring=make_scorer(f1_score))\nmodel_gs.fit(X_train,y_train)\nprint('Best score is:', model_gs.best_score_)\nprint('Best parameter is:', model_gs.best_params_)\n6、过采样\nfrom imblearn.over_sampling import SMOTE\nsmote_model = SMOTE(random_state=7, ratio=0.1)\nX_train_res_rf,y_train_res_rf = smote_model.fit_resample(X_train,y_train)\nprint('Resampled dataset shape {}'.format(y_train_res_rf))\n#过采样比例为 0.1，即目标的正负样本比例为 1:10\n\n十一、其他小知识点\n1、文件操作：\n写文件：\n# 使用 write 方法写文件\nwith open(\"f.txt\", \"w\") as f:\nf.write( \"www.huawei.com\")\n读取文件：\n# 使用 read 方法读取\nwith open(\"f.txt\", \"r\") as f:\nprint(f.read())\n2、实验模型的导出以及导入\nimport pickle\nfrom sklearn.externals import joblib\njoblib.dump(svm, 'svm.pkl')\nsvm = joblib.load('svm.pkl')\n3、间隔取值\nlist[::2 ] #list 间隔取值\ndf = df[[i%2==0 for i in range(len(df.index))]] #df 按行取样本\ndf=df.iloc[:,[i%2==0 for i in range(len(df.columns))]]#df按列取样本\n\n\n\n\n4、取序号值\narr = np.array([3, 22, 4, 11, 2, 44, 9])\nprint(np.max(arr))  # 44\nprint(np.argmax(arr))  # 5\nprint(np.argmin(arr))  # 4\nprint(np.where(arr > 4, arr - 10, arr * 10))  # [10 20 30 40 -5 -4 -3 -2]\n\ndef modthree(x):\n    return x % 3 ==0\nprint(np.where(modthree(arr), arr - 10, arr * 10))  # [ -7 220  40 110  20 440  -1]\n\narr = np.array([3, 4, 5, 6, 7])\nprint(np.argwhere(arr % 2 != 0))\nprint(np.argwhere(arr % 2 != 0).flatten())  # [0 2 4]  可以用来取序号\n3、浮点数两位\ndf.round(2)\n5、斐波那契数列\n# 位置参数\ndef fibs(num):\nresult = [0,1]# 新建列表存储数列的值\nfor i in range(2,num):# 循环 num-2 次\na = result[i-1] + result[i-2]\n# 将值追加至列表\nresult.append(a)\n# 返回列表\nreturn result\nfibs(5)\n# 输出：[0, 1, 1, 2, 3]\n\n十二、概率论：\n1、二项分布贝努力\nfrom scipy.stats import binom\nbinom_sim = binom.rvs(n=10, p=0.3, size=10000) #\n2、泊松分布\nX= np.random.poisson(lam=2, size=10000)\n3、正态分布\nfrom scipy.stats import norm\nx = np.arange(-5, 5, 0.1)\ny = norm.pdf(x, mu, sigma)\n4、指数分布\nfrom scipy.stats import expon\nx = np.arange(0, 15, 0.1)\ny = expon.pdf(x, lam)\n"
    },
    {
        "title": "电信用户分析,聚类",
        "content": "import pandas as pd\nimport numpy as np\nimport matplotlib.pylab as plt\n%matplotlib inline\n\nX = pd.read_csv('./telecom.csv', encoding='utf-8')\nprint(X.shape)\nX.head()\n\n# 数据预处理\nfrom sklearn import preprocessing\nX_scaled = preprocessing.scale(X)  # scale操作之后的数据零均值，单位方差（方差为1）\nX_scaled[0:5]\n\n# 进行PCA数据降维\nfrom sklearn.decomposition import PCA\n \n# 生成PCA实例\npca = PCA(n_components=3)  # 把维度降至3维\n# 进行PCA降维\nX_pca = pca.fit_transform(X_scaled)\n# 生成降维后的dataframe\nX_pca_frame = pd.DataFrame(X_pca, columns=['pca_1', 'pca_2', 'pca_3'])  # 原始数据由(30000, 7)降维至(30000, 3)\nX_pca_frame.head()\n\n# 训练简单模型\nfrom sklearn.cluster import KMeans\n \n# KMeans算法实例化，将其设置为K=10\nest = KMeans(n_clusters=10)\n \n# 作用到降维后的数据上\nest.fit(X_pca)\n\n# 取出聚类后的标签\nkmeans_clustering_labels = pd.DataFrame(est.labels_, columns=['cluster'])  # 0-9,一共10个标签\n \n# 生成有聚类后的dataframe\nX_pca_frame = pd.concat([X_pca_frame, kmeans_clustering_labels], axis=1)\n \nX_pca_frame.head()\n\n# 对不同的k值进行计算，筛选出最优的K值\nfrom mpl_toolkits.mplot3d import Axes3D  # 绘制3D图形\nfrom sklearn import metrics\n \n# KMeans算法实例化，将其设置为K=range(2, 14)\nd = {}\nfig_reduced_data = plt.figure(figsize=(12, 12))  #画图之前首先设置figure对象，此函数相当于设置一块自定义大小的画布，使得后面的图形输出在这块规定了大小的画布上，其中参数figsize设置画布大小\nfor k in range(2, 14):\n    est = KMeans(n_clusters=k, random_state=111)\n    # 作用到降维后的数据上\n    y_pred = est.fit_predict(X_pca)\n    # 评估不同k值聚类算法效果\n    calinski_harabaz_score = metrics.calinski_harabasz_score(X_pca_frame, y_pred)  # X_pca_frame：表示要聚类的样本数据，一般形如（samples，features）的格式。y_pred：即聚类之后得到的label标签，形如（samples，）的格式\n    d.update({k: calinski_harabaz_score})\n    print('calinski_harabaz_score with k={0} is {1}'.format(k, calinski_harabaz_score))  # CH score的数值越大越好\n    # 生成三维图形，每个样本点的坐标分别是三个主成分的值\n    ax = plt.subplot(4, 3, k - 1, projection='3d') #将figure设置的画布大小分成几个部分，表示4(row)x3(colu),即将画布分成4x3，四行三列的12块区域，k-1表示选择图形输出的区域在第k-1块，图形输出区域参数必须在“行x列”范围\n    ax.scatter(X_pca_frame.pca_1, X_pca_frame.pca_2, X_pca_frame.pca_3, c=y_pred)  # pca_1、pca_2、pca_3为输入数据，c表示颜色序列\n    ax.set_xlabel('pca_1')\n    ax.set_ylabel('pca_2')\n    ax.set_zlabel('pca_3')\n\n# 绘制不同k值对应的score，找到最优的k值\nx = []\ny = []\nfor k, score in d.items():\n    x.append(k)\n    y.append(score)\n \nplt.plot(x, y)\nplt.xlabel('k value')\nplt.ylabel('calinski_harabaz_score')\n\nX.index = X_pca_frame.index  # 返回：RangeIndex(start=0, stop=30000, step=1)\n \n# 合并原数据和三个主成分的数据\nX_full = pd.concat([X, X_pca_frame], axis=1)\nX_full.head()\n\n# 按每个聚类分组\ngrouped = X_full.groupby('cluster')\n \nresult_data = pd.DataFrame()\n# 对分组做循环，分别对每组进行去除异常值处理\nfor name, group in grouped:\n    # 每组去除异常值前的个数\n    print('Group:{0}, Samples before:{1}'.format(name, group['pca_1'].count()))\n\n    desp = group[['pca_1', 'pca_2', 'pca_3']].describe() # 返回每组的数量、均值、标准差、最小值、最大值等数据\n    for att in ['pca_1', 'pca_2', 'pca_3']:\n        # 去异常值：箱形图\n        lower25 = desp.loc['25%', att]\n        upper75 = desp.loc['75%', att]\n        IQR = upper75 - lower25\n        min_value = lower25 - 1.5 * IQR\n        max_value = upper75 + 1.5 * IQR\n        # 使用统计中的1.5*IQR法则，删除每个聚类中的噪音和异常点\n        group = group[(group[att] > min_value) & (group[att] < max_value)]\n    result_data = pd.concat([result_data, group], axis=0)\n    # 每组去除异常值后的个数\n    print('Group:{0}, Samples after:{1}'.format(name, group['pca_1'].count()))\nprint('Remain sample:', result_data['pca_1'].count())\n\n# 设置每个簇对应的颜色\ncluster_2_color = {0: 'red', 1: 'green', 2: 'blue', 3: 'yellow', 4: 'cyan', 5: 'black', 6: 'magenta', 7: '#fff0f5',\n                   8: '#ffdab9', 9: '#ffa500'}\n \ncolors_clustered_data = X_pca_frame.cluster.map(cluster_2_color)  # 簇名和颜色映射\nfig_reduced_data = plt.figure()\nax_clustered_data = plt.subplot(111, projection='3d')\n \n# 聚类算法之后的不同簇数据的映射为不同颜色\nax_clustered_data.scatter(X_pca_frame.pca_1.values, X_pca_frame.pca_2.values, X_pca_frame.pca_3.values,\n                          c=colors_clustered_data)\nax_clustered_data.set_xlabel('Component_1')\nax_clustered_data.set_ylabel('Component_2')\nax_clustered_data.set_zlabel('Component_3')\n\n# 筛选后的数据聚类可视化\ncolors_filtered_data = result_data.cluster.map(cluster_2_color)\nfig = plt.figure(figsize=(12,12))\nax = plt.subplot(111, projection='3d')\nax.scatter(result_data.pca_1.values, result_data.pca_2.values, result_data.pca_3.values, c=colors_filtered_data)\nax.set_xlabel('Component_1')\nax.set_ylabel('Component_2')\nax.set_zlabel('Component_3')\n\n# 查看各族中的每月话费情况\nmonthly_Fare = result_data.groupby('cluster').describe().loc[:, u'每月话费']\nmonthly_Fare\n\n# mean：均值；std：标准差\nmonthly_Fare[['mean', 'std']].plot(kind='bar', rot=0, legend=True)  # rot可以控制轴标签的旋转度数。legend是否在图上显示图例\n\n# 查看各族中的入网时间情况\naccess_time = result_data.groupby('cluster').describe().loc[:, u'入网时间']\naccess_time\naccess_time[['mean', 'std']].plot(kind='bar', rot=0, legend=True, title='Access Time')\n\n# 查看各族中的欠费金额情况\narrearage = result_data.groupby('cluster').describe().loc[:, u'欠费金额']\narrearage[['mean', 'std']].plot(kind='bar', rot=0, legend=True, title='Arrearage')\n\n# 综合描述\nnew_column = ['Access_time', u'套餐价格', u'每月流量', 'Monthly_Fare', u'每月通话时长', 'Arrearage', u'欠费月份数', u'pca_1', u'pca_2',\n              u'pca_3', u'cluster']\nresult_data.columns = new_column\nresult_data.groupby('cluster')[['Monthly_Fare', 'Access_time', 'Arrearage']].mean().plot(kind='bar')  # 每个簇的Monthly_Fare、Access_time、Arrearag的均值放在一块比较"
    },
    {
        "title": "手写线性回归",
        "content": "import numpy as np\n\ndef simple_linear_regression(x, y):\n    # 计算x和y的平均值\n    x_mean = np.mean(x)\n    y_mean = np.mean(y)\n    \n    # 计算权重w\n    w = np.sum((x - x_mean) * (y - y_mean)) / np.sum((x - x_mean) ** 2)\n    \n    # 计算偏置b\n    b = y_mean - w * x_mean\n    \n    return w, b\n\ndef predict(x, w, b):\n    # 使用模型参数进行预测\n    return w * x + b\n\n# 示例数据\nx = np.array([10, 4, 6])\ny = np.array([8, 2, 5])\n\n# 训练模型并获取参数\nw, b = simple_linear_regression(x, y)\n\n# 使用模型进行预测\nx_new = np.array([12])\ny_pred = predict(x_new, w, b)\n\nprint(f\"预测结果: y = {w:.2f} * x + {b:.2f}\")\nprint(f\"对于x = {x_new[0]}, 预测的y值是: {y_pred[0]:.2f}\")\n\n\n*基于线性回归算法实现\nfrom sklearn.linear_model import LinearRegression\nmodel=LinearRegression()\nmodel.fit(data,y)\nprint('基于线性逻辑回归算法',model.coef_,model.intercept_)"
    },
    {
        "title": "方差选择模型（VarianceThreshold）",
        "content": "* 题目说阈值为1，但是hreshold=填空，有如下的备注，我就调整了hreshold，用了另外的判断方法，输出的结果为True False。要看True的值。\nfrom sklearn.feature_selection import VarianceThreshold\n\n# 定义方差选择模型，定义方差系数\nmodel_vt = VarianceThreshold(threshold=0.7) #，输出超过3个小于6个。"
    },
    {
        "title": "SMOTE过采样",
        "content": "pip install imbalanced-learn\nfrom imblearn.over_sampling import SMOTE\nfrom sklearn.datasets import make_classification\nfrom sklearn.model_selection import train_test_split\n\n# 创建一个不平衡的数据集\nX, y = make_classification(n_classes=2, class_sep=2,\n                           weights=[0.1, 0.9], n_informative=3, n_redundant=1, flip_y=0,\n                           n_features=20, n_clusters_per_class=1, n_samples=1000, random_state=10)\n\n# 划分数据集\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=0)\n\n# 实例化 SMOTE\nsm = SMOTE(random_state=42)\n\n# 应用 SMOTE\nX_res, y_res = sm.fit_resample(X_train, y_train)\n\n# 现在 X_res 和 y_res 包含了过采样后的训练数据\n"
    },
    {
        "title": "最小二乘法",
        "content": "import numpy as np  \nimport scipy as sp  \nimport pylab as pl  \nfrom scipy.optimize import leastsq  # 引入最小二乘函数  \n\nn = 9  # 多项式次数  \n定义目标函数：  \ndef real_func(x):  \n  #目标函数：sin(2*pi*x)\n    return np.sin(2 * np.pi * x)  \n定义多项式函数，用多项式去拟合数据:  \ndef fit_func(p, x):  \n    f = np.poly1d(p)  \n    return f(x)  \n定义残差函数，残差函数值为多项式拟合结果与真实值的差值：  \ndef residuals_func(p, y, x):  \n    ret = fit_func(p, x) - y  \n    return ret  \n\nx = np.linspace(0, 1, 9)  # 随机选择9个点作为x  \nx_points = np.linspace(0, 1, 1000)  # 画图时需要的连续点  \ny0 = real_func(x)  # 目标函数  \ny1 = [np.random.normal(0, 0.1) + y for y in y0]  # 在目标函数上添加符合正态分布噪声后的函数  \np_init = np.random.randn(n)  # 随机初始化多项式参数  \n# 调用scipy.optimize中的leastsq函数，通过最小化误差的平方和来寻找最佳的匹配函数\n#func 是一个残差函数，x0 是计算的初始参数值，把残差函数中除了初始化以外的参数打包到args中\nplsq = leastsq(func=residuals_func, x0=p_init, args=(y1, x))  \n\nprint('Fitting Parameters: ', plsq[0])  # 输出拟合参数  \n\npl.plot(x_points, real_func(x_points), label='real')  \npl.plot(x_points, fit_func(plsq[0], x_points), label='fitted curve')  \npl.plot(x, y1, 'bo', label='with noise')  \npl.legend()  \npl.show()"
    },
    {
        "title": "美国人口收入分析",
        "content": "import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import LabelEncoder, StandardScaler\nfrom sklearn.neighbors import KNN\nfrom sklearn.metrics import classification_report, confusion_matrix\nfrom sklearn.model_selection import GridSearchCV\n\n# 1、加载数据，查看数据行列分布情况\ndata = pd.read_csv('adult_inconn.csv')\ndata.shape\n\n# 2、查看数据特征，最大值，最小值，中位数，平均数以及四分位数\ndata.describe()\n\n# 3、查看数据缺失值分布，并用柱形图表示\nmissing_values = data.isnull().sum()\nmissing_values.plot(kind='bar')\nmissing_values[missing_values > 0].plot(kind='bar', color='skyblue')\nplt.title(\"缺失值分布\")\nplt.xlabel(\"特征\")\nplt.ylabel(\"缺失值数量\")\nplt.show()\n# 4、创建新数据集、对Predclass 标签进行转化\n# 这里假设Predclass是目标变量，我们需要将其转换为数值型\ndata['Predclass'] = data['Predclass'].map({'<=50K': 0, '>50K': 1})\n\n# 5、使用cut方法对数据进行分箱，分10个箱\n# 这里以年龄为例进行分箱\ndata['age_bins'] = pd.cut(data['age'], bins=10, right=False)\n\n# 6、绘制分箱后的数据分布图\nage_distribution = data['age_bins'].value_counts().sort_index().plot(kind='bar')\n\n# 7、属性衍生 - 这里我们以年龄和收入为例创建一个新属性\ndata['age_income'] = data['age'] * data['Predclass']\n创建新的“年龄组”或“收入水平”特征。\n# 举例，创建“年龄组”特征\ndata['age_group'] = pd.cut(data['age'], bins=[0, 30, 60, 90], labels=['青年', '中年', '老年'])\n\n# 8、查看sex-marital 分布图\nsex_marital_distribution = sns.countplot(data=data, x='sex', hue='marital_status')\n# 性别和婚姻状况分布图\nsns.countplot(data=data, x='sex', hue='marital-status')\nplt.title(\"性别与婚姻状况分布\")\nplt.show()\n# 9、数据特征值处理，属性编码\n# 对类别特征进行编码\nlabel_encoders = {}\nfor column in data.select_dtypes(include=['object']).columns:\n    le = LabelEncoder()\n    data[column] = le.fit_transform(data[column])\n    label_encoders[column] = le\n\n# 10、将df 转为str - 这一步似乎没有必要，可能是有误解\ndata = data.astype(str)\n\n# 11、引入KNN 对数据进行预测\n# 首先划分数据集\nX = data.drop('Predclass', axis=1)\ny = data['Predclass']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n# 标准化特征\nscaler = StandardScaler()\nX_train_scaled = scaler.fit_transform(X_train)\nX_test_scaled = scaler.transform(X_test)\n\n# 12、选择模型最优参数\nknn = KNN()\nparam_grid = {'n_neighbors': np.arange(1, 30)}\nknn_gscv = GridSearchCV(knn, param_grid, cv=5)\nknn_gscv.fit(X_train_scaled, y_train)\nbest_params = knn_gscv.best_params_\n\n# 13、重新生成模型knn_final\nknn_final = KNN(n_neighbors=best_params['n_neighbors'])\n\n# 14、对数据进行拟合\nknn_final.fit(X_train_scaled, y_train)\n\n# 15、重新对X_test 进行预测\ny_pred = knn_final.predict(X_test_scaled)\n\n# 16、导入分类模块，画出分布图\nconf_matrix = confusion_matrix(y_test, y_pred)\nsns.heatmap(conf_matrix, annot=True, fmt='d')\n\n# 输出相关结果\ndata_shape, data_description, missing_values, best_params, conf_matrix\n绘制预测结果分布图\nsns.countplot(x=y_pred)\nplt.title(\"预测结果分布\")\nplt.show()\n"
    },
 
    results = []
    for entry in json_data:
        if key.lower() in entry['content'].lower():
            results.append({'title': entry['title'], 'content': entry['content']})
    df = pd.DataFrame(results)
    df.style.set_properties(**{'white-space': 'pre-wrap', 'text-align': 'left'})
def l_all():
    json_data=[
    {
        "title": "关联规则",
        "content": "# 读取数据并展示前五行\ndf = pd.read_csv('./data/online_retail_II.csv')\ndf.head()\n1.清理df数据：删除缺失值与重复值。\ndf.dropna(inplace=True)\ndf.drop_duplicates(inplace=True)\nfrom mlxtend.frequent_patterns import apriori, fpmax, fpgrowth\nfrom mlxtend.frequent_patterns import association_rules\n\n# 找到所有支持度超过0.03的项集\nbasket = df.groupby(['Invoice', 'StockCode'])['Quantity'].sum().unstack().fillna(0)\nbasket_sets = basket.applymap(lambda x: 1 if x > 0 else 0)\n使用 fpgrowth算法从数据basket_sets中计算频繁项集，并将最小支持度设置为 0.1。返回频繁项集frequent_itemsets\n\nfrequent_itemsets = fpgrowth(basket_sets, min_support=0.1, use_colnames=True)\nfrequent_itemsets\n使用association_rules从频繁项集frequent_itemsets中构建关联规则，metric为'confidence'， min_threshold为0.4。并将结果存储在rules中\n\nrules = association_rules(frequent_itemsets, metric='confidence', min_threshold=0.4)\n# 展示前五条关联规则\nrules.head()\n4.打印出同时满足置信度< 0.8，置信度>0.5，提升度>3.0的rules数据。\n/*此处由考生进行代码填写*/\nfiltered_rules = rules[(rules['confidence'] < 0.8) & (rules['confidence'] > 0.5) & (rules['lift'] > 3.0)]\n\n# 打印结果\nprint(filtered_rules)\n筛选规则rules，将规则中同时满足antecedent_sele为{'22111', '22271'}，consequent_sele为{'22272'}的规则选出来，并存储在final_sele中\n\n# 筛选 antecedents 和 consequents 满足条件的规则\nfinal_sele = (rules['antecedents'] == {'22111', '22271'}) & (rules['consequents'] == {'22272'})\n# 使用 final_sele 筛选出相应的规则\nrules.loc[final_sele ]"
    },
    {
        "title": "线性代数",
        "content": "生成一个包含整数0-11的向量\nx = np.arange(12)\n查看数组大小\nx.shape\n将x转换成二维矩阵，其中矩阵的第一个维度为1\nx = x.reshape(1,12)\n将x转换3x4的矩阵\nx = x.reshape(3,4)\n生成3*4的矩阵\nA = np.arange(12).reshape(3,4)\n转置\nA.T\n矩阵相乘：两个矩阵能够相乘的条件为第一个矩阵的列数等于第二个矩阵的行数。\nnp.matmul(A,B)\n矩阵对应运算：针对形状相同矩阵的运算统称，包括元素对应相乘、相加等，即对两个矩阵相同位置的元素进行加减乘除等运算。\n矩阵相乘：A*A\n矩阵相加：A + A\n逆矩阵实现：只有方阵才有逆矩阵\nA = np.arange(4).reshape(2,2)\nnp.linalg.inv(A)\n矩阵的特征值与特征向量\nA = [[1, 2],[2, 1]] #生成一个2*2的矩阵\nfrom scipy.linalg import eig\nevals, evecs = eig(A) #求A的特征值（evals）和特征向量(evecs)\n行列式\nnp.linalg.det(A)\n解线性方程组\nfrom scipy.linalg import solve\nx = solve(a, b)\n奇异值分解\n"
    },
    {
        "title": "概率论",
        "content": "ll = [[1,2,3,4,5,6],[3,4,5,6,7,8]]\nnp.mean(ll)  #全部元素求均值\nnp.mean(ll,0) #按列求均值，0代表列向量，1表示行向量\n求方差：\nnp.var(b)\nnp.var(ll,1)) #第二个参数为1，表示按行求方差\n标准差\nnp.std(ll)\n相关系数\nnp.corrcoef(a,b)\n二项分布\nfrom scipy.stats import binom, norm, beta, expon\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n#n,p对应二项式公式中的事件成功次数及其概率，size表示采样次数\nbinom_sim = binom.rvs(n=10, p=0.3, size=10000)\nprint('Data:',binom_sim)\nprint('Mean: %g' % np.mean(binom_sim))\nprint('SD: %g' % np.std(binom_sim, ddof=1))\n#生成直方图，x指定每个bin(箱子)分布的数据,对应x轴，binx是总共有几条条状图，normed值密度,也就是每个条状图的占比例比,默认为1\nplt.hist(binom_sim, bins=10, normed=True)\nplt.xlabel(('x'))\nplt.ylabel('density')\nplt.show()\n泊松分布\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n#产生10000个符合lambda=2的泊松分布的数\nX= np.random.poisson(lam=2, size=10000)  \n\na = plt.hist(X, bins=15, normed=True, range=[0, 15])\n#生成网格\nplt.grid()\nplt.show()\n正态分布\nimport matplotlib.pyplot as plt\nmu = 0\nsigma = 1\n#分布采样点\nx = np.arange(-5, 5, 0.1)\n#生成符合mu,sigma的正态分布\ny = norm.pdf(x, mu, sigma)\nplt.plot(x, y)\nplt.xlabel('x')\nplt.ylabel('density')\nplt.show()\n指数分布\nfrom scipy.stats import expon\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nlam = 0.5\n#分布采样点\nx = np.arange(0, 15, 0.1)\n#生成符合lambda为0.5的指数分布\ny = expon.pdf(x, lam)\nplt.plot(x, y)\nplt.title('Exponential: lam=%.2f' % lam)\nplt.xlabel('x')\nplt.ylabel('density')\nplt.show()\n\n中心极限定理\nimport numpy as np\nimport matplotlib.pyplot as plt\n#随机产生10000个范围为(1,6)的数\nramdon_data = np.random.randint(1,7,10000)\nprint(ramdon_data.mean())\nprint(ramdon_data.std())\n生成直方图\nplt.figure()\nplt.hist(ramdon_data,bins=6,facecolor='blue')\nplt.xlabel('x')\nplt.ylabel('n')\nplt.show()\n随机抽取1000组数据，每组50个\nsamples = []\nsamples_mean =[]\nsamples_std = []\n\n#从生成的1000个数中随机抽取1000组\nfor i in range(0,1000):\nsample = []\n#每组随机抽取50个数\n    for j in range(0,50):\n        sample.append(ramdon_data[int(np.random.random() * len(ramdon_data))])\n  #将这50个数组成一个array放入samples列表中\n    sample_ar = np.array(sample)\nsamples.append(sample_ar)\n#保存每50个数的均值和标准差\n    samples_mean.append(sample_ar.mean())\nsamples_std.append(sample_ar.std())\n#samples_std_ar = np.array(samples_std)\n#samples_mean_ar = np.array(samples_mean)\n# print(samples_mean_ar)\n梯度下降法\n训练集(x,y)共5个样本,每个样本点有3个分量 (x0,x1,x2)  \nx = [(1, 0., 3), (1, 1., 3), (1, 2., 3), (1, 3., 2), (1, 4., 4)]  \ny = [95.364, 97.217205, 75.195834, 60.105519, 49.342380]  y[i] 样本点对应的输出  \nepsilon = 0.0001  #迭代阀值，当两次迭代损失函数之差小于该阀值时停止迭代  \nalpha = 0.01  #学习率\ndiff = [0, 0]  \nmax_itor = 1000  \nerror1 = 0  \nerror0 = 0  \ncnt = 0  \nm = len(x)  \n#初始化参数  \ntheta0 = 0  \ntheta1 = 0  \ntheta2 = 0  \nwhile True:  \n    cnt += 1  \n\n    # 参数迭代计算  \n    for i in range(m):  \n        # 拟合函数为 \ny = theta0 * x[0] + theta1 * x[1] +theta2 * x[2]  \n        # 计算残差，即拟合函数值-真实值  \n        diff[0] = (theta0** x[i][0] + theta1 * x[i][1] + theta2 * x[i][2]) - y[i]  \n  \n        # 梯度 = diff[0] * x[i][j]。根据步长*梯度更新参数 \n        theta0 -= alpha * diff[0] * x[i][0]  \n        theta1 -= alpha * diff[0] * x[i][1]  \n        theta2 -= alpha * diff[0] * x[i][2]  \n  \n    # 计算损失函数  \n    error1 = 0  \n    for lp in range(len(x)):  \n        error1 += (y[lp]-(theta0 + theta1 * x[lp][1] + theta2 * x[lp][2]))**2/2  \n    #若当两次迭代损失函数之差小于该阀值时停止迭代，跳出循环；\n    if abs(error1-error0) < epsilon:  \n        break  \n    else:  \n        error0 = error1  \n  \n    print(' theta0 : %f, theta1 : %f, theta2 : %f, error1 : %f' % (theta0, theta1, theta2, error1) )  \n\nprint('Done: theta0 : %f, theta1 : %f, theta2 : %f' % (theta0, theta1, theta2)  )\nprint('迭代次数: %d' % cnt  )\n"
    },
    {
        "title": "斐波那契数列",
        "content": "def fibonacci(n):\nif n==1:\nreturn [0]\nif n==2:\nreturn [0,1]\n    fib = [0, 1]\n    for i in range(2, n):\n        next_value = fib[i-1] + fib[i-2]\n        fib.append(next_value)\n    return fib[:n]\n\n# 生成前10个斐波那契数列的值\nfibonacci_10 = fibonacci(10)"
    },
    {
        "title": "字典",
        "content": "# 创建字典并存储元素\ndict_data = {'name':'lee', 'age':22, 'gender':'male'}\n\n查看字典items\ndict_data.items()\n插入一个键值对\nd[\"mark\"]=99\n# 分别将字典中的键和值全部输出\nkeys = list(dict_data.keys())\nvalues = list(dict_data.values())\n\n# 获取字典中键'name'对应的值\nname_value = dict_data['name']\n# 1. 新建字典 Dict1\nDict1 = {'name': 'lee', 'age': 89, 'num': [1, 2, 8]}\n\n# 2. 浅拷贝 Dict1，副本命名为 Dict_copy\nDict_copy = Dict1.copy()\n\n# 3. 创建集合，命名为 sample_set，包含2个元素 \"Prince\", \"Techs\"\nsample_set = {\"Prince\", \"Techs\"}\n\n# 4. 检查集合 sample_set 中是否存在某一元素 \"Data\" 并打印结论\nexists_data = \"Data\" in sample_set\ncheck_conclusion = \"存在\" if exists_data else \"不存在\"\n\n# 5. 向集合 sample_set 中增加元素 \"Data\"\nsample_set.add(\"Data\")\n\n# 6. 将元素 'Techs' 从集合 sample_set 中移除\nsample_set.remove(\"Techs\")\n\n# 7. 将集合 sample_set 分别转换为元组和列表结构，并打印输出\nsample_set_tuple = tuple(sample_set)\nsample_set_list = list(sample_set)"
    },
    {
        "title": "列表，冒泡排序",
        "content": "# 创建列表并存储元素 [310, 7]\nlst = [310, 7]\n\n# 打印输出列表\nprint(lst)\n#在末尾添加一个元素3\nlst.append(3)\n# 在列表元素下标为2的位置插入元素5\nlst.insert(2, 5)\n#列表从小到大排序\nlst.sort()\n从大到小\nlst.sort(reverse=True)\n# 冒泡排序，从大到小\nfor i in range(len(lst)):\n    for j in range(0, len(lst)-i-1):\n        if lst[j] < lst[j+1]:\n            lst[j], lst[j+1] = lst[j+1], lst[j]\n\n# 打印排序后的列表\nprint(lst)\n\n# 在列表末尾位置添加元素 'fish'\nlst.append('fish')\n将最后一个元素替换为100\nlst[-1]=100\n\n\n# 将列表中的字符串元素 'fish' 转换为全部大写并替换原本的 'fish'\nlst[lst.index('fish')] = 'FISH'\n字符串a=\"Victory\"\na.upper()全改为大写\na.lower()全改为小写\na.title()字符串改为首字母大写\n"
    },
    {
        "title": "集合",
        "content": "#创建集合\ns=set([1,2])\ns={1,2}\ns.add(3)\n# 删除元素（如果元素不存在，会引发KeyError）\ns.remove(2)\n清空集合\nset1.clear()\nset1.update([6, 7, 8])  # 添加元素，可以是列表、元组、字典等\n# 2. 检查集合中是否在元素'numpy'\ncontains_numpy = 'numpy' in sample\n# 3. 删除集合中的元素'pandas'\nsample.discard('pandas')\n# 4. 将集合分别转化为列表和元组并输出\nlist_from_set = list(sample)\ntuple_from_set = tuple(sample)\n\n# 5. 使用 copy()对集合进行浅拷贝\nsample_copy = sample.copy()\n"
    },
    {
        "title": "总-代码总结",
        "content": "一、算法\n1、线性回归\nfrom sklearn.linear_model import LinearRegression\nmodel.coef_ w 系数 model.intercept_ 截距\n2、逻辑回归\nfrom sklearn.linear_model import LogisticRegression\n3、KNN\nfrom sklearn.neighbors import KneiborsClassifier\nKneiborsClassifier(n_neighbors =4, algorithm = “ball_tree”)\n4、朴素贝叶斯\nfrom sklearn.naive_bayes import BernoulliNB\n5、SVM\nfrom sklearn.svm import svc\nsvc(c = 1.0, kernel = “rbf”)\nc：惩罚系数，默认是 0，C 越小，泛化能力越强。\nKernel：核函数 rbf 是径向基（高斯）此外还有线性 linear、多项式、poly、sigmoid\n6、决策树\nfrom sklearn.tree import DecisionTreeClassifier\n7、集成算法\nfrom sklearn.ensemble import RandomForestClassifier,  BaggingClassifier，\nAdaBoostClassifier，GradientBosstingClassifier\n\n通用 参数 n_estimators = 10 ,基类学习器个数。\nGradientBosstingClassifier(max_depth = 5)\nmax_depth = 5 树最大深度\nfrom xgboost.sklearn import XGBClassifier\n8、Kmeans\nfrom sklearn.cluster import Kmeans\nKmeans(n_clusters = k, init=’k_means++’,max_iter = 300)\nmodel.labels_ 类标签\nmodel.cluster_centers_ 簇中心\n\nestimator = KMeans(n_clusters=3)\nestimator.fit(X)\nlabel_pred = estimator.labels_\nx0 = X[label_pred == 0]\nx1 = X[label_pred == 1]\nx2 = X[label_pred == 2]\n\n\n\n\n9、AgglomerativeClustering\nfrom sklearn.cluster import AgglomerativeClustering\nAgglomerativeClustering(n_clusters=2, affinity=“euclidean”, compute_full_tree,linkage = “ward”）\ncompute_full_tree = False 时，训练 n_cluesters 后，训练停止；True 则训练整颗树。\nlinkage 的参数为\n{“ward”, “complete”,“average”, “single”}\n\n10、Birch\nfrom sklearn.cluster import Birch\nBirch(threshold=0.5, branching_factor=50, n_clusters=3)\nthreshold：float，表示设定的半径阈值，默认 0.5。\nbranching_factor：int，默认=50，每个节点最大特征树子集群数。\nn_clusters：int，默认=3，最终聚类数目。\n11、DBSCAN\nfrom sklearn.cluster import DBSCAN\nDBSCAN(eps=0.5, min_samples=5, metric=’euclidean)\neps：两个样本被看作邻居节点的最大距离。\nmin_samples：最小簇的样本数。\nmetric：距离计算方式，euclidean 为欧氏距离计算。\n12、ariori\nfrom mlxtend.frequent_patterns import apriori\nApriori(df, min_support=0.5, use_colnames=False, max_len=None,\nn_jobs=1)\n其中 df 代表数据框数据集，min_support 表示指定的最小支持度，\nuse_colnames=True 表示使用元素名字，默认的 False 使用列名代表元素，max_len 表\n示生成的项目集的最大长度。如果为 None，则评估所有可能的项集长度。\n二、画图\nplt.xlabel('pca1')\nplt.ylabel('pca2')\nplt.title(\"PCA\")\nplt.legend(loc='lower left')\n1、条形图\n数据为：\ngrouped = data.groupby(['是否高质用户', '网络类型'])['用户标识符'].count().unstack()\n\n网络类型 2G 3G 4G\n是否高质用户   \n0 1630.0 1668.0 1699.0\n1 NaN 2530.0 2473.0\ndf.plot.bar(rot = 0) //直接 df 画图 rot 是下标志是否字体是立着的还是卧着的。\ngrouped.plot(kind='bar', alpha=1.0, rot=0)\n2、散点图\ndata.plot.scatter(x = '类目 2 消费金额', y = 6, c = 'br')\nx = 为列名 列名可以用字符串 或如 y 的序列序数\nc = 'br'是颜色， 如蓝色和红色。\nplt.scatter(data.loc[:, '类目 2 消费金额'], data.loc[:, '类目 3 消费金额'], c=y,alpha=0.5)\n# 生成数据可视化\ny = data.loc[:, '是否高质用户']\nplt.scatter(data.loc[:, '类目 2 消费金额'], data.loc[:, '类目 3 消费金额'], c=y,alpha=0.5)\n这种图的颜色点由 y 决定。\n3、饼图\ngrouped.plot.pie(autopct = '%0.01f', subplots = True)\nautopct 为是否在饼图画百分比；Subplots = True 为是否为每个列画单独子图。\n4、3D 画图\nfrom mpl_toolkits.mplot3d import Axes3D\nfig = plt.figure()\n方法一\nax = fig.gca(projection='3d')\n方法二\nax = Axes3D(fig)\nax.scatter(X[y==i ,0], X[y==i, 1], X[y==i,2], c=c,marker=m,\nlabel=l)\n#做三维曲面图，rstride 和 cstride 分别代表行和列的跨度，cmap:曲面颜色\nax.plot_surface(X, Y, Z1, rstride=1, cstride=1, cmap='rainbow')\n5、热力图\nfig = plt.figure(figsize = (25.10))\nplt.subplot(1,2,1)\n# 关系热力图\nmask_corr = np.zeros_like(df.corr())\nmast_corr[np.triu_indices_from(mask_corr)] = True\nsns.heatmap(df.corr(), mask = mask_corr)\n6、直方图\n#x 指定每个 bin(箱子)分布的数据,对应 x 轴，binx 是总共有几条条状图，\nnormed 值密度,也就是每个条状图的占比例比,默认为 1\nplt.hist(binom_sim, bins=10, normed=True)\n\ncond = data['是否高质用户'] == 1\ndata[cond]['类目1消费金额'].hist(alpha=0.5, label='高质用户')\ndata[~cond]['类目1消费金额'].hist(color='r', alpha=0.5, label='非高质用户')\nplt.legend()\n三、缺失值处理\n1、检测\nmissing_sum = df.isnull().sum().sorted_values(ascending=False) //检测缺失数量\nmissing_rate = missing_sum/df.shape[0].sort_values(ascending=False)\nmissing_stat = pd.concat([missing_sum,missing_rate], keys=['missing_sum','missing_rate', axis = ‘columns’])\n2、删除\ndf.drop(columns = cols_to_drop,inplace=True)\ndel df[cols_to_drop]\n3、填充\n3.1 非监督填充方法一\nfrom sklearn.preprocessing import Imputer\ndata = Imputer(missing_values='NaN', strategy='most_frequent',\naxis=0) //strategy =mean/median/most_frequent\ndataMode = data.fit_transform(df)\n方法二、\nvalues = {'A': 0, 'B': 1, 'C': 2, 'D': 3}\ndf.fillna(value=values)\n3.2 算法填充\n参见具体算法\n\n四、异常值处理\n1、散点图检测\n具体见绘图\n2、利用决策树等算法预测\n具体见算法\n3、 3σ原则\nmin_mask = df[\"weight\"] < (df[\"weight\"].mean() - 3 *  df[\"weight\"].std())\nmax_mask = df[\"weight\"] > (df[\"weight\"].mean() + 3 * df[\"weight\"].std())\n# 只要满足上诉表达式的任一个就为异常值，所以这里使用位与运算\nmask = min_mask | max_mask\nprint(df.loc[mask,\"weight\"])\n\n4、IQR\niqr = Ser.quantile(0.75)-Ser.quantile(0.25)\nLow = Ser.quantile(0.25)-1.5*iqr\nUp = Ser.quantile(0.75)+1.5*iqr\nindex = (Ser< Low) | (Ser>Up)\nreturn index\n\n五、特征缩放\n1、标准化\n方法一、\nfrom sklearn.preprocessing import scale\nX =  scale(X)\n方法二、\nfrom sklearn.preprocessing import StandardScaler\nX = StandardScaler().fit_transform(X)\n2、最小值-最大值归一化\nfrom sklearn.preprocessing import MinMaxScaler\nX = MinMaxScaler(feature_range=(0, 1), copy=True).fit_transform(X)\n3、均值归一化\nmean=np.mean(x)\nmin=np.min(x)\nmax=np.max(x)\nMeanNormalization=(x-mean)/(max-min)\n4、缩放成单位向量\nlinalg = np.linalg.norm(x, ord=1)\nX=x/linalg\n\n六、数值离散化\n1、聚类算法\n详细见算法\n2、等宽划分\nx=pd.cut(X,5)\n3、等频划分\nx=pd.qcut(X,5)\n\n七、特征编码\n1、独热编码\nfrom sklearn.preprocessing import OneHotEncoder\nenc = OneHotEncoder(sparse = False)\nx = enc.fit_transform(iris.data)\n2、哑编码\npd.get_dummies(X) //转完后全部记得变为 int\nX = X.astype(np.int)\nX.info()\n\n\n\n\n3、LabelEncoder\n方法一、\nle = preprocessing.LabelEncoder()\nle.fit([\"paris\", \"paris\", \"tokyo\", \"amsterdam\"])\nlist(le.classes_) #['amsterdam', 'paris', 'tokyo']\nle.transform([\"tokyo\", \"tokyo\", \"paris\"]) # array([2, 2, 1]...)\nlist(le.inverse_transform([2, 2, 1]))#['tokyo', 'tokyo', 'paris']\n方法二、\ndf['Sex'] = df['Sex'].map({'female': 1, 'male': 0})\n\n八、特征选择\n1、过滤法\n方法一、方差\nfrom sklearn.feature_selection import VarianceThreshold\nX_var=VarianceThreshold(threshold=0.5).fit_transform(X, y) \n #使用阈值0.5 进行选择，特征方差小于 0.5 的特征会被删除\n方法二、卡方\nfrom sklearn.feature_selection chi2,SelectKBest\nSelectKBest(chi2, k=2).fit_transform(iris.data, iris.target)\n方法三、互信息素\nfrom sklearn.feature_selection import mutual_info_classif\nX_mut = mutual_info_classif(X, y)\n\n2、包装法 RFE\nfrom sklearn.feature_selection import RFE\nfrom sklearn.linear_model import LogisticRegression\nx_rfe=RFE(estimator=LogisticRegression(), n_features_to_select=3).fit(X, y)\nprint(x_rfe.n_features_ ) # 所选特征的数量\nprint(x_rfe.support_ ) # 按特征对应位置展示所选特征，True 表示保留，False 表示剔除。\nprint(x_rfe.ranking_ ) # 特征排名，使得 ranking_[i]对应于第 i 个特征的排名位置。\nprint(x_rfe.estimator_ ) # 递归方法选择的基模型\n\n3、嵌入法\n方法一、逻辑回归\nprint(lr.coef_)\n方法二、L1 Lasso\nfrom sklearn.linear_model import Lasso\nls = Lasso()\nls.fit(X, Y)\nls.coef_\n方法三、随机森林\nrf = RandomForestRegressor(n_estimators=15, max_depth=6)\nboston_rf=rf.fit(X, y)\nboston_rf.feature_importances_\n\n九、降维\n1、PCA\nfrom sklearn.decomposition import PCA\nX_std = preprocessing.scale(X)\npca = PCA(n_components=2)\nX_pca =pca.fit(X_std).transform(X_std)\nprint(pca.explained_variance_ratio_)# 观测降维后特征信息量大小。\n2、LDA\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis\nas LDA\nlda = LDA(n_components=2)\nX_lda =lda.fit(X,y).transform(X)\nprint(lda.explained_variance_ratio_)\n\n十、模型评估调优\n1、常见普通\nfrom sklearn.metrics import accuracy_score, confusion_matrix, recall_score, f1_score,  fbeta_score\nprint(accuracy_score(y_true, y_pred))\nprint(fbeta_score(y_true, y_pred, beta=0.5))\n2、分类结果统计报告\nfrom sklearn.metrics import classification_report\nprint(classification_report(y_test, y_pred))\n3、样本划分\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y_labels, test_size=0.2)\n4、交叉验证\nfrom sklearn.model_selection import cross_val_score\ncv_scores = cross_val_score(rf_model, X_train,\ny_train,scoring=make_scorer(recall_score) ,cv=5)\nprint('mean f1_score socre of raw model{}'.format(np.mean(cv_scores)))\n\n\n\n5、网格调优\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.metrics import make_scorer\nmodel_gbr = GradientBoostingRegressor()\nparameters = {'loss': ['ls','lad','huber','quantile'],'min_samples_leaf':\n[1,2,3,4,5],'alpha': [0.1,0.3,0.6,0.9]}\nmodel_gs = GridSearchCV(estimator=model_gbr, param_grid=parameters, cv=5, scoring=make_scorer(f1_score))\nmodel_gs.fit(X_train,y_train)\nprint('Best score is:', model_gs.best_score_)\nprint('Best parameter is:', model_gs.best_params_)\n6、过采样\nfrom imblearn.over_sampling import SMOTE\nsmote_model = SMOTE(random_state=7, ratio=0.1)\nX_train_res_rf,y_train_res_rf = smote_model.fit_resample(X_train,y_train)\nprint('Resampled dataset shape {}'.format(y_train_res_rf))\n#过采样比例为 0.1，即目标的正负样本比例为 1:10\n\n十一、其他小知识点\n1、文件操作：\n写文件：\n# 使用 write 方法写文件\nwith open(\"f.txt\", \"w\") as f:\nf.write( \"www.huawei.com\")\n读取文件：\n# 使用 read 方法读取\nwith open(\"f.txt\", \"r\") as f:\nprint(f.read())\n2、实验模型的导出以及导入\nimport pickle\nfrom sklearn.externals import joblib\njoblib.dump(svm, 'svm.pkl')\nsvm = joblib.load('svm.pkl')\n3、间隔取值\nlist[::2 ] #list 间隔取值\ndf = df[[i%2==0 for i in range(len(df.index))]] #df 按行取样本\ndf=df.iloc[:,[i%2==0 for i in range(len(df.columns))]]#df按列取样本\n\n\n\n\n4、取序号值\narr = np.array([3, 22, 4, 11, 2, 44, 9])\nprint(np.max(arr))  # 44\nprint(np.argmax(arr))  # 5\nprint(np.argmin(arr))  # 4\nprint(np.where(arr > 4, arr - 10, arr * 10))  # [10 20 30 40 -5 -4 -3 -2]\n\ndef modthree(x):\n    return x % 3 ==0\nprint(np.where(modthree(arr), arr - 10, arr * 10))  # [ -7 220  40 110  20 440  -1]\n\narr = np.array([3, 4, 5, 6, 7])\nprint(np.argwhere(arr % 2 != 0))\nprint(np.argwhere(arr % 2 != 0).flatten())  # [0 2 4]  可以用来取序号\n3、浮点数两位\ndf.round(2)\n5、斐波那契数列\n# 位置参数\ndef fibs(num):\nresult = [0,1]# 新建列表存储数列的值\nfor i in range(2,num):# 循环 num-2 次\na = result[i-1] + result[i-2]\n# 将值追加至列表\nresult.append(a)\n# 返回列表\nreturn result\nfibs(5)\n# 输出：[0, 1, 1, 2, 3]\n\n十二、概率论：\n1、二项分布贝努力\nfrom scipy.stats import binom\nbinom_sim = binom.rvs(n=10, p=0.3, size=10000) #\n2、泊松分布\nX= np.random.poisson(lam=2, size=10000)\n3、正态分布\nfrom scipy.stats import norm\nx = np.arange(-5, 5, 0.1)\ny = norm.pdf(x, mu, sigma)\n4、指数分布\nfrom scipy.stats import expon\nx = np.arange(0, 15, 0.1)\ny = expon.pdf(x, lam)\n"
    },
    {
        "title": "电信用户分析,聚类",
        "content": "import pandas as pd\nimport numpy as np\nimport matplotlib.pylab as plt\n%matplotlib inline\n\nX = pd.read_csv('./telecom.csv', encoding='utf-8')\nprint(X.shape)\nX.head()\n\n# 数据预处理\nfrom sklearn import preprocessing\nX_scaled = preprocessing.scale(X)  # scale操作之后的数据零均值，单位方差（方差为1）\nX_scaled[0:5]\n\n# 进行PCA数据降维\nfrom sklearn.decomposition import PCA\n \n# 生成PCA实例\npca = PCA(n_components=3)  # 把维度降至3维\n# 进行PCA降维\nX_pca = pca.fit_transform(X_scaled)\n# 生成降维后的dataframe\nX_pca_frame = pd.DataFrame(X_pca, columns=['pca_1', 'pca_2', 'pca_3'])  # 原始数据由(30000, 7)降维至(30000, 3)\nX_pca_frame.head()\n\n# 训练简单模型\nfrom sklearn.cluster import KMeans\n \n# KMeans算法实例化，将其设置为K=10\nest = KMeans(n_clusters=10)\n \n# 作用到降维后的数据上\nest.fit(X_pca)\n\n# 取出聚类后的标签\nkmeans_clustering_labels = pd.DataFrame(est.labels_, columns=['cluster'])  # 0-9,一共10个标签\n \n# 生成有聚类后的dataframe\nX_pca_frame = pd.concat([X_pca_frame, kmeans_clustering_labels], axis=1)\n \nX_pca_frame.head()\n\n# 对不同的k值进行计算，筛选出最优的K值\nfrom mpl_toolkits.mplot3d import Axes3D  # 绘制3D图形\nfrom sklearn import metrics\n \n# KMeans算法实例化，将其设置为K=range(2, 14)\nd = {}\nfig_reduced_data = plt.figure(figsize=(12, 12))  #画图之前首先设置figure对象，此函数相当于设置一块自定义大小的画布，使得后面的图形输出在这块规定了大小的画布上，其中参数figsize设置画布大小\nfor k in range(2, 14):\n    est = KMeans(n_clusters=k, random_state=111)\n    # 作用到降维后的数据上\n    y_pred = est.fit_predict(X_pca)\n    # 评估不同k值聚类算法效果\n    calinski_harabaz_score = metrics.calinski_harabasz_score(X_pca_frame, y_pred)  # X_pca_frame：表示要聚类的样本数据，一般形如（samples，features）的格式。y_pred：即聚类之后得到的label标签，形如（samples，）的格式\n    d.update({k: calinski_harabaz_score})\n    print('calinski_harabaz_score with k={0} is {1}'.format(k, calinski_harabaz_score))  # CH score的数值越大越好\n    # 生成三维图形，每个样本点的坐标分别是三个主成分的值\n    ax = plt.subplot(4, 3, k - 1, projection='3d') #将figure设置的画布大小分成几个部分，表示4(row)x3(colu),即将画布分成4x3，四行三列的12块区域，k-1表示选择图形输出的区域在第k-1块，图形输出区域参数必须在“行x列”范围\n    ax.scatter(X_pca_frame.pca_1, X_pca_frame.pca_2, X_pca_frame.pca_3, c=y_pred)  # pca_1、pca_2、pca_3为输入数据，c表示颜色序列\n    ax.set_xlabel('pca_1')\n    ax.set_ylabel('pca_2')\n    ax.set_zlabel('pca_3')\n\n# 绘制不同k值对应的score，找到最优的k值\nx = []\ny = []\nfor k, score in d.items():\n    x.append(k)\n    y.append(score)\n \nplt.plot(x, y)\nplt.xlabel('k value')\nplt.ylabel('calinski_harabaz_score')\n\nX.index = X_pca_frame.index  # 返回：RangeIndex(start=0, stop=30000, step=1)\n \n# 合并原数据和三个主成分的数据\nX_full = pd.concat([X, X_pca_frame], axis=1)\nX_full.head()\n\n# 按每个聚类分组\ngrouped = X_full.groupby('cluster')\n \nresult_data = pd.DataFrame()\n# 对分组做循环，分别对每组进行去除异常值处理\nfor name, group in grouped:\n    # 每组去除异常值前的个数\n    print('Group:{0}, Samples before:{1}'.format(name, group['pca_1'].count()))\n\n    desp = group[['pca_1', 'pca_2', 'pca_3']].describe() # 返回每组的数量、均值、标准差、最小值、最大值等数据\n    for att in ['pca_1', 'pca_2', 'pca_3']:\n        # 去异常值：箱形图\n        lower25 = desp.loc['25%', att]\n        upper75 = desp.loc['75%', att]\n        IQR = upper75 - lower25\n        min_value = lower25 - 1.5 * IQR\n        max_value = upper75 + 1.5 * IQR\n        # 使用统计中的1.5*IQR法则，删除每个聚类中的噪音和异常点\n        group = group[(group[att] > min_value) & (group[att] < max_value)]\n    result_data = pd.concat([result_data, group], axis=0)\n    # 每组去除异常值后的个数\n    print('Group:{0}, Samples after:{1}'.format(name, group['pca_1'].count()))\nprint('Remain sample:', result_data['pca_1'].count())\n\n# 设置每个簇对应的颜色\ncluster_2_color = {0: 'red', 1: 'green', 2: 'blue', 3: 'yellow', 4: 'cyan', 5: 'black', 6: 'magenta', 7: '#fff0f5',\n                   8: '#ffdab9', 9: '#ffa500'}\n \ncolors_clustered_data = X_pca_frame.cluster.map(cluster_2_color)  # 簇名和颜色映射\nfig_reduced_data = plt.figure()\nax_clustered_data = plt.subplot(111, projection='3d')\n \n# 聚类算法之后的不同簇数据的映射为不同颜色\nax_clustered_data.scatter(X_pca_frame.pca_1.values, X_pca_frame.pca_2.values, X_pca_frame.pca_3.values,\n                          c=colors_clustered_data)\nax_clustered_data.set_xlabel('Component_1')\nax_clustered_data.set_ylabel('Component_2')\nax_clustered_data.set_zlabel('Component_3')\n\n# 筛选后的数据聚类可视化\ncolors_filtered_data = result_data.cluster.map(cluster_2_color)\nfig = plt.figure(figsize=(12,12))\nax = plt.subplot(111, projection='3d')\nax.scatter(result_data.pca_1.values, result_data.pca_2.values, result_data.pca_3.values, c=colors_filtered_data)\nax.set_xlabel('Component_1')\nax.set_ylabel('Component_2')\nax.set_zlabel('Component_3')\n\n# 查看各族中的每月话费情况\nmonthly_Fare = result_data.groupby('cluster').describe().loc[:, u'每月话费']\nmonthly_Fare\n\n# mean：均值；std：标准差\nmonthly_Fare[['mean', 'std']].plot(kind='bar', rot=0, legend=True)  # rot可以控制轴标签的旋转度数。legend是否在图上显示图例\n\n# 查看各族中的入网时间情况\naccess_time = result_data.groupby('cluster').describe().loc[:, u'入网时间']\naccess_time\naccess_time[['mean', 'std']].plot(kind='bar', rot=0, legend=True, title='Access Time')\n\n# 查看各族中的欠费金额情况\narrearage = result_data.groupby('cluster').describe().loc[:, u'欠费金额']\narrearage[['mean', 'std']].plot(kind='bar', rot=0, legend=True, title='Arrearage')\n\n# 综合描述\nnew_column = ['Access_time', u'套餐价格', u'每月流量', 'Monthly_Fare', u'每月通话时长', 'Arrearage', u'欠费月份数', u'pca_1', u'pca_2',\n              u'pca_3', u'cluster']\nresult_data.columns = new_column\nresult_data.groupby('cluster')[['Monthly_Fare', 'Access_time', 'Arrearage']].mean().plot(kind='bar')  # 每个簇的Monthly_Fare、Access_time、Arrearag的均值放在一块比较"
    },
    {
        "title": "手写线性回归",
        "content": "import numpy as np\n\ndef simple_linear_regression(x, y):\n    # 计算x和y的平均值\n    x_mean = np.mean(x)\n    y_mean = np.mean(y)\n    \n    # 计算权重w\n    w = np.sum((x - x_mean) * (y - y_mean)) / np.sum((x - x_mean) ** 2)\n    \n    # 计算偏置b\n    b = y_mean - w * x_mean\n    \n    return w, b\n\ndef predict(x, w, b):\n    # 使用模型参数进行预测\n    return w * x + b\n\n# 示例数据\nx = np.array([10, 4, 6])\ny = np.array([8, 2, 5])\n\n# 训练模型并获取参数\nw, b = simple_linear_regression(x, y)\n\n# 使用模型进行预测\nx_new = np.array([12])\ny_pred = predict(x_new, w, b)\n\nprint(f\"预测结果: y = {w:.2f} * x + {b:.2f}\")\nprint(f\"对于x = {x_new[0]}, 预测的y值是: {y_pred[0]:.2f}\")\n\n\n*基于线性回归算法实现\nfrom sklearn.linear_model import LinearRegression\nmodel=LinearRegression()\nmodel.fit(data,y)\nprint('基于线性逻辑回归算法',model.coef_,model.intercept_)"
    },
    {
        "title": "方差选择模型（VarianceThreshold）",
        "content": "* 题目说阈值为1，但是hreshold=填空，有如下的备注，我就调整了hreshold，用了另外的判断方法，输出的结果为True False。要看True的值。\nfrom sklearn.feature_selection import VarianceThreshold\n\n# 定义方差选择模型，定义方差系数\nmodel_vt = VarianceThreshold(threshold=0.7) #，输出超过3个小于6个。"
    },
    {
        "title": "SMOTE过采样",
        "content": "pip install imbalanced-learn\nfrom imblearn.over_sampling import SMOTE\nfrom sklearn.datasets import make_classification\nfrom sklearn.model_selection import train_test_split\n\n# 创建一个不平衡的数据集\nX, y = make_classification(n_classes=2, class_sep=2,\n                           weights=[0.1, 0.9], n_informative=3, n_redundant=1, flip_y=0,\n                           n_features=20, n_clusters_per_class=1, n_samples=1000, random_state=10)\n\n# 划分数据集\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=0)\n\n# 实例化 SMOTE\nsm = SMOTE(random_state=42)\n\n# 应用 SMOTE\nX_res, y_res = sm.fit_resample(X_train, y_train)\n\n# 现在 X_res 和 y_res 包含了过采样后的训练数据\n"
    },
    {
        "title": "最小二乘法",
        "content": "import numpy as np  \nimport scipy as sp  \nimport pylab as pl  \nfrom scipy.optimize import leastsq  # 引入最小二乘函数  \n\nn = 9  # 多项式次数  \n定义目标函数：  \ndef real_func(x):  \n  #目标函数：sin(2*pi*x)\n    return np.sin(2 * np.pi * x)  \n定义多项式函数，用多项式去拟合数据:  \ndef fit_func(p, x):  \n    f = np.poly1d(p)  \n    return f(x)  \n定义残差函数，残差函数值为多项式拟合结果与真实值的差值：  \ndef residuals_func(p, y, x):  \n    ret = fit_func(p, x) - y  \n    return ret  \n\nx = np.linspace(0, 1, 9)  # 随机选择9个点作为x  \nx_points = np.linspace(0, 1, 1000)  # 画图时需要的连续点  \ny0 = real_func(x)  # 目标函数  \ny1 = [np.random.normal(0, 0.1) + y for y in y0]  # 在目标函数上添加符合正态分布噪声后的函数  \np_init = np.random.randn(n)  # 随机初始化多项式参数  \n# 调用scipy.optimize中的leastsq函数，通过最小化误差的平方和来寻找最佳的匹配函数\n#func 是一个残差函数，x0 是计算的初始参数值，把残差函数中除了初始化以外的参数打包到args中\nplsq = leastsq(func=residuals_func, x0=p_init, args=(y1, x))  \n\nprint('Fitting Parameters: ', plsq[0])  # 输出拟合参数  \n\npl.plot(x_points, real_func(x_points), label='real')  \npl.plot(x_points, fit_func(plsq[0], x_points), label='fitted curve')  \npl.plot(x, y1, 'bo', label='with noise')  \npl.legend()  \npl.show()"
    },
    {
        "title": "美国人口收入分析",
        "content": "import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import LabelEncoder, StandardScaler\nfrom sklearn.neighbors import KNN\nfrom sklearn.metrics import classification_report, confusion_matrix\nfrom sklearn.model_selection import GridSearchCV\n\n# 1、加载数据，查看数据行列分布情况\ndata = pd.read_csv('adult_inconn.csv')\ndata.shape\n\n# 2、查看数据特征，最大值，最小值，中位数，平均数以及四分位数\ndata.describe()\n\n# 3、查看数据缺失值分布，并用柱形图表示\nmissing_values = data.isnull().sum()\nmissing_values.plot(kind='bar')\nmissing_values[missing_values > 0].plot(kind='bar', color='skyblue')\nplt.title(\"缺失值分布\")\nplt.xlabel(\"特征\")\nplt.ylabel(\"缺失值数量\")\nplt.show()\n# 4、创建新数据集、对Predclass 标签进行转化\n# 这里假设Predclass是目标变量，我们需要将其转换为数值型\ndata['Predclass'] = data['Predclass'].map({'<=50K': 0, '>50K': 1})\n\n# 5、使用cut方法对数据进行分箱，分10个箱\n# 这里以年龄为例进行分箱\ndata['age_bins'] = pd.cut(data['age'], bins=10, right=False)\n\n# 6、绘制分箱后的数据分布图\nage_distribution = data['age_bins'].value_counts().sort_index().plot(kind='bar')\n\n# 7、属性衍生 - 这里我们以年龄和收入为例创建一个新属性\ndata['age_income'] = data['age'] * data['Predclass']\n创建新的“年龄组”或“收入水平”特征。\n# 举例，创建“年龄组”特征\ndata['age_group'] = pd.cut(data['age'], bins=[0, 30, 60, 90], labels=['青年', '中年', '老年'])\n\n# 8、查看sex-marital 分布图\nsex_marital_distribution = sns.countplot(data=data, x='sex', hue='marital_status')\n# 性别和婚姻状况分布图\nsns.countplot(data=data, x='sex', hue='marital-status')\nplt.title(\"性别与婚姻状况分布\")\nplt.show()\n# 9、数据特征值处理，属性编码\n# 对类别特征进行编码\nlabel_encoders = {}\nfor column in data.select_dtypes(include=['object']).columns:\n    le = LabelEncoder()\n    data[column] = le.fit_transform(data[column])\n    label_encoders[column] = le\n\n# 10、将df 转为str - 这一步似乎没有必要，可能是有误解\ndata = data.astype(str)\n\n# 11、引入KNN 对数据进行预测\n# 首先划分数据集\nX = data.drop('Predclass', axis=1)\ny = data['Predclass']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n# 标准化特征\nscaler = StandardScaler()\nX_train_scaled = scaler.fit_transform(X_train)\nX_test_scaled = scaler.transform(X_test)\n\n# 12、选择模型最优参数\nknn = KNN()\nparam_grid = {'n_neighbors': np.arange(1, 30)}\nknn_gscv = GridSearchCV(knn, param_grid, cv=5)\nknn_gscv.fit(X_train_scaled, y_train)\nbest_params = knn_gscv.best_params_\n\n# 13、重新生成模型knn_final\nknn_final = KNN(n_neighbors=best_params['n_neighbors'])\n\n# 14、对数据进行拟合\nknn_final.fit(X_train_scaled, y_train)\n\n# 15、重新对X_test 进行预测\ny_pred = knn_final.predict(X_test_scaled)\n\n# 16、导入分类模块，画出分布图\nconf_matrix = confusion_matrix(y_test, y_pred)\nsns.heatmap(conf_matrix, annot=True, fmt='d')\n\n# 输出相关结果\ndata_shape, data_description, missing_values, best_params, conf_matrix\n绘制预测结果分布图\nsns.countplot(x=y_pred)\nplt.title(\"预测结果分布\")\nplt.show()\n"
    },
 
    results = []
    for dict_item in json_data:
        results.append({'title': dict_item['title']})
    df = pd.DataFrame(results)
    df.style.set_properties(**{'white-space': 'pre-wrap', 'text-align': 'left'})

def daima_by_t(key):
    json_data=[
    {
        "title": "关联规则",
        "content": "# 读取数据并展示前五行\ndf = pd.read_csv('./data/online_retail_II.csv')\ndf.head()\n1.清理df数据：删除缺失值与重复值。\ndf.dropna(inplace=True)\ndf.drop_duplicates(inplace=True)\nfrom mlxtend.frequent_patterns import apriori, fpmax, fpgrowth\nfrom mlxtend.frequent_patterns import association_rules\n\n# 找到所有支持度超过0.03的项集\nbasket = df.groupby(['Invoice', 'StockCode'])['Quantity'].sum().unstack().fillna(0)\nbasket_sets = basket.applymap(lambda x: 1 if x > 0 else 0)\n使用 fpgrowth算法从数据basket_sets中计算频繁项集，并将最小支持度设置为 0.1。返回频繁项集frequent_itemsets\n\nfrequent_itemsets = fpgrowth(basket_sets, min_support=0.1, use_colnames=True)\nfrequent_itemsets\n使用association_rules从频繁项集frequent_itemsets中构建关联规则，metric为'confidence'， min_threshold为0.4。并将结果存储在rules中\n\nrules = association_rules(frequent_itemsets, metric='confidence', min_threshold=0.4)\n# 展示前五条关联规则\nrules.head()\n4.打印出同时满足置信度< 0.8，置信度>0.5，提升度>3.0的rules数据。\n/*此处由考生进行代码填写*/\nfiltered_rules = rules[(rules['confidence'] < 0.8) & (rules['confidence'] > 0.5) & (rules['lift'] > 3.0)]\n\n# 打印结果\nprint(filtered_rules)\n筛选规则rules，将规则中同时满足antecedent_sele为{'22111', '22271'}，consequent_sele为{'22272'}的规则选出来，并存储在final_sele中\n\n# 筛选 antecedents 和 consequents 满足条件的规则\nfinal_sele = (rules['antecedents'] == {'22111', '22271'}) & (rules['consequents'] == {'22272'})\n# 使用 final_sele 筛选出相应的规则\nrules.loc[final_sele ]"
    },
    {
        "title": "线性代数",
        "content": "生成一个包含整数0-11的向量\nx = np.arange(12)\n查看数组大小\nx.shape\n将x转换成二维矩阵，其中矩阵的第一个维度为1\nx = x.reshape(1,12)\n将x转换3x4的矩阵\nx = x.reshape(3,4)\n生成3*4的矩阵\nA = np.arange(12).reshape(3,4)\n转置\nA.T\n矩阵相乘：两个矩阵能够相乘的条件为第一个矩阵的列数等于第二个矩阵的行数。\nnp.matmul(A,B)\n矩阵对应运算：针对形状相同矩阵的运算统称，包括元素对应相乘、相加等，即对两个矩阵相同位置的元素进行加减乘除等运算。\n矩阵相乘：A*A\n矩阵相加：A + A\n逆矩阵实现：只有方阵才有逆矩阵\nA = np.arange(4).reshape(2,2)\nnp.linalg.inv(A)\n矩阵的特征值与特征向量\nA = [[1, 2],[2, 1]] #生成一个2*2的矩阵\nfrom scipy.linalg import eig\nevals, evecs = eig(A) #求A的特征值（evals）和特征向量(evecs)\n行列式\nnp.linalg.det(A)\n解线性方程组\nfrom scipy.linalg import solve\nx = solve(a, b)\n奇异值分解\n"
    },
    {
        "title": "概率论",
        "content": "ll = [[1,2,3,4,5,6],[3,4,5,6,7,8]]\nnp.mean(ll)  #全部元素求均值\nnp.mean(ll,0) #按列求均值，0代表列向量，1表示行向量\n求方差：\nnp.var(b)\nnp.var(ll,1)) #第二个参数为1，表示按行求方差\n标准差\nnp.std(ll)\n相关系数\nnp.corrcoef(a,b)\n二项分布\nfrom scipy.stats import binom, norm, beta, expon\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n#n,p对应二项式公式中的事件成功次数及其概率，size表示采样次数\nbinom_sim = binom.rvs(n=10, p=0.3, size=10000)\nprint('Data:',binom_sim)\nprint('Mean: %g' % np.mean(binom_sim))\nprint('SD: %g' % np.std(binom_sim, ddof=1))\n#生成直方图，x指定每个bin(箱子)分布的数据,对应x轴，binx是总共有几条条状图，normed值密度,也就是每个条状图的占比例比,默认为1\nplt.hist(binom_sim, bins=10, normed=True)\nplt.xlabel(('x'))\nplt.ylabel('density')\nplt.show()\n泊松分布\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n#产生10000个符合lambda=2的泊松分布的数\nX= np.random.poisson(lam=2, size=10000)  \n\na = plt.hist(X, bins=15, normed=True, range=[0, 15])\n#生成网格\nplt.grid()\nplt.show()\n正态分布\nimport matplotlib.pyplot as plt\nmu = 0\nsigma = 1\n#分布采样点\nx = np.arange(-5, 5, 0.1)\n#生成符合mu,sigma的正态分布\ny = norm.pdf(x, mu, sigma)\nplt.plot(x, y)\nplt.xlabel('x')\nplt.ylabel('density')\nplt.show()\n指数分布\nfrom scipy.stats import expon\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nlam = 0.5\n#分布采样点\nx = np.arange(0, 15, 0.1)\n#生成符合lambda为0.5的指数分布\ny = expon.pdf(x, lam)\nplt.plot(x, y)\nplt.title('Exponential: lam=%.2f' % lam)\nplt.xlabel('x')\nplt.ylabel('density')\nplt.show()\n\n中心极限定理\nimport numpy as np\nimport matplotlib.pyplot as plt\n#随机产生10000个范围为(1,6)的数\nramdon_data = np.random.randint(1,7,10000)\nprint(ramdon_data.mean())\nprint(ramdon_data.std())\n生成直方图\nplt.figure()\nplt.hist(ramdon_data,bins=6,facecolor='blue')\nplt.xlabel('x')\nplt.ylabel('n')\nplt.show()\n随机抽取1000组数据，每组50个\nsamples = []\nsamples_mean =[]\nsamples_std = []\n\n#从生成的1000个数中随机抽取1000组\nfor i in range(0,1000):\nsample = []\n#每组随机抽取50个数\n    for j in range(0,50):\n        sample.append(ramdon_data[int(np.random.random() * len(ramdon_data))])\n  #将这50个数组成一个array放入samples列表中\n    sample_ar = np.array(sample)\nsamples.append(sample_ar)\n#保存每50个数的均值和标准差\n    samples_mean.append(sample_ar.mean())\nsamples_std.append(sample_ar.std())\n#samples_std_ar = np.array(samples_std)\n#samples_mean_ar = np.array(samples_mean)\n# print(samples_mean_ar)\n梯度下降法\n训练集(x,y)共5个样本,每个样本点有3个分量 (x0,x1,x2)  \nx = [(1, 0., 3), (1, 1., 3), (1, 2., 3), (1, 3., 2), (1, 4., 4)]  \ny = [95.364, 97.217205, 75.195834, 60.105519, 49.342380]  y[i] 样本点对应的输出  \nepsilon = 0.0001  #迭代阀值，当两次迭代损失函数之差小于该阀值时停止迭代  \nalpha = 0.01  #学习率\ndiff = [0, 0]  \nmax_itor = 1000  \nerror1 = 0  \nerror0 = 0  \ncnt = 0  \nm = len(x)  \n#初始化参数  \ntheta0 = 0  \ntheta1 = 0  \ntheta2 = 0  \nwhile True:  \n    cnt += 1  \n\n    # 参数迭代计算  \n    for i in range(m):  \n        # 拟合函数为 \ny = theta0 * x[0] + theta1 * x[1] +theta2 * x[2]  \n        # 计算残差，即拟合函数值-真实值  \n        diff[0] = (theta0** x[i][0] + theta1 * x[i][1] + theta2 * x[i][2]) - y[i]  \n  \n        # 梯度 = diff[0] * x[i][j]。根据步长*梯度更新参数 \n        theta0 -= alpha * diff[0] * x[i][0]  \n        theta1 -= alpha * diff[0] * x[i][1]  \n        theta2 -= alpha * diff[0] * x[i][2]  \n  \n    # 计算损失函数  \n    error1 = 0  \n    for lp in range(len(x)):  \n        error1 += (y[lp]-(theta0 + theta1 * x[lp][1] + theta2 * x[lp][2]))**2/2  \n    #若当两次迭代损失函数之差小于该阀值时停止迭代，跳出循环；\n    if abs(error1-error0) < epsilon:  \n        break  \n    else:  \n        error0 = error1  \n  \n    print(' theta0 : %f, theta1 : %f, theta2 : %f, error1 : %f' % (theta0, theta1, theta2, error1) )  \n\nprint('Done: theta0 : %f, theta1 : %f, theta2 : %f' % (theta0, theta1, theta2)  )\nprint('迭代次数: %d' % cnt  )\n"
    },
    {
        "title": "斐波那契数列",
        "content": "def fibonacci(n):\nif n==1:\nreturn [0]\nif n==2:\nreturn [0,1]\n    fib = [0, 1]\n    for i in range(2, n):\n        next_value = fib[i-1] + fib[i-2]\n        fib.append(next_value)\n    return fib[:n]\n\n# 生成前10个斐波那契数列的值\nfibonacci_10 = fibonacci(10)"
    },
    {
        "title": "字典",
        "content": "# 创建字典并存储元素\ndict_data = {'name':'lee', 'age':22, 'gender':'male'}\n\n查看字典items\ndict_data.items()\n插入一个键值对\nd[\"mark\"]=99\n# 分别将字典中的键和值全部输出\nkeys = list(dict_data.keys())\nvalues = list(dict_data.values())\n\n# 获取字典中键'name'对应的值\nname_value = dict_data['name']\n# 1. 新建字典 Dict1\nDict1 = {'name': 'lee', 'age': 89, 'num': [1, 2, 8]}\n\n# 2. 浅拷贝 Dict1，副本命名为 Dict_copy\nDict_copy = Dict1.copy()\n\n# 3. 创建集合，命名为 sample_set，包含2个元素 \"Prince\", \"Techs\"\nsample_set = {\"Prince\", \"Techs\"}\n\n# 4. 检查集合 sample_set 中是否存在某一元素 \"Data\" 并打印结论\nexists_data = \"Data\" in sample_set\ncheck_conclusion = \"存在\" if exists_data else \"不存在\"\n\n# 5. 向集合 sample_set 中增加元素 \"Data\"\nsample_set.add(\"Data\")\n\n# 6. 将元素 'Techs' 从集合 sample_set 中移除\nsample_set.remove(\"Techs\")\n\n# 7. 将集合 sample_set 分别转换为元组和列表结构，并打印输出\nsample_set_tuple = tuple(sample_set)\nsample_set_list = list(sample_set)"
    },
    {
        "title": "列表，冒泡排序",
        "content": "# 创建列表并存储元素 [310, 7]\nlst = [310, 7]\n\n# 打印输出列表\nprint(lst)\n#在末尾添加一个元素3\nlst.append(3)\n# 在列表元素下标为2的位置插入元素5\nlst.insert(2, 5)\n#列表从小到大排序\nlst.sort()\n从大到小\nlst.sort(reverse=True)\n# 冒泡排序，从大到小\nfor i in range(len(lst)):\n    for j in range(0, len(lst)-i-1):\n        if lst[j] < lst[j+1]:\n            lst[j], lst[j+1] = lst[j+1], lst[j]\n\n# 打印排序后的列表\nprint(lst)\n\n# 在列表末尾位置添加元素 'fish'\nlst.append('fish')\n将最后一个元素替换为100\nlst[-1]=100\n\n\n# 将列表中的字符串元素 'fish' 转换为全部大写并替换原本的 'fish'\nlst[lst.index('fish')] = 'FISH'\n字符串a=\"Victory\"\na.upper()全改为大写\na.lower()全改为小写\na.title()字符串改为首字母大写\n"
    },
    {
        "title": "集合",
        "content": "#创建集合\ns=set([1,2])\ns={1,2}\ns.add(3)\n# 删除元素（如果元素不存在，会引发KeyError）\ns.remove(2)\n清空集合\nset1.clear()\nset1.update([6, 7, 8])  # 添加元素，可以是列表、元组、字典等\n# 2. 检查集合中是否在元素'numpy'\ncontains_numpy = 'numpy' in sample\n# 3. 删除集合中的元素'pandas'\nsample.discard('pandas')\n# 4. 将集合分别转化为列表和元组并输出\nlist_from_set = list(sample)\ntuple_from_set = tuple(sample)\n\n# 5. 使用 copy()对集合进行浅拷贝\nsample_copy = sample.copy()\n"
    },
    {
        "title": "总-代码总结",
        "content": "一、算法\n1、线性回归\nfrom sklearn.linear_model import LinearRegression\nmodel.coef_ w 系数 model.intercept_ 截距\n2、逻辑回归\nfrom sklearn.linear_model import LogisticRegression\n3、KNN\nfrom sklearn.neighbors import KneiborsClassifier\nKneiborsClassifier(n_neighbors =4, algorithm = “ball_tree”)\n4、朴素贝叶斯\nfrom sklearn.naive_bayes import BernoulliNB\n5、SVM\nfrom sklearn.svm import svc\nsvc(c = 1.0, kernel = “rbf”)\nc：惩罚系数，默认是 0，C 越小，泛化能力越强。\nKernel：核函数 rbf 是径向基（高斯）此外还有线性 linear、多项式、poly、sigmoid\n6、决策树\nfrom sklearn.tree import DecisionTreeClassifier\n7、集成算法\nfrom sklearn.ensemble import RandomForestClassifier,  BaggingClassifier，\nAdaBoostClassifier，GradientBosstingClassifier\n\n通用 参数 n_estimators = 10 ,基类学习器个数。\nGradientBosstingClassifier(max_depth = 5)\nmax_depth = 5 树最大深度\nfrom xgboost.sklearn import XGBClassifier\n8、Kmeans\nfrom sklearn.cluster import Kmeans\nKmeans(n_clusters = k, init=’k_means++’,max_iter = 300)\nmodel.labels_ 类标签\nmodel.cluster_centers_ 簇中心\n\nestimator = KMeans(n_clusters=3)\nestimator.fit(X)\nlabel_pred = estimator.labels_\nx0 = X[label_pred == 0]\nx1 = X[label_pred == 1]\nx2 = X[label_pred == 2]\n\n\n\n\n9、AgglomerativeClustering\nfrom sklearn.cluster import AgglomerativeClustering\nAgglomerativeClustering(n_clusters=2, affinity=“euclidean”, compute_full_tree,linkage = “ward”）\ncompute_full_tree = False 时，训练 n_cluesters 后，训练停止；True 则训练整颗树。\nlinkage 的参数为\n{“ward”, “complete”,“average”, “single”}\n\n10、Birch\nfrom sklearn.cluster import Birch\nBirch(threshold=0.5, branching_factor=50, n_clusters=3)\nthreshold：float，表示设定的半径阈值，默认 0.5。\nbranching_factor：int，默认=50，每个节点最大特征树子集群数。\nn_clusters：int，默认=3，最终聚类数目。\n11、DBSCAN\nfrom sklearn.cluster import DBSCAN\nDBSCAN(eps=0.5, min_samples=5, metric=’euclidean)\neps：两个样本被看作邻居节点的最大距离。\nmin_samples：最小簇的样本数。\nmetric：距离计算方式，euclidean 为欧氏距离计算。\n12、ariori\nfrom mlxtend.frequent_patterns import apriori\nApriori(df, min_support=0.5, use_colnames=False, max_len=None,\nn_jobs=1)\n其中 df 代表数据框数据集，min_support 表示指定的最小支持度，\nuse_colnames=True 表示使用元素名字，默认的 False 使用列名代表元素，max_len 表\n示生成的项目集的最大长度。如果为 None，则评估所有可能的项集长度。\n二、画图\nplt.xlabel('pca1')\nplt.ylabel('pca2')\nplt.title(\"PCA\")\nplt.legend(loc='lower left')\n1、条形图\n数据为：\ngrouped = data.groupby(['是否高质用户', '网络类型'])['用户标识符'].count().unstack()\n\n网络类型 2G 3G 4G\n是否高质用户   \n0 1630.0 1668.0 1699.0\n1 NaN 2530.0 2473.0\ndf.plot.bar(rot = 0) //直接 df 画图 rot 是下标志是否字体是立着的还是卧着的。\ngrouped.plot(kind='bar', alpha=1.0, rot=0)\n2、散点图\ndata.plot.scatter(x = '类目 2 消费金额', y = 6, c = 'br')\nx = 为列名 列名可以用字符串 或如 y 的序列序数\nc = 'br'是颜色， 如蓝色和红色。\nplt.scatter(data.loc[:, '类目 2 消费金额'], data.loc[:, '类目 3 消费金额'], c=y,alpha=0.5)\n# 生成数据可视化\ny = data.loc[:, '是否高质用户']\nplt.scatter(data.loc[:, '类目 2 消费金额'], data.loc[:, '类目 3 消费金额'], c=y,alpha=0.5)\n这种图的颜色点由 y 决定。\n3、饼图\ngrouped.plot.pie(autopct = '%0.01f', subplots = True)\nautopct 为是否在饼图画百分比；Subplots = True 为是否为每个列画单独子图。\n4、3D 画图\nfrom mpl_toolkits.mplot3d import Axes3D\nfig = plt.figure()\n方法一\nax = fig.gca(projection='3d')\n方法二\nax = Axes3D(fig)\nax.scatter(X[y==i ,0], X[y==i, 1], X[y==i,2], c=c,marker=m,\nlabel=l)\n#做三维曲面图，rstride 和 cstride 分别代表行和列的跨度，cmap:曲面颜色\nax.plot_surface(X, Y, Z1, rstride=1, cstride=1, cmap='rainbow')\n5、热力图\nfig = plt.figure(figsize = (25.10))\nplt.subplot(1,2,1)\n# 关系热力图\nmask_corr = np.zeros_like(df.corr())\nmast_corr[np.triu_indices_from(mask_corr)] = True\nsns.heatmap(df.corr(), mask = mask_corr)\n6、直方图\n#x 指定每个 bin(箱子)分布的数据,对应 x 轴，binx 是总共有几条条状图，\nnormed 值密度,也就是每个条状图的占比例比,默认为 1\nplt.hist(binom_sim, bins=10, normed=True)\n\ncond = data['是否高质用户'] == 1\ndata[cond]['类目1消费金额'].hist(alpha=0.5, label='高质用户')\ndata[~cond]['类目1消费金额'].hist(color='r', alpha=0.5, label='非高质用户')\nplt.legend()\n三、缺失值处理\n1、检测\nmissing_sum = df.isnull().sum().sorted_values(ascending=False) //检测缺失数量\nmissing_rate = missing_sum/df.shape[0].sort_values(ascending=False)\nmissing_stat = pd.concat([missing_sum,missing_rate], keys=['missing_sum','missing_rate', axis = ‘columns’])\n2、删除\ndf.drop(columns = cols_to_drop,inplace=True)\ndel df[cols_to_drop]\n3、填充\n3.1 非监督填充方法一\nfrom sklearn.preprocessing import Imputer\ndata = Imputer(missing_values='NaN', strategy='most_frequent',\naxis=0) //strategy =mean/median/most_frequent\ndataMode = data.fit_transform(df)\n方法二、\nvalues = {'A': 0, 'B': 1, 'C': 2, 'D': 3}\ndf.fillna(value=values)\n3.2 算法填充\n参见具体算法\n\n四、异常值处理\n1、散点图检测\n具体见绘图\n2、利用决策树等算法预测\n具体见算法\n3、 3σ原则\nmin_mask = df[\"weight\"] < (df[\"weight\"].mean() - 3 *  df[\"weight\"].std())\nmax_mask = df[\"weight\"] > (df[\"weight\"].mean() + 3 * df[\"weight\"].std())\n# 只要满足上诉表达式的任一个就为异常值，所以这里使用位与运算\nmask = min_mask | max_mask\nprint(df.loc[mask,\"weight\"])\n\n4、IQR\niqr = Ser.quantile(0.75)-Ser.quantile(0.25)\nLow = Ser.quantile(0.25)-1.5*iqr\nUp = Ser.quantile(0.75)+1.5*iqr\nindex = (Ser< Low) | (Ser>Up)\nreturn index\n\n五、特征缩放\n1、标准化\n方法一、\nfrom sklearn.preprocessing import scale\nX =  scale(X)\n方法二、\nfrom sklearn.preprocessing import StandardScaler\nX = StandardScaler().fit_transform(X)\n2、最小值-最大值归一化\nfrom sklearn.preprocessing import MinMaxScaler\nX = MinMaxScaler(feature_range=(0, 1), copy=True).fit_transform(X)\n3、均值归一化\nmean=np.mean(x)\nmin=np.min(x)\nmax=np.max(x)\nMeanNormalization=(x-mean)/(max-min)\n4、缩放成单位向量\nlinalg = np.linalg.norm(x, ord=1)\nX=x/linalg\n\n六、数值离散化\n1、聚类算法\n详细见算法\n2、等宽划分\nx=pd.cut(X,5)\n3、等频划分\nx=pd.qcut(X,5)\n\n七、特征编码\n1、独热编码\nfrom sklearn.preprocessing import OneHotEncoder\nenc = OneHotEncoder(sparse = False)\nx = enc.fit_transform(iris.data)\n2、哑编码\npd.get_dummies(X) //转完后全部记得变为 int\nX = X.astype(np.int)\nX.info()\n\n\n\n\n3、LabelEncoder\n方法一、\nle = preprocessing.LabelEncoder()\nle.fit([\"paris\", \"paris\", \"tokyo\", \"amsterdam\"])\nlist(le.classes_) #['amsterdam', 'paris', 'tokyo']\nle.transform([\"tokyo\", \"tokyo\", \"paris\"]) # array([2, 2, 1]...)\nlist(le.inverse_transform([2, 2, 1]))#['tokyo', 'tokyo', 'paris']\n方法二、\ndf['Sex'] = df['Sex'].map({'female': 1, 'male': 0})\n\n八、特征选择\n1、过滤法\n方法一、方差\nfrom sklearn.feature_selection import VarianceThreshold\nX_var=VarianceThreshold(threshold=0.5).fit_transform(X, y) \n #使用阈值0.5 进行选择，特征方差小于 0.5 的特征会被删除\n方法二、卡方\nfrom sklearn.feature_selection chi2,SelectKBest\nSelectKBest(chi2, k=2).fit_transform(iris.data, iris.target)\n方法三、互信息素\nfrom sklearn.feature_selection import mutual_info_classif\nX_mut = mutual_info_classif(X, y)\n\n2、包装法 RFE\nfrom sklearn.feature_selection import RFE\nfrom sklearn.linear_model import LogisticRegression\nx_rfe=RFE(estimator=LogisticRegression(), n_features_to_select=3).fit(X, y)\nprint(x_rfe.n_features_ ) # 所选特征的数量\nprint(x_rfe.support_ ) # 按特征对应位置展示所选特征，True 表示保留，False 表示剔除。\nprint(x_rfe.ranking_ ) # 特征排名，使得 ranking_[i]对应于第 i 个特征的排名位置。\nprint(x_rfe.estimator_ ) # 递归方法选择的基模型\n\n3、嵌入法\n方法一、逻辑回归\nprint(lr.coef_)\n方法二、L1 Lasso\nfrom sklearn.linear_model import Lasso\nls = Lasso()\nls.fit(X, Y)\nls.coef_\n方法三、随机森林\nrf = RandomForestRegressor(n_estimators=15, max_depth=6)\nboston_rf=rf.fit(X, y)\nboston_rf.feature_importances_\n\n九、降维\n1、PCA\nfrom sklearn.decomposition import PCA\nX_std = preprocessing.scale(X)\npca = PCA(n_components=2)\nX_pca =pca.fit(X_std).transform(X_std)\nprint(pca.explained_variance_ratio_)# 观测降维后特征信息量大小。\n2、LDA\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis\nas LDA\nlda = LDA(n_components=2)\nX_lda =lda.fit(X,y).transform(X)\nprint(lda.explained_variance_ratio_)\n\n十、模型评估调优\n1、常见普通\nfrom sklearn.metrics import accuracy_score, confusion_matrix, recall_score, f1_score,  fbeta_score\nprint(accuracy_score(y_true, y_pred))\nprint(fbeta_score(y_true, y_pred, beta=0.5))\n2、分类结果统计报告\nfrom sklearn.metrics import classification_report\nprint(classification_report(y_test, y_pred))\n3、样本划分\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y_labels, test_size=0.2)\n4、交叉验证\nfrom sklearn.model_selection import cross_val_score\ncv_scores = cross_val_score(rf_model, X_train,\ny_train,scoring=make_scorer(recall_score) ,cv=5)\nprint('mean f1_score socre of raw model{}'.format(np.mean(cv_scores)))\n\n\n\n5、网格调优\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.metrics import make_scorer\nmodel_gbr = GradientBoostingRegressor()\nparameters = {'loss': ['ls','lad','huber','quantile'],'min_samples_leaf':\n[1,2,3,4,5],'alpha': [0.1,0.3,0.6,0.9]}\nmodel_gs = GridSearchCV(estimator=model_gbr, param_grid=parameters, cv=5, scoring=make_scorer(f1_score))\nmodel_gs.fit(X_train,y_train)\nprint('Best score is:', model_gs.best_score_)\nprint('Best parameter is:', model_gs.best_params_)\n6、过采样\nfrom imblearn.over_sampling import SMOTE\nsmote_model = SMOTE(random_state=7, ratio=0.1)\nX_train_res_rf,y_train_res_rf = smote_model.fit_resample(X_train,y_train)\nprint('Resampled dataset shape {}'.format(y_train_res_rf))\n#过采样比例为 0.1，即目标的正负样本比例为 1:10\n\n十一、其他小知识点\n1、文件操作：\n写文件：\n# 使用 write 方法写文件\nwith open(\"f.txt\", \"w\") as f:\nf.write( \"www.huawei.com\")\n读取文件：\n# 使用 read 方法读取\nwith open(\"f.txt\", \"r\") as f:\nprint(f.read())\n2、实验模型的导出以及导入\nimport pickle\nfrom sklearn.externals import joblib\njoblib.dump(svm, 'svm.pkl')\nsvm = joblib.load('svm.pkl')\n3、间隔取值\nlist[::2 ] #list 间隔取值\ndf = df[[i%2==0 for i in range(len(df.index))]] #df 按行取样本\ndf=df.iloc[:,[i%2==0 for i in range(len(df.columns))]]#df按列取样本\n\n\n\n\n4、取序号值\narr = np.array([3, 22, 4, 11, 2, 44, 9])\nprint(np.max(arr))  # 44\nprint(np.argmax(arr))  # 5\nprint(np.argmin(arr))  # 4\nprint(np.where(arr > 4, arr - 10, arr * 10))  # [10 20 30 40 -5 -4 -3 -2]\n\ndef modthree(x):\n    return x % 3 ==0\nprint(np.where(modthree(arr), arr - 10, arr * 10))  # [ -7 220  40 110  20 440  -1]\n\narr = np.array([3, 4, 5, 6, 7])\nprint(np.argwhere(arr % 2 != 0))\nprint(np.argwhere(arr % 2 != 0).flatten())  # [0 2 4]  可以用来取序号\n3、浮点数两位\ndf.round(2)\n5、斐波那契数列\n# 位置参数\ndef fibs(num):\nresult = [0,1]# 新建列表存储数列的值\nfor i in range(2,num):# 循环 num-2 次\na = result[i-1] + result[i-2]\n# 将值追加至列表\nresult.append(a)\n# 返回列表\nreturn result\nfibs(5)\n# 输出：[0, 1, 1, 2, 3]\n\n十二、概率论：\n1、二项分布贝努力\nfrom scipy.stats import binom\nbinom_sim = binom.rvs(n=10, p=0.3, size=10000) #\n2、泊松分布\nX= np.random.poisson(lam=2, size=10000)\n3、正态分布\nfrom scipy.stats import norm\nx = np.arange(-5, 5, 0.1)\ny = norm.pdf(x, mu, sigma)\n4、指数分布\nfrom scipy.stats import expon\nx = np.arange(0, 15, 0.1)\ny = expon.pdf(x, lam)\n"
    },
    {
        "title": "电信用户分析,聚类",
        "content": "import pandas as pd\nimport numpy as np\nimport matplotlib.pylab as plt\n%matplotlib inline\n\nX = pd.read_csv('./telecom.csv', encoding='utf-8')\nprint(X.shape)\nX.head()\n\n# 数据预处理\nfrom sklearn import preprocessing\nX_scaled = preprocessing.scale(X)  # scale操作之后的数据零均值，单位方差（方差为1）\nX_scaled[0:5]\n\n# 进行PCA数据降维\nfrom sklearn.decomposition import PCA\n \n# 生成PCA实例\npca = PCA(n_components=3)  # 把维度降至3维\n# 进行PCA降维\nX_pca = pca.fit_transform(X_scaled)\n# 生成降维后的dataframe\nX_pca_frame = pd.DataFrame(X_pca, columns=['pca_1', 'pca_2', 'pca_3'])  # 原始数据由(30000, 7)降维至(30000, 3)\nX_pca_frame.head()\n\n# 训练简单模型\nfrom sklearn.cluster import KMeans\n \n# KMeans算法实例化，将其设置为K=10\nest = KMeans(n_clusters=10)\n \n# 作用到降维后的数据上\nest.fit(X_pca)\n\n# 取出聚类后的标签\nkmeans_clustering_labels = pd.DataFrame(est.labels_, columns=['cluster'])  # 0-9,一共10个标签\n \n# 生成有聚类后的dataframe\nX_pca_frame = pd.concat([X_pca_frame, kmeans_clustering_labels], axis=1)\n \nX_pca_frame.head()\n\n# 对不同的k值进行计算，筛选出最优的K值\nfrom mpl_toolkits.mplot3d import Axes3D  # 绘制3D图形\nfrom sklearn import metrics\n \n# KMeans算法实例化，将其设置为K=range(2, 14)\nd = {}\nfig_reduced_data = plt.figure(figsize=(12, 12))  #画图之前首先设置figure对象，此函数相当于设置一块自定义大小的画布，使得后面的图形输出在这块规定了大小的画布上，其中参数figsize设置画布大小\nfor k in range(2, 14):\n    est = KMeans(n_clusters=k, random_state=111)\n    # 作用到降维后的数据上\n    y_pred = est.fit_predict(X_pca)\n    # 评估不同k值聚类算法效果\n    calinski_harabaz_score = metrics.calinski_harabasz_score(X_pca_frame, y_pred)  # X_pca_frame：表示要聚类的样本数据，一般形如（samples，features）的格式。y_pred：即聚类之后得到的label标签，形如（samples，）的格式\n    d.update({k: calinski_harabaz_score})\n    print('calinski_harabaz_score with k={0} is {1}'.format(k, calinski_harabaz_score))  # CH score的数值越大越好\n    # 生成三维图形，每个样本点的坐标分别是三个主成分的值\n    ax = plt.subplot(4, 3, k - 1, projection='3d') #将figure设置的画布大小分成几个部分，表示4(row)x3(colu),即将画布分成4x3，四行三列的12块区域，k-1表示选择图形输出的区域在第k-1块，图形输出区域参数必须在“行x列”范围\n    ax.scatter(X_pca_frame.pca_1, X_pca_frame.pca_2, X_pca_frame.pca_3, c=y_pred)  # pca_1、pca_2、pca_3为输入数据，c表示颜色序列\n    ax.set_xlabel('pca_1')\n    ax.set_ylabel('pca_2')\n    ax.set_zlabel('pca_3')\n\n# 绘制不同k值对应的score，找到最优的k值\nx = []\ny = []\nfor k, score in d.items():\n    x.append(k)\n    y.append(score)\n \nplt.plot(x, y)\nplt.xlabel('k value')\nplt.ylabel('calinski_harabaz_score')\n\nX.index = X_pca_frame.index  # 返回：RangeIndex(start=0, stop=30000, step=1)\n \n# 合并原数据和三个主成分的数据\nX_full = pd.concat([X, X_pca_frame], axis=1)\nX_full.head()\n\n# 按每个聚类分组\ngrouped = X_full.groupby('cluster')\n \nresult_data = pd.DataFrame()\n# 对分组做循环，分别对每组进行去除异常值处理\nfor name, group in grouped:\n    # 每组去除异常值前的个数\n    print('Group:{0}, Samples before:{1}'.format(name, group['pca_1'].count()))\n\n    desp = group[['pca_1', 'pca_2', 'pca_3']].describe() # 返回每组的数量、均值、标准差、最小值、最大值等数据\n    for att in ['pca_1', 'pca_2', 'pca_3']:\n        # 去异常值：箱形图\n        lower25 = desp.loc['25%', att]\n        upper75 = desp.loc['75%', att]\n        IQR = upper75 - lower25\n        min_value = lower25 - 1.5 * IQR\n        max_value = upper75 + 1.5 * IQR\n        # 使用统计中的1.5*IQR法则，删除每个聚类中的噪音和异常点\n        group = group[(group[att] > min_value) & (group[att] < max_value)]\n    result_data = pd.concat([result_data, group], axis=0)\n    # 每组去除异常值后的个数\n    print('Group:{0}, Samples after:{1}'.format(name, group['pca_1'].count()))\nprint('Remain sample:', result_data['pca_1'].count())\n\n# 设置每个簇对应的颜色\ncluster_2_color = {0: 'red', 1: 'green', 2: 'blue', 3: 'yellow', 4: 'cyan', 5: 'black', 6: 'magenta', 7: '#fff0f5',\n                   8: '#ffdab9', 9: '#ffa500'}\n \ncolors_clustered_data = X_pca_frame.cluster.map(cluster_2_color)  # 簇名和颜色映射\nfig_reduced_data = plt.figure()\nax_clustered_data = plt.subplot(111, projection='3d')\n \n# 聚类算法之后的不同簇数据的映射为不同颜色\nax_clustered_data.scatter(X_pca_frame.pca_1.values, X_pca_frame.pca_2.values, X_pca_frame.pca_3.values,\n                          c=colors_clustered_data)\nax_clustered_data.set_xlabel('Component_1')\nax_clustered_data.set_ylabel('Component_2')\nax_clustered_data.set_zlabel('Component_3')\n\n# 筛选后的数据聚类可视化\ncolors_filtered_data = result_data.cluster.map(cluster_2_color)\nfig = plt.figure(figsize=(12,12))\nax = plt.subplot(111, projection='3d')\nax.scatter(result_data.pca_1.values, result_data.pca_2.values, result_data.pca_3.values, c=colors_filtered_data)\nax.set_xlabel('Component_1')\nax.set_ylabel('Component_2')\nax.set_zlabel('Component_3')\n\n# 查看各族中的每月话费情况\nmonthly_Fare = result_data.groupby('cluster').describe().loc[:, u'每月话费']\nmonthly_Fare\n\n# mean：均值；std：标准差\nmonthly_Fare[['mean', 'std']].plot(kind='bar', rot=0, legend=True)  # rot可以控制轴标签的旋转度数。legend是否在图上显示图例\n\n# 查看各族中的入网时间情况\naccess_time = result_data.groupby('cluster').describe().loc[:, u'入网时间']\naccess_time\naccess_time[['mean', 'std']].plot(kind='bar', rot=0, legend=True, title='Access Time')\n\n# 查看各族中的欠费金额情况\narrearage = result_data.groupby('cluster').describe().loc[:, u'欠费金额']\narrearage[['mean', 'std']].plot(kind='bar', rot=0, legend=True, title='Arrearage')\n\n# 综合描述\nnew_column = ['Access_time', u'套餐价格', u'每月流量', 'Monthly_Fare', u'每月通话时长', 'Arrearage', u'欠费月份数', u'pca_1', u'pca_2',\n              u'pca_3', u'cluster']\nresult_data.columns = new_column\nresult_data.groupby('cluster')[['Monthly_Fare', 'Access_time', 'Arrearage']].mean().plot(kind='bar')  # 每个簇的Monthly_Fare、Access_time、Arrearag的均值放在一块比较"
    },
    {
        "title": "手写线性回归",
        "content": "import numpy as np\n\ndef simple_linear_regression(x, y):\n    # 计算x和y的平均值\n    x_mean = np.mean(x)\n    y_mean = np.mean(y)\n    \n    # 计算权重w\n    w = np.sum((x - x_mean) * (y - y_mean)) / np.sum((x - x_mean) ** 2)\n    \n    # 计算偏置b\n    b = y_mean - w * x_mean\n    \n    return w, b\n\ndef predict(x, w, b):\n    # 使用模型参数进行预测\n    return w * x + b\n\n# 示例数据\nx = np.array([10, 4, 6])\ny = np.array([8, 2, 5])\n\n# 训练模型并获取参数\nw, b = simple_linear_regression(x, y)\n\n# 使用模型进行预测\nx_new = np.array([12])\ny_pred = predict(x_new, w, b)\n\nprint(f\"预测结果: y = {w:.2f} * x + {b:.2f}\")\nprint(f\"对于x = {x_new[0]}, 预测的y值是: {y_pred[0]:.2f}\")\n\n\n*基于线性回归算法实现\nfrom sklearn.linear_model import LinearRegression\nmodel=LinearRegression()\nmodel.fit(data,y)\nprint('基于线性逻辑回归算法',model.coef_,model.intercept_)"
    },
    {
        "title": "方差选择模型（VarianceThreshold）",
        "content": "* 题目说阈值为1，但是hreshold=填空，有如下的备注，我就调整了hreshold，用了另外的判断方法，输出的结果为True False。要看True的值。\nfrom sklearn.feature_selection import VarianceThreshold\n\n# 定义方差选择模型，定义方差系数\nmodel_vt = VarianceThreshold(threshold=0.7) #，输出超过3个小于6个。"
    },
    {
        "title": "SMOTE过采样",
        "content": "pip install imbalanced-learn\nfrom imblearn.over_sampling import SMOTE\nfrom sklearn.datasets import make_classification\nfrom sklearn.model_selection import train_test_split\n\n# 创建一个不平衡的数据集\nX, y = make_classification(n_classes=2, class_sep=2,\n                           weights=[0.1, 0.9], n_informative=3, n_redundant=1, flip_y=0,\n                           n_features=20, n_clusters_per_class=1, n_samples=1000, random_state=10)\n\n# 划分数据集\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=0)\n\n# 实例化 SMOTE\nsm = SMOTE(random_state=42)\n\n# 应用 SMOTE\nX_res, y_res = sm.fit_resample(X_train, y_train)\n\n# 现在 X_res 和 y_res 包含了过采样后的训练数据\n"
    },
    {
        "title": "最小二乘法",
        "content": "import numpy as np  \nimport scipy as sp  \nimport pylab as pl  \nfrom scipy.optimize import leastsq  # 引入最小二乘函数  \n\nn = 9  # 多项式次数  \n定义目标函数：  \ndef real_func(x):  \n  #目标函数：sin(2*pi*x)\n    return np.sin(2 * np.pi * x)  \n定义多项式函数，用多项式去拟合数据:  \ndef fit_func(p, x):  \n    f = np.poly1d(p)  \n    return f(x)  \n定义残差函数，残差函数值为多项式拟合结果与真实值的差值：  \ndef residuals_func(p, y, x):  \n    ret = fit_func(p, x) - y  \n    return ret  \n\nx = np.linspace(0, 1, 9)  # 随机选择9个点作为x  \nx_points = np.linspace(0, 1, 1000)  # 画图时需要的连续点  \ny0 = real_func(x)  # 目标函数  \ny1 = [np.random.normal(0, 0.1) + y for y in y0]  # 在目标函数上添加符合正态分布噪声后的函数  \np_init = np.random.randn(n)  # 随机初始化多项式参数  \n# 调用scipy.optimize中的leastsq函数，通过最小化误差的平方和来寻找最佳的匹配函数\n#func 是一个残差函数，x0 是计算的初始参数值，把残差函数中除了初始化以外的参数打包到args中\nplsq = leastsq(func=residuals_func, x0=p_init, args=(y1, x))  \n\nprint('Fitting Parameters: ', plsq[0])  # 输出拟合参数  \n\npl.plot(x_points, real_func(x_points), label='real')  \npl.plot(x_points, fit_func(plsq[0], x_points), label='fitted curve')  \npl.plot(x, y1, 'bo', label='with noise')  \npl.legend()  \npl.show()"
    },
    {
        "title": "美国人口收入分析",
        "content": "import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import LabelEncoder, StandardScaler\nfrom sklearn.neighbors import KNN\nfrom sklearn.metrics import classification_report, confusion_matrix\nfrom sklearn.model_selection import GridSearchCV\n\n# 1、加载数据，查看数据行列分布情况\ndata = pd.read_csv('adult_inconn.csv')\ndata.shape\n\n# 2、查看数据特征，最大值，最小值，中位数，平均数以及四分位数\ndata.describe()\n\n# 3、查看数据缺失值分布，并用柱形图表示\nmissing_values = data.isnull().sum()\nmissing_values.plot(kind='bar')\nmissing_values[missing_values > 0].plot(kind='bar', color='skyblue')\nplt.title(\"缺失值分布\")\nplt.xlabel(\"特征\")\nplt.ylabel(\"缺失值数量\")\nplt.show()\n# 4、创建新数据集、对Predclass 标签进行转化\n# 这里假设Predclass是目标变量，我们需要将其转换为数值型\ndata['Predclass'] = data['Predclass'].map({'<=50K': 0, '>50K': 1})\n\n# 5、使用cut方法对数据进行分箱，分10个箱\n# 这里以年龄为例进行分箱\ndata['age_bins'] = pd.cut(data['age'], bins=10, right=False)\n\n# 6、绘制分箱后的数据分布图\nage_distribution = data['age_bins'].value_counts().sort_index().plot(kind='bar')\n\n# 7、属性衍生 - 这里我们以年龄和收入为例创建一个新属性\ndata['age_income'] = data['age'] * data['Predclass']\n创建新的“年龄组”或“收入水平”特征。\n# 举例，创建“年龄组”特征\ndata['age_group'] = pd.cut(data['age'], bins=[0, 30, 60, 90], labels=['青年', '中年', '老年'])\n\n# 8、查看sex-marital 分布图\nsex_marital_distribution = sns.countplot(data=data, x='sex', hue='marital_status')\n# 性别和婚姻状况分布图\nsns.countplot(data=data, x='sex', hue='marital-status')\nplt.title(\"性别与婚姻状况分布\")\nplt.show()\n# 9、数据特征值处理，属性编码\n# 对类别特征进行编码\nlabel_encoders = {}\nfor column in data.select_dtypes(include=['object']).columns:\n    le = LabelEncoder()\n    data[column] = le.fit_transform(data[column])\n    label_encoders[column] = le\n\n# 10、将df 转为str - 这一步似乎没有必要，可能是有误解\ndata = data.astype(str)\n\n# 11、引入KNN 对数据进行预测\n# 首先划分数据集\nX = data.drop('Predclass', axis=1)\ny = data['Predclass']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n# 标准化特征\nscaler = StandardScaler()\nX_train_scaled = scaler.fit_transform(X_train)\nX_test_scaled = scaler.transform(X_test)\n\n# 12、选择模型最优参数\nknn = KNN()\nparam_grid = {'n_neighbors': np.arange(1, 30)}\nknn_gscv = GridSearchCV(knn, param_grid, cv=5)\nknn_gscv.fit(X_train_scaled, y_train)\nbest_params = knn_gscv.best_params_\n\n# 13、重新生成模型knn_final\nknn_final = KNN(n_neighbors=best_params['n_neighbors'])\n\n# 14、对数据进行拟合\nknn_final.fit(X_train_scaled, y_train)\n\n# 15、重新对X_test 进行预测\ny_pred = knn_final.predict(X_test_scaled)\n\n# 16、导入分类模块，画出分布图\nconf_matrix = confusion_matrix(y_test, y_pred)\nsns.heatmap(conf_matrix, annot=True, fmt='d')\n\n# 输出相关结果\ndata_shape, data_description, missing_values, best_params, conf_matrix\n绘制预测结果分布图\nsns.countplot(x=y_pred)\nplt.title(\"预测结果分布\")\nplt.show()\n"
    },
    {
        "title": "黑色星期五",
        "content": "# 检查缺失值\nprint(data.isnull().sum())\n\n# 填充缺失值或删除缺失行（根据数据分析需求决定策略）\n# 假设我们选择填充缺失值\ndata.fillna(method='ffill', inplace=True)\n\n# 生成user_info表\nuser_info = data[['User_ID', 'Gender', 'Age', 'Occupation', 'City_Category']]\nprint(user_info.head())\n# 年龄和性别分布\nage_distribution = user_info['Age'].value_counts()\ngender_distribution = user_info['Gender'].value_counts()\n\nprint(\"年龄分布:\\n\", age_distribution)\nprint(\"性别分布:\\n\", gender_distribution)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# 按年龄和性别分组计算消费金额总和\nage_gender_purchase = data.groupby(['Age', 'Gender'])['Purchase'].sum().unstack()\n\n# 绘制条形图\nage_gender_purchase.plot(kind='bar', stacked=True, figsize=(12, 8))\nplt.title(\"消费情况按年龄和性别分布\")\nplt.xlabel(\"年龄\")\nplt.ylabel(\"总消费金额\")\nplt.legend(title=\"性别\")\nplt.show()"
    },
    {
        "title": "信用违约预测",
        "content": "1. 读取数据并查看前5行信息\ndata = pd.read_csv(\"credit-default.csv\")\nprint(data.head())\n2. 查看Target分布\nprint(data['Target'].value_counts())\ndata['Target'].value_counts().plot(kind='bar', title='Target分布')\n3. 相关系数矩阵及热力图\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n# 相关系数矩阵\ncorr_matrix = data.corr()\n\n# 绘制热力图\nplt.figure(figsize=(12, 10))\nsns.heatmap(corr_matrix, annot=True, cmap='coolwarm', fmt='.2f')\nplt.title(\"相关系数矩阵热力图\")\nplt.show()\n4. 删除相关性超过0.8的特征\n删除高相关性的特征，以减少多重共线性问题。\n# 找出相关性超过0.8的特征对并删除一个特征\nhigh_corr = corr_matrix[(corr_matrix.abs() > 0.8) & (corr_matrix.abs() < 1.0)]\ncolumns_to_drop = [column for column in high_corr.columns if any(high_corr[column])]\ndata = data.drop(columns=columns_to_drop)\n5. 统计各特征的缺失率\n# 统计缺失率\nmissing_rate = data.isnull().mean()\nprint(missing_rate)\n6. 名义型变量的缺失值处理\n针对名义型变量的缺失值，可以选择填充方式。\n# 填充缺失值 - 以mode填充名义型变量缺失值\nfor col in data.select_dtypes(include=['object']).columns:\n    data[col].fillna(data[col].mode()[0], inplace=True)\n7. 数据填充\n针对数值型变量缺失值，可以用均值或中位数填充。\n# 填充数值型变量缺失值\nfor col in data.select_dtypes(exclude=['object']).columns:\n    data[col].fillna(data[col].median(), inplace=True)\n8. 名义型变量独热编码\n对名义型变量进行独热编码，以增强表达能力。\n# 独热编码\ndata = pd.get_dummies(data, drop_first=True)\n9. 数据拆分\n将数据集拆分为训练集和测试集。\nfrom sklearn.model_selection import train_test_split\n\nX = data.drop(columns=['Target'])\ny = data['Target']\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n10. 引入随机森林算法并进行预测\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import f1_score\n\n# 初始化模型并训练\nrf_model = RandomForestClassifier(random_state=42)\nrf_model.fit(X_train, y_train)\n\n# 预测并计算f1 score\ny_pred = rf_model.predict(X_test)\nprint(\"初始模型 F1 Score:\", f1_score(y_test, y_pred))\n11. 使用SMOTE进行过采样\n使用SMOTE对数据进行过采样以平衡类别。\nfrom imblearn.over_sampling import SMOTE\n# 过采样\nsmote = SMOTE(random_state=42)\nX_train_res, y_train_res = smote.fit_resample(X_train, y_train)\n12. 重新训练随机森林模型\n# 重新训练模型\nrf_model.fit(X_train_res, y_train_res)\ny_pred_resampled = rf_model.predict(X_test)\nprint(\"SMOTE后模型 F1 Score:\", f1_score(y_test, y_pred_resampled))\n13. 模型调优\n可以使用网格搜索调优超参数。\nfrom sklearn.model_selection import GridSearchCV\n\n# 参数网格\nparam_grid = {\n    'n_estimators': [50, 100, 200],\n    'max_depth': [10, 20, 30],\n    'min_samples_split': [2, 5, 10]\n}\n\n# 网格搜索\ngrid_search = GridSearchCV(RandomForestClassifier(random_state=42), param_grid, cv=5, scoring='f1')\ngrid_search.fit(X_train_res, y_train_res)\n\n# 最优参数\nprint(\"最优参数:\", grid_search.best_params_)\n14. 输出最终模型的F1 Score\n# 使用最优参数重新训练模型\nrf_final = RandomForestClassifier(**grid_search.best_params_, random_state=42)\nrf_final.fit(X_train_res, y_train_res)\ny_pred_final = rf_final.predict(X_test)\n\n# 计算f1 score\nprint(\"最终模型 F1 Score:\", f1_score(y_test, y_pred_final))"
    },
    {
        "title": "data_cluster.csv\n 1、导入数据，统计行、列、时间日期\n 2、查看数据中的最大值，最小值，中位数，平均数以及四分位数\n 3、选择日期为20200319的数据判读空值处理并生成新数据集data_new\n 4、按用户ID浏量生成数据集data_new2\n 5、选择用户ID为5的流量信息分布绘制柱形图\n 6、选择用户ID为10的流量信息分布绘制饼状图",
        "content": "1. 导入数据，统计行、列及时间日期信息\ndata.shape\n# 假设时间日期列为'Date'，统计日期信息\ndata['Date'] = pd.to_datetime(data['Date'])\nprint(\"日期范围:\", data['Date'].min(), \"到\", data['Date'].max())\n2. 查看数据的最大值、最小值、中位数、平均数及四分位数\ndata.describe()\n3. 筛选日期为20200319的数据，处理空值并生成data_new\ndata_new = data[data['Date'] == '2020-03-19']\n\n# 空值处理，可以选择删除或填充\ndata_new = data_new.dropna()  # 或者使用data_new.fillna(0)填充\n\n# 显示空值处理后的数据集\nprint(data_new.head())\n4. 按用户ID生成data_new2数据集\n\n# 假设用户ID列为'UserID'，按UserID分组计算流量总和\ndata_new2 = data_new.groupby('UserID').sum().reset_index()\nprint(data_new2.head())\n5. 绘制用户ID为5的流量信息柱形图\nimport matplotlib.pyplot as plt\n\n# 筛选用户ID为5的数据\nuser_5_data = data_new[data_new['UserID'] == 5]\n\n# 绘制柱形图\nuser_5_data.plot(kind='bar', x='Date', y='Flow', title=\"用户ID为5的流量信息\")\nplt.xlabel(\"日期\")\nplt.ylabel(\"流量\")\nplt.show()\n6. 绘制用户ID为10的流量信息饼状图\nuser_10_data = data_new[data_new['UserID'] == 10]\n\n# 绘制饼状图\nuser_10_data.set_index('Date')['Flow'].plot(kind='pie', autopct='%1.1f%%', title=\"用户ID为10的流量分布\")\nplt.ylabel(\"\")  # 去掉y轴标签\nplt.show()"
    },
    {
        "title": "使用数据datamining01.csv\n导入数据集文件，查看除表头外的前5行\n统计每列数据的最大值，最小值，中位数，平均数以及四分位数\n计算每列缺失值所占百分比，并输出结果，输出结果包含缺失值数量，缺失值占比\n将缺失值超过70%列删除，并选取适当方法对缺失值进行填充",
        "content": "2. 统计每列数据的最大值、最小值、中位数、平均数及四分位数\n# 使用 describe 方法进行统计\nstats = data.describe().T  # 转置方便查看\nstats['median'] = data.median()\nprint(stats[['min', 'max', 'median', 'mean', '25%', '50%', '75%']])\n3. 计算每列缺失值的数量和占比\n# 计算每列缺失值数量\nmissing_counts = data.isnull().sum()\n# 计算每列缺失值占比\nmissing_percentage = (missing_counts / len(data)) * 100\n# 输出缺失值数量和占比\nmissing_info = pd.DataFrame({'缺失值数量': missing_counts, '缺失值占比': missing_percentage})\nprint(missing_info)\n4. 删除缺失值超过70%的列并对剩余缺失值进行填\n# 删除缺失值超过70%的列\ndata_cleaned = data.loc[:, missing_percentage <= 70]\n\n# 对剩余列的缺失值进行填充，使用适当的填充方法\n# 数值型列填充中位数，分类变量填充众数\nfor col in data_cleaned.columns:\n    if data_cleaned[col].dtype == 'object':  # 分类变量\n        data_cleaned[col].fillna(data_cleaned[col].mode()[0], inplace=True)\n    else:  # 数值型变量\n        data_cleaned[col].fillna(data_cleaned[col].median(), inplace=True)\n\n# 查看填充后的数据\nprint(data_cleaned.head())"
    },
    {
        "title": "使用数据train_data.csv,datamining01.csv\n导入数据查看除表头外的前5行head()\n统计每列数据中的最大值，最小值，中位数，平均数以及四分位数describe()\n计算每列中的缺失值占比，并输出结果，输出结果包含缺失值数量，缺失值占比isnull().\n删除无意义数据（unique=1列），删除无特征变化数据（值为0的列）\n将缺失值超过80%的列进行删除，\n绘制AGE 直方图",
        "content": "1. 导入数据并查看前5行\ndata = pd.read_csv(\"train_data.csv\")\ndata.head()\n2. 统计每列数据的最大值、最小值、中位数、平均数及四分位数\n# 统计每列的描述性统计信息\ndata.describe()\n3. 计算每列缺失值数量和占比\n# 计算每列的缺失值数量和缺失值占比\nmissing_counts = data.isnull().sum()\nmissing_percentage = (missing_counts / len(data)) * 100\n# 创建包含缺失值数量和占比的数据框\nmissing_info = pd.DataFrame({'缺失值数量': missing_counts, '缺失值占比': missing_percentage})\nprint(missing_info)\n4. 删除无意义数据列（唯一值为1的列）和无特征变化数据（全为0的列）\n# 删除唯一值为1的列\nunique_cols = data.columns[data.nunique() == 1]\ndata = data.drop(columns=unique_cols)\n\n# 删除所有值为0的列\nzero_cols = data.columns[(data == 0).all()]\ndata = data.drop(columns=zero_cols)\n\n# 查看删除后的数据\nprint(\"删除无意义列和无变化列后的数据:\", data.shape)\n5. 删除缺失值超过80%的列\n# 筛选出缺失值占比小于等于80%的列\ndata = data.loc[:, missing_percentage <= 80]\n\nprint(\"删除缺失值超过80%的列后的数据:\", data.shape)\n6. 绘制AGE的直方图\nimport matplotlib.pyplot as plt\nplt.hist(data['AGE'].dropna(), bins=20, color='skyblue', edgecolor='black')\nplt.title(\"AGE 直方图\")\nplt.xlabel(\"AGE\")\nplt.ylabel(\"频数\")\nplt.show()"
    },
    {
        "title": "电影数据集，推荐",
        "content": NaN
    },
    {
        "title": "数据集diabetes.csv",
        "content": "1. 导入数据并查看前5行\ndata = pd.read_csv(\"diabetes.csv\")\n\n2. 查看OutCome列的数据分布\noutcome_distribution = data['OutCome'].value_counts()\n3. 使用train_test_split对数据进行XY拆分\nfrom sklearn.model_selection import train_test_split\n# 特征和目标变量拆分\nX = data.drop('OutCome', axis=1)\ny = data['OutCome']\n\n# 使用80-20比例拆分数据集\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n4. 使用XGBoost分类算法XGBClassifier进行预测并优化参数\nfrom xgboost import XGBClassifier\nfrom sklearn.metrics import roc_auc_score\n# 定义初始模型\nmodel = XGBClassifier(max_depth=3, learning_rate=0.1, random_state=42)\n\n# 训练模型\nmodel.fit(X_train, y_train)\n\n# 预测\ny_pred = model.predict_proba(X_test)[:, 1]\n\n# 计算AUC得分\nauc_score = roc_auc_score(y_test, y_pred)\nprint(\"初始模型的AUC得分:\", auc_score)\n\n# 参数调优，提高AUC得分\nbest_auc = auc_score\nbest_params = {'max_depth': 3, 'learning_rate': 0.1}\n\n# 尝试不同的参数组合\nfor depth in range(3, 8):\n    for lr in [0.01, 0.05, 0.1, 0.2]:\n        model = XGBClassifier(max_depth=depth, learning_rate=lr, random_state=42)\n        model.fit(X_train, y_train)\n        y_pred = model.predict_proba(X_test)[:, 1]\n        auc = roc_auc_score(y_test, y_pred)\n        \n        if auc > best_auc:\n            best_auc = auc\n            best_params = {'max_depth': depth, 'learning_rate': lr}\n\nprint(\"最佳AUC得分:\", best_auc)\nprint(\"最佳参数:\", best_params)"
    },
    {
        "title": "bigdata",
        "content": "data = [50,46,33,27,55,36,73,101] \nfor i in range(len(data)): #建立for循环，循环整个data列表\n    for j in range(0,len(data)-i-1):\n        if data[j]>data[j+1]: #if语句设定条件\n            data[j],data[j+1] = data[j+1],data[j] #结果，比较两个数，比较后按照大小互换位置\nprint(data) #输出排序后的列表\n\nimport numpy as np\nimport scipy as sp\n#1、生成一个0-14向量\nx = np.arange(15)\nprint(x)\n#2、将x转换为二维矩阵，矩阵的第一维度为1\ny = x.reshape(1,15)\nprint(y)\n#3、生成3*4矩阵\nA = np.arange(12).reshape(3,4)\nprint(A)\n#4、转置3中的矩阵\nprint(A.T)\n#5、给定线性方程组求解\n#10x_1 + 8x_2 + 12x_3 = 20\n#4x_1 + 4x_2 + 2x_3 = 8\n#2x_1 - 4x_2- 2x_3 = -5\nfrom scipy.linalg import solve\na = np.array([[10, 8, 12], [4, 4, 2], [2, -4, -2]])\nb = np.array([10,8,-5])\nx = solve(a, b)\nprint(x)\n\n#计算每列缺失值所占百分比，并输出结果，输出结果包含缺失值数量，缺失值占比\ncolumns = data.columns\nfor i in columns:\n    null_count = data[i].isnull().sum()\n    qszb = null_count/10000*100\n    print(i,\"缺失值数量为:{}\".format(null_count),\"-->缺失值占比为:{}%\".format(round(qszb,2)))\n\n#将缺失值超过70%列删除，并选取适当方法对缺失值进行填充\ndata_drop = data.dropna(axis=1,thresh=len(data)*0.3)\ndata_drop\n\n#各列均值填充各列缺失值\nfor i in data_drop.columns:\n    data_drop[i] = data_drop[i].fillna(np.mean(data_drop[i]))\n\n#再次查看数据集确实情况\ncolumns = data_drop.columns\nfor i in columns:\n    null_count = data_drop[i].isnull().sum()\n    qszb = null_count/10000*100\n    print(i,\"缺失值数量为:{}\".format(null_count),\"-->缺失值占比为:{}%\".format(round(qszb,2)))\n    #缺失全部填充完毕\n\nprint(df.isnull().any()) #查看数据是否含有缺失值，false表示不含缺失值\n\nprint(df.info()) #查看数据类型，浮点型数据共22列，整型数据共2列\n\nimport matplotlib.pyplot as plt #导入画图库\nimport seaborn as sns #导入seaborn画图库\nimport warnings \nwarnings.filterwarnings('ignore') #忽略告警提示\nplt.rcParams['font.family'] = ['sans-serif'] \nplt.rcParams['font.sans-serif'] = ['SimHei']#画图时防止中文不显示\n\nimport matplotlib\nmatplotlib.rcParams['axes.unicode_minus']=False #设置负号显示\nplt.figure(figsize=(20,10))\ndf_corr = df.corr(method='pearson') #特征相关性矩阵\nsns.heatmap(df_corr,annot=True) #特征热力图\nplt.show()\n#颜色越浅表示正相关越强，颜色越深表示负相关越强\n\ndrop = [] #设置一个列表，存储筛选出的相关性高的特征对\nfor i in df_corr.index: #遍历相关系数矩阵所有的行\n    for j in df_corr.columns:#遍历相关系数矩阵所有的列\n        if df_corr.loc[i,j]>0.8 and i!=j and (j,i) not in drop: #设定if语句条件，筛选出相关性大于0.8的特征对\n            drop.append((i,j)) #将筛选出的特征对加入drop列表中\nprint(drop) #输出存储的特征对\n\n#丢弃特征对中的一个，进行特征选择\ncols_to_drop = np.unique([col[1] for col in drop]) #对特征对进行去重，筛选出需要剔除的特征，包括Var4,10,16,17,18,19,20,21,22,23列\ndf.drop(cols_to_drop,axis=1,inplace=True)\nprint(df.head())\n\nfrom sklearn.tree import DecisionTreeClassifier #决策树\nfrom sklearn.neighbors import KNeighborsClassifier #KNN\nfrom sklearn.preprocessing import StandardScaler #标准化函数\nfrom sklearn.model_selection import cross_val_score #交叉验证得分函数\nfrom sklearn.metrics import confusion_matrix #混淆矩阵\nfrom sklearn import metrics\nfrom sklearn.metrics import roc_auc_score,f1_score,accuracy_score,precision_score,recall_score #评价指标\n\n\n#三个评估函数的集合\ndef eva(x,y):\n    print('f1_score:',round(f1_score(y,x),2),'查准率:',round(precision_score(y,x),2),'查全率',round(recall_score(y,x),2))\n    return\n\n#绘制ROC曲线\ndef plot_roc_curve(y_test,preds):\n    fpr,tpr,threshold = metrics.roc_curve(y_test,preds)\n    roc_auc = metrics.auc(fpr,tpr)\n    plt.title('ROC曲线')\n    plt.plot(fpr,tpr,'b',label = 'AUC = %0.2f'%roc_auc)\n    plt.legend(loc='lower right')\n    plt.plot([0,1],[0,1],'r--')\n    plt.xlim([-0.01,1.01])\n    plt.ylim([-0.01,1.01])\n    plt.ylabel('真正例率',fontsize=16)\n    plt.xlabel('假正例率',fontsize=16)\nplt.show()\n\n\n#划分训练集和测试集\nfrom sklearn.model_selection import train_test_split\nX = df.loc[:,df.columns!='lab'] #属性特征列\nY = df['lab'] #目标列\nx_train,x_test,y_train,y_test = train_test_split(X,Y,test_size=0.3,random_state=12) #训练集/测试集=7：3，随机种子数为12\n\n\nx_train_std = StandardScaler().fit_transform(x_train) #标准化数据\nx_test_std = StandardScaler().fit_transform(x_test)\n\ndtc = DecisionTreeClassifier(criterion='gini',max_depth=8)  #建立决策树模型，特征划分准则采用gini，最大树深度为8\ndtc.fit(x_train,y_train) #模型训练\nprint(dtc.score(x_test,y_test)) #模型得分\n\nplt.rcParams['font.family'] = ['sans-serif'] \nplt.rcParams['font.sans-serif'] = ['SimHei']#画图时防止中文不显示\ntest=[] #设定一个列表，存储模型得分\nplt.figure(figsize=(15,5)) #设置画布大小\nfor i in range(13): #for循环遍历0,12\n    dtc = DecisionTreeClassifier(max_depth=i+1,criterion='gini',random_state=30) #循环不同的max_depth来建立决策树模型\n    dtc.fit(x_train,y_train) #模型训练\n    score = dtc.score(x_test,y_test) #模型得分\n    test.append(score) #存储不同max_depth建立的决策树的得分\nplt.plot(range(1,14),test,color='blue',label='max_depth') #折线图可视化\nplt.xlabel('数值设置')\nplt.ylabel('模型准确率')\nplt.legend()\nplt.title('最大树深度max_depth影响图')\nplt.show()\n#可以看出max_depth为13时模型准确率最高，下一步基于max_depth为13时建立新的决策树模型\n\nnew_dtc = DecisionTreeClassifier(max_depth=13,criterion='gini',random_state=30) #建立新的决策树模型，max_depth为13\nnew_dtc.fit(x_train,y_train) #模型训练\nprint(new_dtc.score(x_test,y_test)) #模型得分\n\ny_pre_dtc = new_dtc.predict(x_test) #决策树模型预测值\n\ny_pro_dtc = new_dtc.predict_proba(x_test)[:,1] #模型预测概率\n\nprint(confusion_matrix(y_test,y_pre_dtc))#决策树模型混淆矩阵\n\nprint(eva(y_pre_dtc,y_test)) #决策树模型评估\n\n#决策树模型ROC曲线及AUC值\nplt.figure(figsize=(20,8))\nplt.xticks(fontsize=18)\nplt.yticks(fontsize=18)\nplot_roc_curve(y_test,y_pro_dtc)\nplt.show()\n\n#运用KNN算法建模\nK = np.arange(1,14) #K值选取范围为1到13\nacc=[] #存储不同K值建立的KNN模型准确率的列表\nfor i in K:\n    #交叉验证得分，使用accuracy准确率判定，cv=10是10折交叉验证\n    cv_score = cross_val_score(KNeighborsClassifier(n_neighbors=i,weights='uniform'),x_train_std,y_train,scoring='accuracy',cv=10)\n    acc.append(cv_score.mean()) #将n_neighbors为1到13所有情况下得分的均值存储在列表acc中\nplt.figure(figsize=(15,5)) #设置画布大小\nplt.plot(K,acc) #绘制K值-acc折线图\narg_max = np.array(acc).argmax() #取平均准确率中最大值的对应的下标\nplt.text(K[arg_max], acc[arg_max], '最佳k值为{}'.format(K[arg_max])) #设置文字说明，找到最佳K值\nplt.xlabel('k值')\nplt.ylabel('模型准确率')\nplt.title('不同K值下模型的平均准确率')\nplt.show()\n\n#建立最佳K值下的KNN模型\nknn = KNeighborsClassifier(n_neighbors=3,weights='uniform') #建立KNN模型\nknn.fit(x_train_std,y_train) #模型训练\nprint(knn.score(x_test_std,y_test)) #模型准确率得分\n\ny_pre_knn = knn.predict(x_test_std)#模型预测\ny_pro_knn = knn.predict_proba(x_test_std)[:,1] #KNN模型预测概率\n\nprint(confusion_matrix(y_test,y_pre_knn))#KNN模型混淆矩阵\nprint(eva(y_pre_knn,y_test)) #KNN模型评估\n#KNN模型ROC曲线及AUC值\nplt.figure(figsize=(20,8))\nplt.xticks(fontsize=18)\nplt.yticks(fontsize=18)\nplot_roc_curve(y_test,y_pro_knn)\nplt.show()\n\n#运用SVM建模\nfrom sklearn.svm import SVC\nsvm = SVC(C=1.0) #建立SVM模型，惩罚项C为1，其余参数默认\nsvm.fit(x_train_std,y_train) #模型训练\nprint(svm.score(x_test_std,y_test)) #模型准确率得分\nfrom sklearn.model_selection import GridSearchCV #导入网格搜索算法\nparam_svm = {'C':[0.05,0.1,0.3,0.5,1,2]} #搜索参数C\ngsearch_svm = GridSearchCV(estimator=svm,param_grid=param_svm,cv=5) #构建网格搜索，5折交叉验证\ngsearch_svm.fit(x_train_std,y_train) #模型训练\nprint(gsearch_svm.best_params_ )#搜索得到的模型最佳参数\nnew_svm = SVC(C=2.0,probability=True) #建立搜索得到的最优参数C的SVM模型\nnew_svm.fit(x_train_std,y_train) #模型训练\nprint(new_svm.score(x_test_std,y_test)) #模型得分\n\ny_pre_svm = new_svm.predict(x_test_std)#SVM模型预测\ny_pro_svm = new_svm.predict_proba(x_test_std)[:,1]#SVM模型预测概率\nprint(confusion_matrix(y_test,y_pre_svm)) #SVM模型的混淆矩阵\nprint(eva(y_pre_svm,y_test)) #SVM模型评估\n#SVM模型ROC曲线及AUC值\nplt.figure(figsize=(20,8))\nplt.xticks(fontsize=18)\nplt.yticks(fontsize=18)\nplot_roc_curve(y_test,y_pro_svm)\nplt.show()\n\n#保存模型到本地\nimport joblib\n#保存KNN模型\njoblib.dump(knn,'./knn.pkl')\n\nmodel_knn = joblib.load('./knn.pkl') #加载上述保存的KNN模型\nknn_yuce = model_knn.predict(x_test_std) #使用KNN模型对标准化后的测试集x_test_std进行预测\nprint(knn_yuce) #输出预测数据\n\ndata = [] #设定一个存储预测数据的列表data\nfor i in list(knn_yuce): #遍历模型预测出的所有数据\n    data.append(i) #向列表中加入数据\n\nf = open('C:\\\\yuce.txt','w') #在桌面上写入一个保存预测数据的txt文件，名称为yuce.txt\nfor i in data: #遍历data中的数据\n    f.write(str(i)+'\\n') #向txt文本中写入所有预测的数据，稍后打开桌面中的yuce.txt文档即可看到预测出的所有数据\n\ndf.describe(include='O') #统计离散型特征信息\n\ndf.describe() #查看连续型特征信息\n\n#各特征缺失值可视化\nimport missingno as mnso\nmnso.bar(df)\n\ndf.isnull().any() #不存在缺失值\nplt.rcParams['font.family'] = ['sans-serif'] \nplt.rcParams['font.sans-serif'] = ['SimHei']#画图时防止中文不显示\nplt.subplot(2,2,1)\ndf['Churn'].value_counts().plot.pie(autopct='%.1f%%',figsize =(15,15),fontsize=20)\nplt.subplot(2,2,2)\ndf['SeniorCitizen'].value_counts().plot.pie(autopct='%.1f%%',figsize =(15,15),fontsize=20)\n#可以看出流失用户占整体用户的26.5%，大约16%的人群是老年人，84%的人群是年轻人\n\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nplt.figure(figsize=(20,8),dpi=80)\nsns.countplot(df['Churn'],hue='gender',data=df)\nplt.xticks(fontsize=18)\nplt.yticks(fontsize=18)#可视化看出客户流失中男女分布均匀，性别属性对结果并无明显决策作用\n\n#查看年龄与客户流失之间的关系，发现年轻人群体保留率占比很高,在流失用户中，老年用户占比明显比年轻用户更高\nplt.figure(figsize=(20,8),dpi=80)\nsns.countplot(df['Churn'],hue='SeniorCitizen',data=df)\nplt.xticks(fontsize=18)\nplt.yticks(fontsize=18)\n\nplt.figure(figsize=(15,10))\nplt.subplot(2,2,1)\nsns.kdeplot(df.TotalCharges[(df[\"Churn\"] == 'No')],color=\"Red\", shade =True)\nsns.kdeplot(df.TotalCharges[(df[\"Churn\"] == 'Yes')],color=\"Blue\", shade= True)\nplt.legend([\"Not Churn\",\"Churn\"],loc='upper right')\nplt.xlabel('总费用')\nplt.ylabel('密度')\nplt.title('客户人群是否流失与总费用分布') #可以发现总费用在500左右时用户流失率最高\nplt.subplot(2,2,2)\nsns.kdeplot(df.MonthlyCharges[(df[\"Churn\"] == 'No')],color=\"Red\", shade = True)\nsns.kdeplot(df.MonthlyCharges[(df[\"Churn\"] == 'Yes')],color=\"Blue\", shade = True)\nplt.legend([\"Not Churn\",\"Churn\"],loc='upper right')\nplt.xlabel('月费用')\nplt.ylabel('密度')\nplt.title('客户人群是否流失与月费用分布')#可以发现月费用较高时，客户流失率也会增高\n\n#可视化PhoneService(是否有电话服务)与客户是否流失之间的关系，看出拥有电话服务的人群占比高，但是该特征对用户是否流失并无明显决策作用\nplt.figure(figsize=(20,8),dpi=80)\nsns.countplot(df['Churn'],hue='PhoneService',data=df)\nplt.xticks(fontsize=18)\nplt.yticks(fontsize=18)\n\n#命名属性集合为list_all\nlist_all = ['InternetService','OnlineSecurity', 'OnlineBackup', 'DeviceProtection', 'TechSupport','StreamingTV']\nplt.figure(figsize=(15,15))\nfor i,m in enumerate(list_all):\n    plt.subplot(3,3,(i+1))\n    sns.countplot(x=m,hue='Churn',data=df)\n    plt.xlabel(str(m))\n    plt.title('Churn by '+str(m))\n    i+=1\nplt.show() #可以看出No internet service(无互联网服务)用户的流失率相同，说明这几个因素对于不使用互联网服务的用户是否流失不具影响\n#而在网络服务中，不使用网络服务的用户流失占比较低，对于使用DSL和Fiber optic的用户来说，使用Fiber optic的用户更容易流失\n\n#删除customerID,使用LabelEncoder编码，将数据转换为连续型数值变量\nfrom sklearn.preprocessing import LabelEncoder\ndel df['customerID']\nle = LabelEncoder()\nnew_data = df.astype(str)\nnew_data_le = new_data.apply(le.fit_transform)\n\ndf_corr = new_data_le.corr(method=\"spearman\")\nplt.figure(figsize=(20,10))\nplt.xticks(fontsize=16)\nplt.yticks(fontsize=16)\nsns.heatmap(df_corr,annot=True)\n\n#数据集划分为训练集和测试集，拆分比例为0.3\nfrom sklearn.model_selection import train_test_split\nX = new_data_le.loc[:,new_data_le.columns!='Churn']\nY = new_data_le['Churn']\nx_train,x_test,y_train,y_test = train_test_split(X,Y,test_size=0.3)\n\nfrom sklearn import metrics \nfrom sklearn.preprocessing import StandardScaler #导入标准化函数\nfrom sklearn.metrics import roc_auc_score,f1_score,accuracy_score,precision_score,recall_score #评价指标\nx_train_std = StandardScaler().fit_transform(x_train) #标准化训练集\nx_test_std = StandardScaler().fit_transform(x_test)\n\n\n#三个评估函数的集合\ndef eva(x,y):\n    print('f1_score:',f1_score(y,x),'查准率:',precision_score(y,x),'查全率',recall_score(y,x))\n    return\n\n#绘制ROC曲线\ndef plot_roc_curve(y_test,preds):\n    fpr,tpr,threshold = metrics.roc_curve(y_test,preds)\n    roc_auc = metrics.auc(fpr,tpr)\n    plt.title('Receiver Operating Characteristic')\n    plt.plot(fpr,tpr,'b',label = 'AUC = %0.2f'%roc_auc)\n    plt.legend(loc='lower right')\n    plt.plot([0,1],[0,1],'r--')\n    plt.xlim([-0.01,1.01])\n    plt.ylim([-0.01,1.01])\n    plt.ylabel('True Positive Rate',fontsize=16)\n    plt.xlabel('False Positive Rate',fontsize=16)\nplt.show()\n\n#随机森林算法建模\nfrom sklearn.ensemble import RandomForestClassifier\nrf = RandomForestClassifier()\nrf.fit(x_train_std,y_train)\nrf.score(x_test_std,y_test)\n\n#预测值y_pre_rf与预测概率y_pro_rf\ny_pre_rf = rf.predict(x_test_std)\ny_pro_rf = rf.predict_proba(x_test_std)[:,1]\neva(y_pre_rf,y_test)#模型评估\n#ROC曲线图及AUC值\nplt.figure(figsize=(20,8))\nplt.xticks(fontsize=18)\nplt.yticks(fontsize=18)\nplot_roc_curve(y_test,y_pro_rf)\n\n#最后查看各个特征重要性，可视化并按照重要性数值大小排序\n#特征重要性图\nplt.rcParams['font.family'] = ['sans-serif'] \nplt.rcParams['font.sans-serif'] = ['SimHei']#画图时防止中文不显示\nimportance = pd.DataFrame() #建立一个DataFrame\nimportance['变量'] = x_train.columns\n#查看属性重要性\nimportance['重要性程度'] = rf.feature_importances_ #得到随机森林对特征的评价重要度\n#根据数值大小排序\nimportance = importance.sort_values(by = '重要性程度',ascending=False)\nplt.figure(figsize=(16,10))\nplt.yticks(fontsize=14)\nplt.xticks(fontsize=16,rotation=90)\nplt.ylabel('重要性程度',fontsize=14)\nplt.bar(importance['变量'].values.tolist()[:15],importance['重要性程度'].values.tolist()[:15])\n\ndf['class'].value_counts() #查看蘑菇类别分布\ndf.isnull().any() #查看数据是否有缺失值\n#查看蘑菇类别的占比(毒蘑菇：p，正常蘑菇：e)\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nplt.figure(figsize=(9,9))\nplt.rcParams['font.family'] = ['sans-serif'] \nplt.rcParams['font.sans-serif'] = ['SimHei']#画图时防止中文不显示\ndf['class'].value_counts().plot.pie(explode=[0,0.05],autopct='%.2f%%',textprops={'fontsize':20})\nplt.title('蘑菇类别占比',fontsize=20)\n\n#菌盖颜色进行直方图可视化，查看不同颜色菌盖的数量\nplt.rcParams['font.family'] = ['sans-serif'] \nplt.rcParams['font.sans-serif'] = ['SimHei']#画图时防止中文不显示\nplt.figure(figsize=(20,10))\n#设置条形图颜色\ndf['cap_color'].value_counts().plot.barh()\nplt.xlabel('数量',fontsize=20)\nplt.ylabel('菌盖颜色种类',fontsize=20)\n#可以看出在蘑菇届，棕、灰、红、黄、白的蘑菇占大多数\n\nplt.figure(figsize=(20,10))\nplt.rcParams['font.family'] = ['sans-serif'] \nplt.rcParams['font.sans-serif'] = ['SimHei']#画图时防止中文不显示\nsns.countplot(df['cap_color'],hue='class',data=df)\nplt.xlabel('菌盖颜色种类',fontsize=20)\nplt.ylabel('数量',fontsize=20)\n#(毒蘑菇：p，正常蘑菇：e)\n#可视化可以看出鲜艳的蘑菇有毒的可能性还是较高的，比如红色和棕色\n#但其他颜色的蘑菇并非就是完全可食用，棕色和灰色的蘑菇都是很常见的\n\n"
    }
]
    results = []
    for entry in json_data:
        if key.lower() in entry['title'].lower():
            results.append({'title': entry['title'], 'content': entry['content']})
    df = pd.DataFrame(results)
    df.style.set_properties(**{'white-space': 'pre-wrap', 'text-align': 'left'})
def daima_by_c(key):
    json_data=[
    {
        "title": "关联规则",
        "content": "# 读取数据并展示前五行\ndf = pd.read_csv('./data/online_retail_II.csv')\ndf.head()\n1.清理df数据：删除缺失值与重复值。\ndf.dropna(inplace=True)\ndf.drop_duplicates(inplace=True)\nfrom mlxtend.frequent_patterns import apriori, fpmax, fpgrowth\nfrom mlxtend.frequent_patterns import association_rules\n\n# 找到所有支持度超过0.03的项集\nbasket = df.groupby(['Invoice', 'StockCode'])['Quantity'].sum().unstack().fillna(0)\nbasket_sets = basket.applymap(lambda x: 1 if x > 0 else 0)\n使用 fpgrowth算法从数据basket_sets中计算频繁项集，并将最小支持度设置为 0.1。返回频繁项集frequent_itemsets\n\nfrequent_itemsets = fpgrowth(basket_sets, min_support=0.1, use_colnames=True)\nfrequent_itemsets\n使用association_rules从频繁项集frequent_itemsets中构建关联规则，metric为'confidence'， min_threshold为0.4。并将结果存储在rules中\n\nrules = association_rules(frequent_itemsets, metric='confidence', min_threshold=0.4)\n# 展示前五条关联规则\nrules.head()\n4.打印出同时满足置信度< 0.8，置信度>0.5，提升度>3.0的rules数据。\n/*此处由考生进行代码填写*/\nfiltered_rules = rules[(rules['confidence'] < 0.8) & (rules['confidence'] > 0.5) & (rules['lift'] > 3.0)]\n\n# 打印结果\nprint(filtered_rules)\n筛选规则rules，将规则中同时满足antecedent_sele为{'22111', '22271'}，consequent_sele为{'22272'}的规则选出来，并存储在final_sele中\n\n# 筛选 antecedents 和 consequents 满足条件的规则\nfinal_sele = (rules['antecedents'] == {'22111', '22271'}) & (rules['consequents'] == {'22272'})\n# 使用 final_sele 筛选出相应的规则\nrules.loc[final_sele ]"
    },
    {
        "title": "线性代数",
        "content": "生成一个包含整数0-11的向量\nx = np.arange(12)\n查看数组大小\nx.shape\n将x转换成二维矩阵，其中矩阵的第一个维度为1\nx = x.reshape(1,12)\n将x转换3x4的矩阵\nx = x.reshape(3,4)\n生成3*4的矩阵\nA = np.arange(12).reshape(3,4)\n转置\nA.T\n矩阵相乘：两个矩阵能够相乘的条件为第一个矩阵的列数等于第二个矩阵的行数。\nnp.matmul(A,B)\n矩阵对应运算：针对形状相同矩阵的运算统称，包括元素对应相乘、相加等，即对两个矩阵相同位置的元素进行加减乘除等运算。\n矩阵相乘：A*A\n矩阵相加：A + A\n逆矩阵实现：只有方阵才有逆矩阵\nA = np.arange(4).reshape(2,2)\nnp.linalg.inv(A)\n矩阵的特征值与特征向量\nA = [[1, 2],[2, 1]] #生成一个2*2的矩阵\nfrom scipy.linalg import eig\nevals, evecs = eig(A) #求A的特征值（evals）和特征向量(evecs)\n行列式\nnp.linalg.det(A)\n解线性方程组\nfrom scipy.linalg import solve\nx = solve(a, b)\n奇异值分解\n"
    },
    {
        "title": "概率论",
        "content": "ll = [[1,2,3,4,5,6],[3,4,5,6,7,8]]\nnp.mean(ll)  #全部元素求均值\nnp.mean(ll,0) #按列求均值，0代表列向量，1表示行向量\n求方差：\nnp.var(b)\nnp.var(ll,1)) #第二个参数为1，表示按行求方差\n标准差\nnp.std(ll)\n相关系数\nnp.corrcoef(a,b)\n二项分布\nfrom scipy.stats import binom, norm, beta, expon\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n#n,p对应二项式公式中的事件成功次数及其概率，size表示采样次数\nbinom_sim = binom.rvs(n=10, p=0.3, size=10000)\nprint('Data:',binom_sim)\nprint('Mean: %g' % np.mean(binom_sim))\nprint('SD: %g' % np.std(binom_sim, ddof=1))\n#生成直方图，x指定每个bin(箱子)分布的数据,对应x轴，binx是总共有几条条状图，normed值密度,也就是每个条状图的占比例比,默认为1\nplt.hist(binom_sim, bins=10, normed=True)\nplt.xlabel(('x'))\nplt.ylabel('density')\nplt.show()\n泊松分布\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n#产生10000个符合lambda=2的泊松分布的数\nX= np.random.poisson(lam=2, size=10000)  \n\na = plt.hist(X, bins=15, normed=True, range=[0, 15])\n#生成网格\nplt.grid()\nplt.show()\n正态分布\nimport matplotlib.pyplot as plt\nmu = 0\nsigma = 1\n#分布采样点\nx = np.arange(-5, 5, 0.1)\n#生成符合mu,sigma的正态分布\ny = norm.pdf(x, mu, sigma)\nplt.plot(x, y)\nplt.xlabel('x')\nplt.ylabel('density')\nplt.show()\n指数分布\nfrom scipy.stats import expon\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nlam = 0.5\n#分布采样点\nx = np.arange(0, 15, 0.1)\n#生成符合lambda为0.5的指数分布\ny = expon.pdf(x, lam)\nplt.plot(x, y)\nplt.title('Exponential: lam=%.2f' % lam)\nplt.xlabel('x')\nplt.ylabel('density')\nplt.show()\n\n中心极限定理\nimport numpy as np\nimport matplotlib.pyplot as plt\n#随机产生10000个范围为(1,6)的数\nramdon_data = np.random.randint(1,7,10000)\nprint(ramdon_data.mean())\nprint(ramdon_data.std())\n生成直方图\nplt.figure()\nplt.hist(ramdon_data,bins=6,facecolor='blue')\nplt.xlabel('x')\nplt.ylabel('n')\nplt.show()\n随机抽取1000组数据，每组50个\nsamples = []\nsamples_mean =[]\nsamples_std = []\n\n#从生成的1000个数中随机抽取1000组\nfor i in range(0,1000):\nsample = []\n#每组随机抽取50个数\n    for j in range(0,50):\n        sample.append(ramdon_data[int(np.random.random() * len(ramdon_data))])\n  #将这50个数组成一个array放入samples列表中\n    sample_ar = np.array(sample)\nsamples.append(sample_ar)\n#保存每50个数的均值和标准差\n    samples_mean.append(sample_ar.mean())\nsamples_std.append(sample_ar.std())\n#samples_std_ar = np.array(samples_std)\n#samples_mean_ar = np.array(samples_mean)\n# print(samples_mean_ar)\n梯度下降法\n训练集(x,y)共5个样本,每个样本点有3个分量 (x0,x1,x2)  \nx = [(1, 0., 3), (1, 1., 3), (1, 2., 3), (1, 3., 2), (1, 4., 4)]  \ny = [95.364, 97.217205, 75.195834, 60.105519, 49.342380]  y[i] 样本点对应的输出  \nepsilon = 0.0001  #迭代阀值，当两次迭代损失函数之差小于该阀值时停止迭代  \nalpha = 0.01  #学习率\ndiff = [0, 0]  \nmax_itor = 1000  \nerror1 = 0  \nerror0 = 0  \ncnt = 0  \nm = len(x)  \n#初始化参数  \ntheta0 = 0  \ntheta1 = 0  \ntheta2 = 0  \nwhile True:  \n    cnt += 1  \n\n    # 参数迭代计算  \n    for i in range(m):  \n        # 拟合函数为 \ny = theta0 * x[0] + theta1 * x[1] +theta2 * x[2]  \n        # 计算残差，即拟合函数值-真实值  \n        diff[0] = (theta0** x[i][0] + theta1 * x[i][1] + theta2 * x[i][2]) - y[i]  \n  \n        # 梯度 = diff[0] * x[i][j]。根据步长*梯度更新参数 \n        theta0 -= alpha * diff[0] * x[i][0]  \n        theta1 -= alpha * diff[0] * x[i][1]  \n        theta2 -= alpha * diff[0] * x[i][2]  \n  \n    # 计算损失函数  \n    error1 = 0  \n    for lp in range(len(x)):  \n        error1 += (y[lp]-(theta0 + theta1 * x[lp][1] + theta2 * x[lp][2]))**2/2  \n    #若当两次迭代损失函数之差小于该阀值时停止迭代，跳出循环；\n    if abs(error1-error0) < epsilon:  \n        break  \n    else:  \n        error0 = error1  \n  \n    print(' theta0 : %f, theta1 : %f, theta2 : %f, error1 : %f' % (theta0, theta1, theta2, error1) )  \n\nprint('Done: theta0 : %f, theta1 : %f, theta2 : %f' % (theta0, theta1, theta2)  )\nprint('迭代次数: %d' % cnt  )\n"
    },
    {
        "title": "斐波那契数列",
        "content": "def fibonacci(n):\nif n==1:\nreturn [0]\nif n==2:\nreturn [0,1]\n    fib = [0, 1]\n    for i in range(2, n):\n        next_value = fib[i-1] + fib[i-2]\n        fib.append(next_value)\n    return fib[:n]\n\n# 生成前10个斐波那契数列的值\nfibonacci_10 = fibonacci(10)"
    },
    {
        "title": "字典",
        "content": "# 创建字典并存储元素\ndict_data = {'name':'lee', 'age':22, 'gender':'male'}\n\n查看字典items\ndict_data.items()\n插入一个键值对\nd[\"mark\"]=99\n# 分别将字典中的键和值全部输出\nkeys = list(dict_data.keys())\nvalues = list(dict_data.values())\n\n# 获取字典中键'name'对应的值\nname_value = dict_data['name']\n# 1. 新建字典 Dict1\nDict1 = {'name': 'lee', 'age': 89, 'num': [1, 2, 8]}\n\n# 2. 浅拷贝 Dict1，副本命名为 Dict_copy\nDict_copy = Dict1.copy()\n\n# 3. 创建集合，命名为 sample_set，包含2个元素 \"Prince\", \"Techs\"\nsample_set = {\"Prince\", \"Techs\"}\n\n# 4. 检查集合 sample_set 中是否存在某一元素 \"Data\" 并打印结论\nexists_data = \"Data\" in sample_set\ncheck_conclusion = \"存在\" if exists_data else \"不存在\"\n\n# 5. 向集合 sample_set 中增加元素 \"Data\"\nsample_set.add(\"Data\")\n\n# 6. 将元素 'Techs' 从集合 sample_set 中移除\nsample_set.remove(\"Techs\")\n\n# 7. 将集合 sample_set 分别转换为元组和列表结构，并打印输出\nsample_set_tuple = tuple(sample_set)\nsample_set_list = list(sample_set)"
    },
    {
        "title": "列表，冒泡排序",
        "content": "# 创建列表并存储元素 [310, 7]\nlst = [310, 7]\n\n# 打印输出列表\nprint(lst)\n#在末尾添加一个元素3\nlst.append(3)\n# 在列表元素下标为2的位置插入元素5\nlst.insert(2, 5)\n#列表从小到大排序\nlst.sort()\n从大到小\nlst.sort(reverse=True)\n# 冒泡排序，从大到小\nfor i in range(len(lst)):\n    for j in range(0, len(lst)-i-1):\n        if lst[j] < lst[j+1]:\n            lst[j], lst[j+1] = lst[j+1], lst[j]\n\n# 打印排序后的列表\nprint(lst)\n\n# 在列表末尾位置添加元素 'fish'\nlst.append('fish')\n将最后一个元素替换为100\nlst[-1]=100\n\n\n# 将列表中的字符串元素 'fish' 转换为全部大写并替换原本的 'fish'\nlst[lst.index('fish')] = 'FISH'\n字符串a=\"Victory\"\na.upper()全改为大写\na.lower()全改为小写\na.title()字符串改为首字母大写\n"
    },
    {
        "title": "集合",
        "content": "#创建集合\ns=set([1,2])\ns={1,2}\ns.add(3)\n# 删除元素（如果元素不存在，会引发KeyError）\ns.remove(2)\n清空集合\nset1.clear()\nset1.update([6, 7, 8])  # 添加元素，可以是列表、元组、字典等\n# 2. 检查集合中是否在元素'numpy'\ncontains_numpy = 'numpy' in sample\n# 3. 删除集合中的元素'pandas'\nsample.discard('pandas')\n# 4. 将集合分别转化为列表和元组并输出\nlist_from_set = list(sample)\ntuple_from_set = tuple(sample)\n\n# 5. 使用 copy()对集合进行浅拷贝\nsample_copy = sample.copy()\n"
    },
    {
        "title": "总-代码总结",
        "content": "一、算法\n1、线性回归\nfrom sklearn.linear_model import LinearRegression\nmodel.coef_ w 系数 model.intercept_ 截距\n2、逻辑回归\nfrom sklearn.linear_model import LogisticRegression\n3、KNN\nfrom sklearn.neighbors import KneiborsClassifier\nKneiborsClassifier(n_neighbors =4, algorithm = “ball_tree”)\n4、朴素贝叶斯\nfrom sklearn.naive_bayes import BernoulliNB\n5、SVM\nfrom sklearn.svm import svc\nsvc(c = 1.0, kernel = “rbf”)\nc：惩罚系数，默认是 0，C 越小，泛化能力越强。\nKernel：核函数 rbf 是径向基（高斯）此外还有线性 linear、多项式、poly、sigmoid\n6、决策树\nfrom sklearn.tree import DecisionTreeClassifier\n7、集成算法\nfrom sklearn.ensemble import RandomForestClassifier,  BaggingClassifier，\nAdaBoostClassifier，GradientBosstingClassifier\n\n通用 参数 n_estimators = 10 ,基类学习器个数。\nGradientBosstingClassifier(max_depth = 5)\nmax_depth = 5 树最大深度\nfrom xgboost.sklearn import XGBClassifier\n8、Kmeans\nfrom sklearn.cluster import Kmeans\nKmeans(n_clusters = k, init=’k_means++’,max_iter = 300)\nmodel.labels_ 类标签\nmodel.cluster_centers_ 簇中心\n\nestimator = KMeans(n_clusters=3)\nestimator.fit(X)\nlabel_pred = estimator.labels_\nx0 = X[label_pred == 0]\nx1 = X[label_pred == 1]\nx2 = X[label_pred == 2]\n\n\n\n\n9、AgglomerativeClustering\nfrom sklearn.cluster import AgglomerativeClustering\nAgglomerativeClustering(n_clusters=2, affinity=“euclidean”, compute_full_tree,linkage = “ward”）\ncompute_full_tree = False 时，训练 n_cluesters 后，训练停止；True 则训练整颗树。\nlinkage 的参数为\n{“ward”, “complete”,“average”, “single”}\n\n10、Birch\nfrom sklearn.cluster import Birch\nBirch(threshold=0.5, branching_factor=50, n_clusters=3)\nthreshold：float，表示设定的半径阈值，默认 0.5。\nbranching_factor：int，默认=50，每个节点最大特征树子集群数。\nn_clusters：int，默认=3，最终聚类数目。\n11、DBSCAN\nfrom sklearn.cluster import DBSCAN\nDBSCAN(eps=0.5, min_samples=5, metric=’euclidean)\neps：两个样本被看作邻居节点的最大距离。\nmin_samples：最小簇的样本数。\nmetric：距离计算方式，euclidean 为欧氏距离计算。\n12、ariori\nfrom mlxtend.frequent_patterns import apriori\nApriori(df, min_support=0.5, use_colnames=False, max_len=None,\nn_jobs=1)\n其中 df 代表数据框数据集，min_support 表示指定的最小支持度，\nuse_colnames=True 表示使用元素名字，默认的 False 使用列名代表元素，max_len 表\n示生成的项目集的最大长度。如果为 None，则评估所有可能的项集长度。\n二、画图\nplt.xlabel('pca1')\nplt.ylabel('pca2')\nplt.title(\"PCA\")\nplt.legend(loc='lower left')\n1、条形图\n数据为：\ngrouped = data.groupby(['是否高质用户', '网络类型'])['用户标识符'].count().unstack()\n\n网络类型 2G 3G 4G\n是否高质用户   \n0 1630.0 1668.0 1699.0\n1 NaN 2530.0 2473.0\ndf.plot.bar(rot = 0) //直接 df 画图 rot 是下标志是否字体是立着的还是卧着的。\ngrouped.plot(kind='bar', alpha=1.0, rot=0)\n2、散点图\ndata.plot.scatter(x = '类目 2 消费金额', y = 6, c = 'br')\nx = 为列名 列名可以用字符串 或如 y 的序列序数\nc = 'br'是颜色， 如蓝色和红色。\nplt.scatter(data.loc[:, '类目 2 消费金额'], data.loc[:, '类目 3 消费金额'], c=y,alpha=0.5)\n# 生成数据可视化\ny = data.loc[:, '是否高质用户']\nplt.scatter(data.loc[:, '类目 2 消费金额'], data.loc[:, '类目 3 消费金额'], c=y,alpha=0.5)\n这种图的颜色点由 y 决定。\n3、饼图\ngrouped.plot.pie(autopct = '%0.01f', subplots = True)\nautopct 为是否在饼图画百分比；Subplots = True 为是否为每个列画单独子图。\n4、3D 画图\nfrom mpl_toolkits.mplot3d import Axes3D\nfig = plt.figure()\n方法一\nax = fig.gca(projection='3d')\n方法二\nax = Axes3D(fig)\nax.scatter(X[y==i ,0], X[y==i, 1], X[y==i,2], c=c,marker=m,\nlabel=l)\n#做三维曲面图，rstride 和 cstride 分别代表行和列的跨度，cmap:曲面颜色\nax.plot_surface(X, Y, Z1, rstride=1, cstride=1, cmap='rainbow')\n5、热力图\nfig = plt.figure(figsize = (25.10))\nplt.subplot(1,2,1)\n# 关系热力图\nmask_corr = np.zeros_like(df.corr())\nmast_corr[np.triu_indices_from(mask_corr)] = True\nsns.heatmap(df.corr(), mask = mask_corr)\n6、直方图\n#x 指定每个 bin(箱子)分布的数据,对应 x 轴，binx 是总共有几条条状图，\nnormed 值密度,也就是每个条状图的占比例比,默认为 1\nplt.hist(binom_sim, bins=10, normed=True)\n\ncond = data['是否高质用户'] == 1\ndata[cond]['类目1消费金额'].hist(alpha=0.5, label='高质用户')\ndata[~cond]['类目1消费金额'].hist(color='r', alpha=0.5, label='非高质用户')\nplt.legend()\n三、缺失值处理\n1、检测\nmissing_sum = df.isnull().sum().sorted_values(ascending=False) //检测缺失数量\nmissing_rate = missing_sum/df.shape[0].sort_values(ascending=False)\nmissing_stat = pd.concat([missing_sum,missing_rate], keys=['missing_sum','missing_rate', axis = ‘columns’])\n2、删除\ndf.drop(columns = cols_to_drop,inplace=True)\ndel df[cols_to_drop]\n3、填充\n3.1 非监督填充方法一\nfrom sklearn.preprocessing import Imputer\ndata = Imputer(missing_values='NaN', strategy='most_frequent',\naxis=0) //strategy =mean/median/most_frequent\ndataMode = data.fit_transform(df)\n方法二、\nvalues = {'A': 0, 'B': 1, 'C': 2, 'D': 3}\ndf.fillna(value=values)\n3.2 算法填充\n参见具体算法\n\n四、异常值处理\n1、散点图检测\n具体见绘图\n2、利用决策树等算法预测\n具体见算法\n3、 3σ原则\nmin_mask = df[\"weight\"] < (df[\"weight\"].mean() - 3 *  df[\"weight\"].std())\nmax_mask = df[\"weight\"] > (df[\"weight\"].mean() + 3 * df[\"weight\"].std())\n# 只要满足上诉表达式的任一个就为异常值，所以这里使用位与运算\nmask = min_mask | max_mask\nprint(df.loc[mask,\"weight\"])\n\n4、IQR\niqr = Ser.quantile(0.75)-Ser.quantile(0.25)\nLow = Ser.quantile(0.25)-1.5*iqr\nUp = Ser.quantile(0.75)+1.5*iqr\nindex = (Ser< Low) | (Ser>Up)\nreturn index\n\n五、特征缩放\n1、标准化\n方法一、\nfrom sklearn.preprocessing import scale\nX =  scale(X)\n方法二、\nfrom sklearn.preprocessing import StandardScaler\nX = StandardScaler().fit_transform(X)\n2、最小值-最大值归一化\nfrom sklearn.preprocessing import MinMaxScaler\nX = MinMaxScaler(feature_range=(0, 1), copy=True).fit_transform(X)\n3、均值归一化\nmean=np.mean(x)\nmin=np.min(x)\nmax=np.max(x)\nMeanNormalization=(x-mean)/(max-min)\n4、缩放成单位向量\nlinalg = np.linalg.norm(x, ord=1)\nX=x/linalg\n\n六、数值离散化\n1、聚类算法\n详细见算法\n2、等宽划分\nx=pd.cut(X,5)\n3、等频划分\nx=pd.qcut(X,5)\n\n七、特征编码\n1、独热编码\nfrom sklearn.preprocessing import OneHotEncoder\nenc = OneHotEncoder(sparse = False)\nx = enc.fit_transform(iris.data)\n2、哑编码\npd.get_dummies(X) //转完后全部记得变为 int\nX = X.astype(np.int)\nX.info()\n\n\n\n\n3、LabelEncoder\n方法一、\nle = preprocessing.LabelEncoder()\nle.fit([\"paris\", \"paris\", \"tokyo\", \"amsterdam\"])\nlist(le.classes_) #['amsterdam', 'paris', 'tokyo']\nle.transform([\"tokyo\", \"tokyo\", \"paris\"]) # array([2, 2, 1]...)\nlist(le.inverse_transform([2, 2, 1]))#['tokyo', 'tokyo', 'paris']\n方法二、\ndf['Sex'] = df['Sex'].map({'female': 1, 'male': 0})\n\n八、特征选择\n1、过滤法\n方法一、方差\nfrom sklearn.feature_selection import VarianceThreshold\nX_var=VarianceThreshold(threshold=0.5).fit_transform(X, y) \n #使用阈值0.5 进行选择，特征方差小于 0.5 的特征会被删除\n方法二、卡方\nfrom sklearn.feature_selection chi2,SelectKBest\nSelectKBest(chi2, k=2).fit_transform(iris.data, iris.target)\n方法三、互信息素\nfrom sklearn.feature_selection import mutual_info_classif\nX_mut = mutual_info_classif(X, y)\n\n2、包装法 RFE\nfrom sklearn.feature_selection import RFE\nfrom sklearn.linear_model import LogisticRegression\nx_rfe=RFE(estimator=LogisticRegression(), n_features_to_select=3).fit(X, y)\nprint(x_rfe.n_features_ ) # 所选特征的数量\nprint(x_rfe.support_ ) # 按特征对应位置展示所选特征，True 表示保留，False 表示剔除。\nprint(x_rfe.ranking_ ) # 特征排名，使得 ranking_[i]对应于第 i 个特征的排名位置。\nprint(x_rfe.estimator_ ) # 递归方法选择的基模型\n\n3、嵌入法\n方法一、逻辑回归\nprint(lr.coef_)\n方法二、L1 Lasso\nfrom sklearn.linear_model import Lasso\nls = Lasso()\nls.fit(X, Y)\nls.coef_\n方法三、随机森林\nrf = RandomForestRegressor(n_estimators=15, max_depth=6)\nboston_rf=rf.fit(X, y)\nboston_rf.feature_importances_\n\n九、降维\n1、PCA\nfrom sklearn.decomposition import PCA\nX_std = preprocessing.scale(X)\npca = PCA(n_components=2)\nX_pca =pca.fit(X_std).transform(X_std)\nprint(pca.explained_variance_ratio_)# 观测降维后特征信息量大小。\n2、LDA\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis\nas LDA\nlda = LDA(n_components=2)\nX_lda =lda.fit(X,y).transform(X)\nprint(lda.explained_variance_ratio_)\n\n十、模型评估调优\n1、常见普通\nfrom sklearn.metrics import accuracy_score, confusion_matrix, recall_score, f1_score,  fbeta_score\nprint(accuracy_score(y_true, y_pred))\nprint(fbeta_score(y_true, y_pred, beta=0.5))\n2、分类结果统计报告\nfrom sklearn.metrics import classification_report\nprint(classification_report(y_test, y_pred))\n3、样本划分\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y_labels, test_size=0.2)\n4、交叉验证\nfrom sklearn.model_selection import cross_val_score\ncv_scores = cross_val_score(rf_model, X_train,\ny_train,scoring=make_scorer(recall_score) ,cv=5)\nprint('mean f1_score socre of raw model{}'.format(np.mean(cv_scores)))\n\n\n\n5、网格调优\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.metrics import make_scorer\nmodel_gbr = GradientBoostingRegressor()\nparameters = {'loss': ['ls','lad','huber','quantile'],'min_samples_leaf':\n[1,2,3,4,5],'alpha': [0.1,0.3,0.6,0.9]}\nmodel_gs = GridSearchCV(estimator=model_gbr, param_grid=parameters, cv=5, scoring=make_scorer(f1_score))\nmodel_gs.fit(X_train,y_train)\nprint('Best score is:', model_gs.best_score_)\nprint('Best parameter is:', model_gs.best_params_)\n6、过采样\nfrom imblearn.over_sampling import SMOTE\nsmote_model = SMOTE(random_state=7, ratio=0.1)\nX_train_res_rf,y_train_res_rf = smote_model.fit_resample(X_train,y_train)\nprint('Resampled dataset shape {}'.format(y_train_res_rf))\n#过采样比例为 0.1，即目标的正负样本比例为 1:10\n\n十一、其他小知识点\n1、文件操作：\n写文件：\n# 使用 write 方法写文件\nwith open(\"f.txt\", \"w\") as f:\nf.write( \"www.huawei.com\")\n读取文件：\n# 使用 read 方法读取\nwith open(\"f.txt\", \"r\") as f:\nprint(f.read())\n2、实验模型的导出以及导入\nimport pickle\nfrom sklearn.externals import joblib\njoblib.dump(svm, 'svm.pkl')\nsvm = joblib.load('svm.pkl')\n3、间隔取值\nlist[::2 ] #list 间隔取值\ndf = df[[i%2==0 for i in range(len(df.index))]] #df 按行取样本\ndf=df.iloc[:,[i%2==0 for i in range(len(df.columns))]]#df按列取样本\n\n\n\n\n4、取序号值\narr = np.array([3, 22, 4, 11, 2, 44, 9])\nprint(np.max(arr))  # 44\nprint(np.argmax(arr))  # 5\nprint(np.argmin(arr))  # 4\nprint(np.where(arr > 4, arr - 10, arr * 10))  # [10 20 30 40 -5 -4 -3 -2]\n\ndef modthree(x):\n    return x % 3 ==0\nprint(np.where(modthree(arr), arr - 10, arr * 10))  # [ -7 220  40 110  20 440  -1]\n\narr = np.array([3, 4, 5, 6, 7])\nprint(np.argwhere(arr % 2 != 0))\nprint(np.argwhere(arr % 2 != 0).flatten())  # [0 2 4]  可以用来取序号\n3、浮点数两位\ndf.round(2)\n5、斐波那契数列\n# 位置参数\ndef fibs(num):\nresult = [0,1]# 新建列表存储数列的值\nfor i in range(2,num):# 循环 num-2 次\na = result[i-1] + result[i-2]\n# 将值追加至列表\nresult.append(a)\n# 返回列表\nreturn result\nfibs(5)\n# 输出：[0, 1, 1, 2, 3]\n\n十二、概率论：\n1、二项分布贝努力\nfrom scipy.stats import binom\nbinom_sim = binom.rvs(n=10, p=0.3, size=10000) #\n2、泊松分布\nX= np.random.poisson(lam=2, size=10000)\n3、正态分布\nfrom scipy.stats import norm\nx = np.arange(-5, 5, 0.1)\ny = norm.pdf(x, mu, sigma)\n4、指数分布\nfrom scipy.stats import expon\nx = np.arange(0, 15, 0.1)\ny = expon.pdf(x, lam)\n"
    },
    {
        "title": "电信用户分析,聚类",
        "content": "import pandas as pd\nimport numpy as np\nimport matplotlib.pylab as plt\n%matplotlib inline\n\nX = pd.read_csv('./telecom.csv', encoding='utf-8')\nprint(X.shape)\nX.head()\n\n# 数据预处理\nfrom sklearn import preprocessing\nX_scaled = preprocessing.scale(X)  # scale操作之后的数据零均值，单位方差（方差为1）\nX_scaled[0:5]\n\n# 进行PCA数据降维\nfrom sklearn.decomposition import PCA\n \n# 生成PCA实例\npca = PCA(n_components=3)  # 把维度降至3维\n# 进行PCA降维\nX_pca = pca.fit_transform(X_scaled)\n# 生成降维后的dataframe\nX_pca_frame = pd.DataFrame(X_pca, columns=['pca_1', 'pca_2', 'pca_3'])  # 原始数据由(30000, 7)降维至(30000, 3)\nX_pca_frame.head()\n\n# 训练简单模型\nfrom sklearn.cluster import KMeans\n \n# KMeans算法实例化，将其设置为K=10\nest = KMeans(n_clusters=10)\n \n# 作用到降维后的数据上\nest.fit(X_pca)\n\n# 取出聚类后的标签\nkmeans_clustering_labels = pd.DataFrame(est.labels_, columns=['cluster'])  # 0-9,一共10个标签\n \n# 生成有聚类后的dataframe\nX_pca_frame = pd.concat([X_pca_frame, kmeans_clustering_labels], axis=1)\n \nX_pca_frame.head()\n\n# 对不同的k值进行计算，筛选出最优的K值\nfrom mpl_toolkits.mplot3d import Axes3D  # 绘制3D图形\nfrom sklearn import metrics\n \n# KMeans算法实例化，将其设置为K=range(2, 14)\nd = {}\nfig_reduced_data = plt.figure(figsize=(12, 12))  #画图之前首先设置figure对象，此函数相当于设置一块自定义大小的画布，使得后面的图形输出在这块规定了大小的画布上，其中参数figsize设置画布大小\nfor k in range(2, 14):\n    est = KMeans(n_clusters=k, random_state=111)\n    # 作用到降维后的数据上\n    y_pred = est.fit_predict(X_pca)\n    # 评估不同k值聚类算法效果\n    calinski_harabaz_score = metrics.calinski_harabasz_score(X_pca_frame, y_pred)  # X_pca_frame：表示要聚类的样本数据，一般形如（samples，features）的格式。y_pred：即聚类之后得到的label标签，形如（samples，）的格式\n    d.update({k: calinski_harabaz_score})\n    print('calinski_harabaz_score with k={0} is {1}'.format(k, calinski_harabaz_score))  # CH score的数值越大越好\n    # 生成三维图形，每个样本点的坐标分别是三个主成分的值\n    ax = plt.subplot(4, 3, k - 1, projection='3d') #将figure设置的画布大小分成几个部分，表示4(row)x3(colu),即将画布分成4x3，四行三列的12块区域，k-1表示选择图形输出的区域在第k-1块，图形输出区域参数必须在“行x列”范围\n    ax.scatter(X_pca_frame.pca_1, X_pca_frame.pca_2, X_pca_frame.pca_3, c=y_pred)  # pca_1、pca_2、pca_3为输入数据，c表示颜色序列\n    ax.set_xlabel('pca_1')\n    ax.set_ylabel('pca_2')\n    ax.set_zlabel('pca_3')\n\n# 绘制不同k值对应的score，找到最优的k值\nx = []\ny = []\nfor k, score in d.items():\n    x.append(k)\n    y.append(score)\n \nplt.plot(x, y)\nplt.xlabel('k value')\nplt.ylabel('calinski_harabaz_score')\n\nX.index = X_pca_frame.index  # 返回：RangeIndex(start=0, stop=30000, step=1)\n \n# 合并原数据和三个主成分的数据\nX_full = pd.concat([X, X_pca_frame], axis=1)\nX_full.head()\n\n# 按每个聚类分组\ngrouped = X_full.groupby('cluster')\n \nresult_data = pd.DataFrame()\n# 对分组做循环，分别对每组进行去除异常值处理\nfor name, group in grouped:\n    # 每组去除异常值前的个数\n    print('Group:{0}, Samples before:{1}'.format(name, group['pca_1'].count()))\n\n    desp = group[['pca_1', 'pca_2', 'pca_3']].describe() # 返回每组的数量、均值、标准差、最小值、最大值等数据\n    for att in ['pca_1', 'pca_2', 'pca_3']:\n        # 去异常值：箱形图\n        lower25 = desp.loc['25%', att]\n        upper75 = desp.loc['75%', att]\n        IQR = upper75 - lower25\n        min_value = lower25 - 1.5 * IQR\n        max_value = upper75 + 1.5 * IQR\n        # 使用统计中的1.5*IQR法则，删除每个聚类中的噪音和异常点\n        group = group[(group[att] > min_value) & (group[att] < max_value)]\n    result_data = pd.concat([result_data, group], axis=0)\n    # 每组去除异常值后的个数\n    print('Group:{0}, Samples after:{1}'.format(name, group['pca_1'].count()))\nprint('Remain sample:', result_data['pca_1'].count())\n\n# 设置每个簇对应的颜色\ncluster_2_color = {0: 'red', 1: 'green', 2: 'blue', 3: 'yellow', 4: 'cyan', 5: 'black', 6: 'magenta', 7: '#fff0f5',\n                   8: '#ffdab9', 9: '#ffa500'}\n \ncolors_clustered_data = X_pca_frame.cluster.map(cluster_2_color)  # 簇名和颜色映射\nfig_reduced_data = plt.figure()\nax_clustered_data = plt.subplot(111, projection='3d')\n \n# 聚类算法之后的不同簇数据的映射为不同颜色\nax_clustered_data.scatter(X_pca_frame.pca_1.values, X_pca_frame.pca_2.values, X_pca_frame.pca_3.values,\n                          c=colors_clustered_data)\nax_clustered_data.set_xlabel('Component_1')\nax_clustered_data.set_ylabel('Component_2')\nax_clustered_data.set_zlabel('Component_3')\n\n# 筛选后的数据聚类可视化\ncolors_filtered_data = result_data.cluster.map(cluster_2_color)\nfig = plt.figure(figsize=(12,12))\nax = plt.subplot(111, projection='3d')\nax.scatter(result_data.pca_1.values, result_data.pca_2.values, result_data.pca_3.values, c=colors_filtered_data)\nax.set_xlabel('Component_1')\nax.set_ylabel('Component_2')\nax.set_zlabel('Component_3')\n\n# 查看各族中的每月话费情况\nmonthly_Fare = result_data.groupby('cluster').describe().loc[:, u'每月话费']\nmonthly_Fare\n\n# mean：均值；std：标准差\nmonthly_Fare[['mean', 'std']].plot(kind='bar', rot=0, legend=True)  # rot可以控制轴标签的旋转度数。legend是否在图上显示图例\n\n# 查看各族中的入网时间情况\naccess_time = result_data.groupby('cluster').describe().loc[:, u'入网时间']\naccess_time\naccess_time[['mean', 'std']].plot(kind='bar', rot=0, legend=True, title='Access Time')\n\n# 查看各族中的欠费金额情况\narrearage = result_data.groupby('cluster').describe().loc[:, u'欠费金额']\narrearage[['mean', 'std']].plot(kind='bar', rot=0, legend=True, title='Arrearage')\n\n# 综合描述\nnew_column = ['Access_time', u'套餐价格', u'每月流量', 'Monthly_Fare', u'每月通话时长', 'Arrearage', u'欠费月份数', u'pca_1', u'pca_2',\n              u'pca_3', u'cluster']\nresult_data.columns = new_column\nresult_data.groupby('cluster')[['Monthly_Fare', 'Access_time', 'Arrearage']].mean().plot(kind='bar')  # 每个簇的Monthly_Fare、Access_time、Arrearag的均值放在一块比较"
    },
    {
        "title": "手写线性回归",
        "content": "import numpy as np\n\ndef simple_linear_regression(x, y):\n    # 计算x和y的平均值\n    x_mean = np.mean(x)\n    y_mean = np.mean(y)\n    \n    # 计算权重w\n    w = np.sum((x - x_mean) * (y - y_mean)) / np.sum((x - x_mean) ** 2)\n    \n    # 计算偏置b\n    b = y_mean - w * x_mean\n    \n    return w, b\n\ndef predict(x, w, b):\n    # 使用模型参数进行预测\n    return w * x + b\n\n# 示例数据\nx = np.array([10, 4, 6])\ny = np.array([8, 2, 5])\n\n# 训练模型并获取参数\nw, b = simple_linear_regression(x, y)\n\n# 使用模型进行预测\nx_new = np.array([12])\ny_pred = predict(x_new, w, b)\n\nprint(f\"预测结果: y = {w:.2f} * x + {b:.2f}\")\nprint(f\"对于x = {x_new[0]}, 预测的y值是: {y_pred[0]:.2f}\")\n\n\n*基于线性回归算法实现\nfrom sklearn.linear_model import LinearRegression\nmodel=LinearRegression()\nmodel.fit(data,y)\nprint('基于线性逻辑回归算法',model.coef_,model.intercept_)"
    },
    {
        "title": "方差选择模型（VarianceThreshold）",
        "content": "* 题目说阈值为1，但是hreshold=填空，有如下的备注，我就调整了hreshold，用了另外的判断方法，输出的结果为True False。要看True的值。\nfrom sklearn.feature_selection import VarianceThreshold\n\n# 定义方差选择模型，定义方差系数\nmodel_vt = VarianceThreshold(threshold=0.7) #，输出超过3个小于6个。"
    },
    {
        "title": "SMOTE过采样",
        "content": "pip install imbalanced-learn\nfrom imblearn.over_sampling import SMOTE\nfrom sklearn.datasets import make_classification\nfrom sklearn.model_selection import train_test_split\n\n# 创建一个不平衡的数据集\nX, y = make_classification(n_classes=2, class_sep=2,\n                           weights=[0.1, 0.9], n_informative=3, n_redundant=1, flip_y=0,\n                           n_features=20, n_clusters_per_class=1, n_samples=1000, random_state=10)\n\n# 划分数据集\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=0)\n\n# 实例化 SMOTE\nsm = SMOTE(random_state=42)\n\n# 应用 SMOTE\nX_res, y_res = sm.fit_resample(X_train, y_train)\n\n# 现在 X_res 和 y_res 包含了过采样后的训练数据\n"
    },
    {
        "title": "最小二乘法",
        "content": "import numpy as np  \nimport scipy as sp  \nimport pylab as pl  \nfrom scipy.optimize import leastsq  # 引入最小二乘函数  \n\nn = 9  # 多项式次数  \n定义目标函数：  \ndef real_func(x):  \n  #目标函数：sin(2*pi*x)\n    return np.sin(2 * np.pi * x)  \n定义多项式函数，用多项式去拟合数据:  \ndef fit_func(p, x):  \n    f = np.poly1d(p)  \n    return f(x)  \n定义残差函数，残差函数值为多项式拟合结果与真实值的差值：  \ndef residuals_func(p, y, x):  \n    ret = fit_func(p, x) - y  \n    return ret  \n\nx = np.linspace(0, 1, 9)  # 随机选择9个点作为x  \nx_points = np.linspace(0, 1, 1000)  # 画图时需要的连续点  \ny0 = real_func(x)  # 目标函数  \ny1 = [np.random.normal(0, 0.1) + y for y in y0]  # 在目标函数上添加符合正态分布噪声后的函数  \np_init = np.random.randn(n)  # 随机初始化多项式参数  \n# 调用scipy.optimize中的leastsq函数，通过最小化误差的平方和来寻找最佳的匹配函数\n#func 是一个残差函数，x0 是计算的初始参数值，把残差函数中除了初始化以外的参数打包到args中\nplsq = leastsq(func=residuals_func, x0=p_init, args=(y1, x))  \n\nprint('Fitting Parameters: ', plsq[0])  # 输出拟合参数  \n\npl.plot(x_points, real_func(x_points), label='real')  \npl.plot(x_points, fit_func(plsq[0], x_points), label='fitted curve')  \npl.plot(x, y1, 'bo', label='with noise')  \npl.legend()  \npl.show()"
    },
    {
        "title": "美国人口收入分析",
        "content": "import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import LabelEncoder, StandardScaler\nfrom sklearn.neighbors import KNN\nfrom sklearn.metrics import classification_report, confusion_matrix\nfrom sklearn.model_selection import GridSearchCV\n\n# 1、加载数据，查看数据行列分布情况\ndata = pd.read_csv('adult_inconn.csv')\ndata.shape\n\n# 2、查看数据特征，最大值，最小值，中位数，平均数以及四分位数\ndata.describe()\n\n# 3、查看数据缺失值分布，并用柱形图表示\nmissing_values = data.isnull().sum()\nmissing_values.plot(kind='bar')\nmissing_values[missing_values > 0].plot(kind='bar', color='skyblue')\nplt.title(\"缺失值分布\")\nplt.xlabel(\"特征\")\nplt.ylabel(\"缺失值数量\")\nplt.show()\n# 4、创建新数据集、对Predclass 标签进行转化\n# 这里假设Predclass是目标变量，我们需要将其转换为数值型\ndata['Predclass'] = data['Predclass'].map({'<=50K': 0, '>50K': 1})\n\n# 5、使用cut方法对数据进行分箱，分10个箱\n# 这里以年龄为例进行分箱\ndata['age_bins'] = pd.cut(data['age'], bins=10, right=False)\n\n# 6、绘制分箱后的数据分布图\nage_distribution = data['age_bins'].value_counts().sort_index().plot(kind='bar')\n\n# 7、属性衍生 - 这里我们以年龄和收入为例创建一个新属性\ndata['age_income'] = data['age'] * data['Predclass']\n创建新的“年龄组”或“收入水平”特征。\n# 举例，创建“年龄组”特征\ndata['age_group'] = pd.cut(data['age'], bins=[0, 30, 60, 90], labels=['青年', '中年', '老年'])\n\n# 8、查看sex-marital 分布图\nsex_marital_distribution = sns.countplot(data=data, x='sex', hue='marital_status')\n# 性别和婚姻状况分布图\nsns.countplot(data=data, x='sex', hue='marital-status')\nplt.title(\"性别与婚姻状况分布\")\nplt.show()\n# 9、数据特征值处理，属性编码\n# 对类别特征进行编码\nlabel_encoders = {}\nfor column in data.select_dtypes(include=['object']).columns:\n    le = LabelEncoder()\n    data[column] = le.fit_transform(data[column])\n    label_encoders[column] = le\n\n# 10、将df 转为str - 这一步似乎没有必要，可能是有误解\ndata = data.astype(str)\n\n# 11、引入KNN 对数据进行预测\n# 首先划分数据集\nX = data.drop('Predclass', axis=1)\ny = data['Predclass']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n# 标准化特征\nscaler = StandardScaler()\nX_train_scaled = scaler.fit_transform(X_train)\nX_test_scaled = scaler.transform(X_test)\n\n# 12、选择模型最优参数\nknn = KNN()\nparam_grid = {'n_neighbors': np.arange(1, 30)}\nknn_gscv = GridSearchCV(knn, param_grid, cv=5)\nknn_gscv.fit(X_train_scaled, y_train)\nbest_params = knn_gscv.best_params_\n\n# 13、重新生成模型knn_final\nknn_final = KNN(n_neighbors=best_params['n_neighbors'])\n\n# 14、对数据进行拟合\nknn_final.fit(X_train_scaled, y_train)\n\n# 15、重新对X_test 进行预测\ny_pred = knn_final.predict(X_test_scaled)\n\n# 16、导入分类模块，画出分布图\nconf_matrix = confusion_matrix(y_test, y_pred)\nsns.heatmap(conf_matrix, annot=True, fmt='d')\n\n# 输出相关结果\ndata_shape, data_description, missing_values, best_params, conf_matrix\n绘制预测结果分布图\nsns.countplot(x=y_pred)\nplt.title(\"预测结果分布\")\nplt.show()\n"
    },
    {
        "title": "黑色星期五",
        "content": "# 检查缺失值\nprint(data.isnull().sum())\n\n# 填充缺失值或删除缺失行（根据数据分析需求决定策略）\n# 假设我们选择填充缺失值\ndata.fillna(method='ffill', inplace=True)\n\n# 生成user_info表\nuser_info = data[['User_ID', 'Gender', 'Age', 'Occupation', 'City_Category']]\nprint(user_info.head())\n# 年龄和性别分布\nage_distribution = user_info['Age'].value_counts()\ngender_distribution = user_info['Gender'].value_counts()\n\nprint(\"年龄分布:\\n\", age_distribution)\nprint(\"性别分布:\\n\", gender_distribution)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# 按年龄和性别分组计算消费金额总和\nage_gender_purchase = data.groupby(['Age', 'Gender'])['Purchase'].sum().unstack()\n\n# 绘制条形图\nage_gender_purchase.plot(kind='bar', stacked=True, figsize=(12, 8))\nplt.title(\"消费情况按年龄和性别分布\")\nplt.xlabel(\"年龄\")\nplt.ylabel(\"总消费金额\")\nplt.legend(title=\"性别\")\nplt.show()"
    },
    {
        "title": "信用违约预测",
        "content": "1. 读取数据并查看前5行信息\ndata = pd.read_csv(\"credit-default.csv\")\nprint(data.head())\n2. 查看Target分布\nprint(data['Target'].value_counts())\ndata['Target'].value_counts().plot(kind='bar', title='Target分布')\n3. 相关系数矩阵及热力图\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n# 相关系数矩阵\ncorr_matrix = data.corr()\n\n# 绘制热力图\nplt.figure(figsize=(12, 10))\nsns.heatmap(corr_matrix, annot=True, cmap='coolwarm', fmt='.2f')\nplt.title(\"相关系数矩阵热力图\")\nplt.show()\n4. 删除相关性超过0.8的特征\n删除高相关性的特征，以减少多重共线性问题。\n# 找出相关性超过0.8的特征对并删除一个特征\nhigh_corr = corr_matrix[(corr_matrix.abs() > 0.8) & (corr_matrix.abs() < 1.0)]\ncolumns_to_drop = [column for column in high_corr.columns if any(high_corr[column])]\ndata = data.drop(columns=columns_to_drop)\n5. 统计各特征的缺失率\n# 统计缺失率\nmissing_rate = data.isnull().mean()\nprint(missing_rate)\n6. 名义型变量的缺失值处理\n针对名义型变量的缺失值，可以选择填充方式。\n# 填充缺失值 - 以mode填充名义型变量缺失值\nfor col in data.select_dtypes(include=['object']).columns:\n    data[col].fillna(data[col].mode()[0], inplace=True)\n7. 数据填充\n针对数值型变量缺失值，可以用均值或中位数填充。\n# 填充数值型变量缺失值\nfor col in data.select_dtypes(exclude=['object']).columns:\n    data[col].fillna(data[col].median(), inplace=True)\n8. 名义型变量独热编码\n对名义型变量进行独热编码，以增强表达能力。\n# 独热编码\ndata = pd.get_dummies(data, drop_first=True)\n9. 数据拆分\n将数据集拆分为训练集和测试集。\nfrom sklearn.model_selection import train_test_split\n\nX = data.drop(columns=['Target'])\ny = data['Target']\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n10. 引入随机森林算法并进行预测\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import f1_score\n\n# 初始化模型并训练\nrf_model = RandomForestClassifier(random_state=42)\nrf_model.fit(X_train, y_train)\n\n# 预测并计算f1 score\ny_pred = rf_model.predict(X_test)\nprint(\"初始模型 F1 Score:\", f1_score(y_test, y_pred))\n11. 使用SMOTE进行过采样\n使用SMOTE对数据进行过采样以平衡类别。\nfrom imblearn.over_sampling import SMOTE\n# 过采样\nsmote = SMOTE(random_state=42)\nX_train_res, y_train_res = smote.fit_resample(X_train, y_train)\n12. 重新训练随机森林模型\n# 重新训练模型\nrf_model.fit(X_train_res, y_train_res)\ny_pred_resampled = rf_model.predict(X_test)\nprint(\"SMOTE后模型 F1 Score:\", f1_score(y_test, y_pred_resampled))\n13. 模型调优\n可以使用网格搜索调优超参数。\nfrom sklearn.model_selection import GridSearchCV\n\n# 参数网格\nparam_grid = {\n    'n_estimators': [50, 100, 200],\n    'max_depth': [10, 20, 30],\n    'min_samples_split': [2, 5, 10]\n}\n\n# 网格搜索\ngrid_search = GridSearchCV(RandomForestClassifier(random_state=42), param_grid, cv=5, scoring='f1')\ngrid_search.fit(X_train_res, y_train_res)\n\n# 最优参数\nprint(\"最优参数:\", grid_search.best_params_)\n14. 输出最终模型的F1 Score\n# 使用最优参数重新训练模型\nrf_final = RandomForestClassifier(**grid_search.best_params_, random_state=42)\nrf_final.fit(X_train_res, y_train_res)\ny_pred_final = rf_final.predict(X_test)\n\n# 计算f1 score\nprint(\"最终模型 F1 Score:\", f1_score(y_test, y_pred_final))"
    },
    {
        "title": "data_cluster.csv\n 1、导入数据，统计行、列、时间日期\n 2、查看数据中的最大值，最小值，中位数，平均数以及四分位数\n 3、选择日期为20200319的数据判读空值处理并生成新数据集data_new\n 4、按用户ID浏量生成数据集data_new2\n 5、选择用户ID为5的流量信息分布绘制柱形图\n 6、选择用户ID为10的流量信息分布绘制饼状图",
        "content": "1. 导入数据，统计行、列及时间日期信息\ndata.shape\n# 假设时间日期列为'Date'，统计日期信息\ndata['Date'] = pd.to_datetime(data['Date'])\nprint(\"日期范围:\", data['Date'].min(), \"到\", data['Date'].max())\n2. 查看数据的最大值、最小值、中位数、平均数及四分位数\ndata.describe()\n3. 筛选日期为20200319的数据，处理空值并生成data_new\ndata_new = data[data['Date'] == '2020-03-19']\n\n# 空值处理，可以选择删除或填充\ndata_new = data_new.dropna()  # 或者使用data_new.fillna(0)填充\n\n# 显示空值处理后的数据集\nprint(data_new.head())\n4. 按用户ID生成data_new2数据集\n\n# 假设用户ID列为'UserID'，按UserID分组计算流量总和\ndata_new2 = data_new.groupby('UserID').sum().reset_index()\nprint(data_new2.head())\n5. 绘制用户ID为5的流量信息柱形图\nimport matplotlib.pyplot as plt\n\n# 筛选用户ID为5的数据\nuser_5_data = data_new[data_new['UserID'] == 5]\n\n# 绘制柱形图\nuser_5_data.plot(kind='bar', x='Date', y='Flow', title=\"用户ID为5的流量信息\")\nplt.xlabel(\"日期\")\nplt.ylabel(\"流量\")\nplt.show()\n6. 绘制用户ID为10的流量信息饼状图\nuser_10_data = data_new[data_new['UserID'] == 10]\n\n# 绘制饼状图\nuser_10_data.set_index('Date')['Flow'].plot(kind='pie', autopct='%1.1f%%', title=\"用户ID为10的流量分布\")\nplt.ylabel(\"\")  # 去掉y轴标签\nplt.show()"
    },
    {
        "title": "使用数据datamining01.csv\n导入数据集文件，查看除表头外的前5行\n统计每列数据的最大值，最小值，中位数，平均数以及四分位数\n计算每列缺失值所占百分比，并输出结果，输出结果包含缺失值数量，缺失值占比\n将缺失值超过70%列删除，并选取适当方法对缺失值进行填充",
        "content": "2. 统计每列数据的最大值、最小值、中位数、平均数及四分位数\n# 使用 describe 方法进行统计\nstats = data.describe().T  # 转置方便查看\nstats['median'] = data.median()\nprint(stats[['min', 'max', 'median', 'mean', '25%', '50%', '75%']])\n3. 计算每列缺失值的数量和占比\n# 计算每列缺失值数量\nmissing_counts = data.isnull().sum()\n# 计算每列缺失值占比\nmissing_percentage = (missing_counts / len(data)) * 100\n# 输出缺失值数量和占比\nmissing_info = pd.DataFrame({'缺失值数量': missing_counts, '缺失值占比': missing_percentage})\nprint(missing_info)\n4. 删除缺失值超过70%的列并对剩余缺失值进行填\n# 删除缺失值超过70%的列\ndata_cleaned = data.loc[:, missing_percentage <= 70]\n\n# 对剩余列的缺失值进行填充，使用适当的填充方法\n# 数值型列填充中位数，分类变量填充众数\nfor col in data_cleaned.columns:\n    if data_cleaned[col].dtype == 'object':  # 分类变量\n        data_cleaned[col].fillna(data_cleaned[col].mode()[0], inplace=True)\n    else:  # 数值型变量\n        data_cleaned[col].fillna(data_cleaned[col].median(), inplace=True)\n\n# 查看填充后的数据\nprint(data_cleaned.head())"
    },
    {
        "title": "使用数据train_data.csv,datamining01.csv\n导入数据查看除表头外的前5行head()\n统计每列数据中的最大值，最小值，中位数，平均数以及四分位数describe()\n计算每列中的缺失值占比，并输出结果，输出结果包含缺失值数量，缺失值占比isnull().\n删除无意义数据（unique=1列），删除无特征变化数据（值为0的列）\n将缺失值超过80%的列进行删除，\n绘制AGE 直方图",
        "content": "1. 导入数据并查看前5行\ndata = pd.read_csv(\"train_data.csv\")\ndata.head()\n2. 统计每列数据的最大值、最小值、中位数、平均数及四分位数\n# 统计每列的描述性统计信息\ndata.describe()\n3. 计算每列缺失值数量和占比\n# 计算每列的缺失值数量和缺失值占比\nmissing_counts = data.isnull().sum()\nmissing_percentage = (missing_counts / len(data)) * 100\n# 创建包含缺失值数量和占比的数据框\nmissing_info = pd.DataFrame({'缺失值数量': missing_counts, '缺失值占比': missing_percentage})\nprint(missing_info)\n4. 删除无意义数据列（唯一值为1的列）和无特征变化数据（全为0的列）\n# 删除唯一值为1的列\nunique_cols = data.columns[data.nunique() == 1]\ndata = data.drop(columns=unique_cols)\n\n# 删除所有值为0的列\nzero_cols = data.columns[(data == 0).all()]\ndata = data.drop(columns=zero_cols)\n\n# 查看删除后的数据\nprint(\"删除无意义列和无变化列后的数据:\", data.shape)\n5. 删除缺失值超过80%的列\n# 筛选出缺失值占比小于等于80%的列\ndata = data.loc[:, missing_percentage <= 80]\n\nprint(\"删除缺失值超过80%的列后的数据:\", data.shape)\n6. 绘制AGE的直方图\nimport matplotlib.pyplot as plt\nplt.hist(data['AGE'].dropna(), bins=20, color='skyblue', edgecolor='black')\nplt.title(\"AGE 直方图\")\nplt.xlabel(\"AGE\")\nplt.ylabel(\"频数\")\nplt.show()"
    },
    {
        "title": "电影数据集，推荐",
        "content": NaN
    },
    {
        "title": "数据集diabetes.csv",
        "content": "1. 导入数据并查看前5行\ndata = pd.read_csv(\"diabetes.csv\")\n\n2. 查看OutCome列的数据分布\noutcome_distribution = data['OutCome'].value_counts()\n3. 使用train_test_split对数据进行XY拆分\nfrom sklearn.model_selection import train_test_split\n# 特征和目标变量拆分\nX = data.drop('OutCome', axis=1)\ny = data['OutCome']\n\n# 使用80-20比例拆分数据集\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n4. 使用XGBoost分类算法XGBClassifier进行预测并优化参数\nfrom xgboost import XGBClassifier\nfrom sklearn.metrics import roc_auc_score\n# 定义初始模型\nmodel = XGBClassifier(max_depth=3, learning_rate=0.1, random_state=42)\n\n# 训练模型\nmodel.fit(X_train, y_train)\n\n# 预测\ny_pred = model.predict_proba(X_test)[:, 1]\n\n# 计算AUC得分\nauc_score = roc_auc_score(y_test, y_pred)\nprint(\"初始模型的AUC得分:\", auc_score)\n\n# 参数调优，提高AUC得分\nbest_auc = auc_score\nbest_params = {'max_depth': 3, 'learning_rate': 0.1}\n\n# 尝试不同的参数组合\nfor depth in range(3, 8):\n    for lr in [0.01, 0.05, 0.1, 0.2]:\n        model = XGBClassifier(max_depth=depth, learning_rate=lr, random_state=42)\n        model.fit(X_train, y_train)\n        y_pred = model.predict_proba(X_test)[:, 1]\n        auc = roc_auc_score(y_test, y_pred)\n        \n        if auc > best_auc:\n            best_auc = auc\n            best_params = {'max_depth': depth, 'learning_rate': lr}\n\nprint(\"最佳AUC得分:\", best_auc)\nprint(\"最佳参数:\", best_params)"
    },
    {
        "title": "bigdata",
        "content": "data = [50,46,33,27,55,36,73,101] \nfor i in range(len(data)): #建立for循环，循环整个data列表\n    for j in range(0,len(data)-i-1):\n        if data[j]>data[j+1]: #if语句设定条件\n            data[j],data[j+1] = data[j+1],data[j] #结果，比较两个数，比较后按照大小互换位置\nprint(data) #输出排序后的列表\n\nimport numpy as np\nimport scipy as sp\n#1、生成一个0-14向量\nx = np.arange(15)\nprint(x)\n#2、将x转换为二维矩阵，矩阵的第一维度为1\ny = x.reshape(1,15)\nprint(y)\n#3、生成3*4矩阵\nA = np.arange(12).reshape(3,4)\nprint(A)\n#4、转置3中的矩阵\nprint(A.T)\n#5、给定线性方程组求解\n#10x_1 + 8x_2 + 12x_3 = 20\n#4x_1 + 4x_2 + 2x_3 = 8\n#2x_1 - 4x_2- 2x_3 = -5\nfrom scipy.linalg import solve\na = np.array([[10, 8, 12], [4, 4, 2], [2, -4, -2]])\nb = np.array([10,8,-5])\nx = solve(a, b)\nprint(x)\n\n#计算每列缺失值所占百分比，并输出结果，输出结果包含缺失值数量，缺失值占比\ncolumns = data.columns\nfor i in columns:\n    null_count = data[i].isnull().sum()\n    qszb = null_count/10000*100\n    print(i,\"缺失值数量为:{}\".format(null_count),\"-->缺失值占比为:{}%\".format(round(qszb,2)))\n\n#将缺失值超过70%列删除，并选取适当方法对缺失值进行填充\ndata_drop = data.dropna(axis=1,thresh=len(data)*0.3)\ndata_drop\n\n#各列均值填充各列缺失值\nfor i in data_drop.columns:\n    data_drop[i] = data_drop[i].fillna(np.mean(data_drop[i]))\n\n#再次查看数据集确实情况\ncolumns = data_drop.columns\nfor i in columns:\n    null_count = data_drop[i].isnull().sum()\n    qszb = null_count/10000*100\n    print(i,\"缺失值数量为:{}\".format(null_count),\"-->缺失值占比为:{}%\".format(round(qszb,2)))\n    #缺失全部填充完毕\n\nprint(df.isnull().any()) #查看数据是否含有缺失值，false表示不含缺失值\n\nprint(df.info()) #查看数据类型，浮点型数据共22列，整型数据共2列\n\nimport matplotlib.pyplot as plt #导入画图库\nimport seaborn as sns #导入seaborn画图库\nimport warnings \nwarnings.filterwarnings('ignore') #忽略告警提示\nplt.rcParams['font.family'] = ['sans-serif'] \nplt.rcParams['font.sans-serif'] = ['SimHei']#画图时防止中文不显示\n\nimport matplotlib\nmatplotlib.rcParams['axes.unicode_minus']=False #设置负号显示\nplt.figure(figsize=(20,10))\ndf_corr = df.corr(method='pearson') #特征相关性矩阵\nsns.heatmap(df_corr,annot=True) #特征热力图\nplt.show()\n#颜色越浅表示正相关越强，颜色越深表示负相关越强\n\ndrop = [] #设置一个列表，存储筛选出的相关性高的特征对\nfor i in df_corr.index: #遍历相关系数矩阵所有的行\n    for j in df_corr.columns:#遍历相关系数矩阵所有的列\n        if df_corr.loc[i,j]>0.8 and i!=j and (j,i) not in drop: #设定if语句条件，筛选出相关性大于0.8的特征对\n            drop.append((i,j)) #将筛选出的特征对加入drop列表中\nprint(drop) #输出存储的特征对\n\n#丢弃特征对中的一个，进行特征选择\ncols_to_drop = np.unique([col[1] for col in drop]) #对特征对进行去重，筛选出需要剔除的特征，包括Var4,10,16,17,18,19,20,21,22,23列\ndf.drop(cols_to_drop,axis=1,inplace=True)\nprint(df.head())\n\nfrom sklearn.tree import DecisionTreeClassifier #决策树\nfrom sklearn.neighbors import KNeighborsClassifier #KNN\nfrom sklearn.preprocessing import StandardScaler #标准化函数\nfrom sklearn.model_selection import cross_val_score #交叉验证得分函数\nfrom sklearn.metrics import confusion_matrix #混淆矩阵\nfrom sklearn import metrics\nfrom sklearn.metrics import roc_auc_score,f1_score,accuracy_score,precision_score,recall_score #评价指标\n\n\n#三个评估函数的集合\ndef eva(x,y):\n    print('f1_score:',round(f1_score(y,x),2),'查准率:',round(precision_score(y,x),2),'查全率',round(recall_score(y,x),2))\n    return\n\n#绘制ROC曲线\ndef plot_roc_curve(y_test,preds):\n    fpr,tpr,threshold = metrics.roc_curve(y_test,preds)\n    roc_auc = metrics.auc(fpr,tpr)\n    plt.title('ROC曲线')\n    plt.plot(fpr,tpr,'b',label = 'AUC = %0.2f'%roc_auc)\n    plt.legend(loc='lower right')\n    plt.plot([0,1],[0,1],'r--')\n    plt.xlim([-0.01,1.01])\n    plt.ylim([-0.01,1.01])\n    plt.ylabel('真正例率',fontsize=16)\n    plt.xlabel('假正例率',fontsize=16)\nplt.show()\n\n\n#划分训练集和测试集\nfrom sklearn.model_selection import train_test_split\nX = df.loc[:,df.columns!='lab'] #属性特征列\nY = df['lab'] #目标列\nx_train,x_test,y_train,y_test = train_test_split(X,Y,test_size=0.3,random_state=12) #训练集/测试集=7：3，随机种子数为12\n\n\nx_train_std = StandardScaler().fit_transform(x_train) #标准化数据\nx_test_std = StandardScaler().fit_transform(x_test)\n\ndtc = DecisionTreeClassifier(criterion='gini',max_depth=8)  #建立决策树模型，特征划分准则采用gini，最大树深度为8\ndtc.fit(x_train,y_train) #模型训练\nprint(dtc.score(x_test,y_test)) #模型得分\n\nplt.rcParams['font.family'] = ['sans-serif'] \nplt.rcParams['font.sans-serif'] = ['SimHei']#画图时防止中文不显示\ntest=[] #设定一个列表，存储模型得分\nplt.figure(figsize=(15,5)) #设置画布大小\nfor i in range(13): #for循环遍历0,12\n    dtc = DecisionTreeClassifier(max_depth=i+1,criterion='gini',random_state=30) #循环不同的max_depth来建立决策树模型\n    dtc.fit(x_train,y_train) #模型训练\n    score = dtc.score(x_test,y_test) #模型得分\n    test.append(score) #存储不同max_depth建立的决策树的得分\nplt.plot(range(1,14),test,color='blue',label='max_depth') #折线图可视化\nplt.xlabel('数值设置')\nplt.ylabel('模型准确率')\nplt.legend()\nplt.title('最大树深度max_depth影响图')\nplt.show()\n#可以看出max_depth为13时模型准确率最高，下一步基于max_depth为13时建立新的决策树模型\n\nnew_dtc = DecisionTreeClassifier(max_depth=13,criterion='gini',random_state=30) #建立新的决策树模型，max_depth为13\nnew_dtc.fit(x_train,y_train) #模型训练\nprint(new_dtc.score(x_test,y_test)) #模型得分\n\ny_pre_dtc = new_dtc.predict(x_test) #决策树模型预测值\n\ny_pro_dtc = new_dtc.predict_proba(x_test)[:,1] #模型预测概率\n\nprint(confusion_matrix(y_test,y_pre_dtc))#决策树模型混淆矩阵\n\nprint(eva(y_pre_dtc,y_test)) #决策树模型评估\n\n#决策树模型ROC曲线及AUC值\nplt.figure(figsize=(20,8))\nplt.xticks(fontsize=18)\nplt.yticks(fontsize=18)\nplot_roc_curve(y_test,y_pro_dtc)\nplt.show()\n\n#运用KNN算法建模\nK = np.arange(1,14) #K值选取范围为1到13\nacc=[] #存储不同K值建立的KNN模型准确率的列表\nfor i in K:\n    #交叉验证得分，使用accuracy准确率判定，cv=10是10折交叉验证\n    cv_score = cross_val_score(KNeighborsClassifier(n_neighbors=i,weights='uniform'),x_train_std,y_train,scoring='accuracy',cv=10)\n    acc.append(cv_score.mean()) #将n_neighbors为1到13所有情况下得分的均值存储在列表acc中\nplt.figure(figsize=(15,5)) #设置画布大小\nplt.plot(K,acc) #绘制K值-acc折线图\narg_max = np.array(acc).argmax() #取平均准确率中最大值的对应的下标\nplt.text(K[arg_max], acc[arg_max], '最佳k值为{}'.format(K[arg_max])) #设置文字说明，找到最佳K值\nplt.xlabel('k值')\nplt.ylabel('模型准确率')\nplt.title('不同K值下模型的平均准确率')\nplt.show()\n\n#建立最佳K值下的KNN模型\nknn = KNeighborsClassifier(n_neighbors=3,weights='uniform') #建立KNN模型\nknn.fit(x_train_std,y_train) #模型训练\nprint(knn.score(x_test_std,y_test)) #模型准确率得分\n\ny_pre_knn = knn.predict(x_test_std)#模型预测\ny_pro_knn = knn.predict_proba(x_test_std)[:,1] #KNN模型预测概率\n\nprint(confusion_matrix(y_test,y_pre_knn))#KNN模型混淆矩阵\nprint(eva(y_pre_knn,y_test)) #KNN模型评估\n#KNN模型ROC曲线及AUC值\nplt.figure(figsize=(20,8))\nplt.xticks(fontsize=18)\nplt.yticks(fontsize=18)\nplot_roc_curve(y_test,y_pro_knn)\nplt.show()\n\n#运用SVM建模\nfrom sklearn.svm import SVC\nsvm = SVC(C=1.0) #建立SVM模型，惩罚项C为1，其余参数默认\nsvm.fit(x_train_std,y_train) #模型训练\nprint(svm.score(x_test_std,y_test)) #模型准确率得分\nfrom sklearn.model_selection import GridSearchCV #导入网格搜索算法\nparam_svm = {'C':[0.05,0.1,0.3,0.5,1,2]} #搜索参数C\ngsearch_svm = GridSearchCV(estimator=svm,param_grid=param_svm,cv=5) #构建网格搜索，5折交叉验证\ngsearch_svm.fit(x_train_std,y_train) #模型训练\nprint(gsearch_svm.best_params_ )#搜索得到的模型最佳参数\nnew_svm = SVC(C=2.0,probability=True) #建立搜索得到的最优参数C的SVM模型\nnew_svm.fit(x_train_std,y_train) #模型训练\nprint(new_svm.score(x_test_std,y_test)) #模型得分\n\ny_pre_svm = new_svm.predict(x_test_std)#SVM模型预测\ny_pro_svm = new_svm.predict_proba(x_test_std)[:,1]#SVM模型预测概率\nprint(confusion_matrix(y_test,y_pre_svm)) #SVM模型的混淆矩阵\nprint(eva(y_pre_svm,y_test)) #SVM模型评估\n#SVM模型ROC曲线及AUC值\nplt.figure(figsize=(20,8))\nplt.xticks(fontsize=18)\nplt.yticks(fontsize=18)\nplot_roc_curve(y_test,y_pro_svm)\nplt.show()\n\n#保存模型到本地\nimport joblib\n#保存KNN模型\njoblib.dump(knn,'./knn.pkl')\n\nmodel_knn = joblib.load('./knn.pkl') #加载上述保存的KNN模型\nknn_yuce = model_knn.predict(x_test_std) #使用KNN模型对标准化后的测试集x_test_std进行预测\nprint(knn_yuce) #输出预测数据\n\ndata = [] #设定一个存储预测数据的列表data\nfor i in list(knn_yuce): #遍历模型预测出的所有数据\n    data.append(i) #向列表中加入数据\n\nf = open('C:\\\\yuce.txt','w') #在桌面上写入一个保存预测数据的txt文件，名称为yuce.txt\nfor i in data: #遍历data中的数据\n    f.write(str(i)+'\\n') #向txt文本中写入所有预测的数据，稍后打开桌面中的yuce.txt文档即可看到预测出的所有数据\n\ndf.describe(include='O') #统计离散型特征信息\n\ndf.describe() #查看连续型特征信息\n\n#各特征缺失值可视化\nimport missingno as mnso\nmnso.bar(df)\n\ndf.isnull().any() #不存在缺失值\nplt.rcParams['font.family'] = ['sans-serif'] \nplt.rcParams['font.sans-serif'] = ['SimHei']#画图时防止中文不显示\nplt.subplot(2,2,1)\ndf['Churn'].value_counts().plot.pie(autopct='%.1f%%',figsize =(15,15),fontsize=20)\nplt.subplot(2,2,2)\ndf['SeniorCitizen'].value_counts().plot.pie(autopct='%.1f%%',figsize =(15,15),fontsize=20)\n#可以看出流失用户占整体用户的26.5%，大约16%的人群是老年人，84%的人群是年轻人\n\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nplt.figure(figsize=(20,8),dpi=80)\nsns.countplot(df['Churn'],hue='gender',data=df)\nplt.xticks(fontsize=18)\nplt.yticks(fontsize=18)#可视化看出客户流失中男女分布均匀，性别属性对结果并无明显决策作用\n\n#查看年龄与客户流失之间的关系，发现年轻人群体保留率占比很高,在流失用户中，老年用户占比明显比年轻用户更高\nplt.figure(figsize=(20,8),dpi=80)\nsns.countplot(df['Churn'],hue='SeniorCitizen',data=df)\nplt.xticks(fontsize=18)\nplt.yticks(fontsize=18)\n\nplt.figure(figsize=(15,10))\nplt.subplot(2,2,1)\nsns.kdeplot(df.TotalCharges[(df[\"Churn\"] == 'No')],color=\"Red\", shade =True)\nsns.kdeplot(df.TotalCharges[(df[\"Churn\"] == 'Yes')],color=\"Blue\", shade= True)\nplt.legend([\"Not Churn\",\"Churn\"],loc='upper right')\nplt.xlabel('总费用')\nplt.ylabel('密度')\nplt.title('客户人群是否流失与总费用分布') #可以发现总费用在500左右时用户流失率最高\nplt.subplot(2,2,2)\nsns.kdeplot(df.MonthlyCharges[(df[\"Churn\"] == 'No')],color=\"Red\", shade = True)\nsns.kdeplot(df.MonthlyCharges[(df[\"Churn\"] == 'Yes')],color=\"Blue\", shade = True)\nplt.legend([\"Not Churn\",\"Churn\"],loc='upper right')\nplt.xlabel('月费用')\nplt.ylabel('密度')\nplt.title('客户人群是否流失与月费用分布')#可以发现月费用较高时，客户流失率也会增高\n\n#可视化PhoneService(是否有电话服务)与客户是否流失之间的关系，看出拥有电话服务的人群占比高，但是该特征对用户是否流失并无明显决策作用\nplt.figure(figsize=(20,8),dpi=80)\nsns.countplot(df['Churn'],hue='PhoneService',data=df)\nplt.xticks(fontsize=18)\nplt.yticks(fontsize=18)\n\n#命名属性集合为list_all\nlist_all = ['InternetService','OnlineSecurity', 'OnlineBackup', 'DeviceProtection', 'TechSupport','StreamingTV']\nplt.figure(figsize=(15,15))\nfor i,m in enumerate(list_all):\n    plt.subplot(3,3,(i+1))\n    sns.countplot(x=m,hue='Churn',data=df)\n    plt.xlabel(str(m))\n    plt.title('Churn by '+str(m))\n    i+=1\nplt.show() #可以看出No internet service(无互联网服务)用户的流失率相同，说明这几个因素对于不使用互联网服务的用户是否流失不具影响\n#而在网络服务中，不使用网络服务的用户流失占比较低，对于使用DSL和Fiber optic的用户来说，使用Fiber optic的用户更容易流失\n\n#删除customerID,使用LabelEncoder编码，将数据转换为连续型数值变量\nfrom sklearn.preprocessing import LabelEncoder\ndel df['customerID']\nle = LabelEncoder()\nnew_data = df.astype(str)\nnew_data_le = new_data.apply(le.fit_transform)\n\ndf_corr = new_data_le.corr(method=\"spearman\")\nplt.figure(figsize=(20,10))\nplt.xticks(fontsize=16)\nplt.yticks(fontsize=16)\nsns.heatmap(df_corr,annot=True)\n\n#数据集划分为训练集和测试集，拆分比例为0.3\nfrom sklearn.model_selection import train_test_split\nX = new_data_le.loc[:,new_data_le.columns!='Churn']\nY = new_data_le['Churn']\nx_train,x_test,y_train,y_test = train_test_split(X,Y,test_size=0.3)\n\nfrom sklearn import metrics \nfrom sklearn.preprocessing import StandardScaler #导入标准化函数\nfrom sklearn.metrics import roc_auc_score,f1_score,accuracy_score,precision_score,recall_score #评价指标\nx_train_std = StandardScaler().fit_transform(x_train) #标准化训练集\nx_test_std = StandardScaler().fit_transform(x_test)\n\n\n#三个评估函数的集合\ndef eva(x,y):\n    print('f1_score:',f1_score(y,x),'查准率:',precision_score(y,x),'查全率',recall_score(y,x))\n    return\n\n#绘制ROC曲线\ndef plot_roc_curve(y_test,preds):\n    fpr,tpr,threshold = metrics.roc_curve(y_test,preds)\n    roc_auc = metrics.auc(fpr,tpr)\n    plt.title('Receiver Operating Characteristic')\n    plt.plot(fpr,tpr,'b',label = 'AUC = %0.2f'%roc_auc)\n    plt.legend(loc='lower right')\n    plt.plot([0,1],[0,1],'r--')\n    plt.xlim([-0.01,1.01])\n    plt.ylim([-0.01,1.01])\n    plt.ylabel('True Positive Rate',fontsize=16)\n    plt.xlabel('False Positive Rate',fontsize=16)\nplt.show()\n\n#随机森林算法建模\nfrom sklearn.ensemble import RandomForestClassifier\nrf = RandomForestClassifier()\nrf.fit(x_train_std,y_train)\nrf.score(x_test_std,y_test)\n\n#预测值y_pre_rf与预测概率y_pro_rf\ny_pre_rf = rf.predict(x_test_std)\ny_pro_rf = rf.predict_proba(x_test_std)[:,1]\neva(y_pre_rf,y_test)#模型评估\n#ROC曲线图及AUC值\nplt.figure(figsize=(20,8))\nplt.xticks(fontsize=18)\nplt.yticks(fontsize=18)\nplot_roc_curve(y_test,y_pro_rf)\n\n#最后查看各个特征重要性，可视化并按照重要性数值大小排序\n#特征重要性图\nplt.rcParams['font.family'] = ['sans-serif'] \nplt.rcParams['font.sans-serif'] = ['SimHei']#画图时防止中文不显示\nimportance = pd.DataFrame() #建立一个DataFrame\nimportance['变量'] = x_train.columns\n#查看属性重要性\nimportance['重要性程度'] = rf.feature_importances_ #得到随机森林对特征的评价重要度\n#根据数值大小排序\nimportance = importance.sort_values(by = '重要性程度',ascending=False)\nplt.figure(figsize=(16,10))\nplt.yticks(fontsize=14)\nplt.xticks(fontsize=16,rotation=90)\nplt.ylabel('重要性程度',fontsize=14)\nplt.bar(importance['变量'].values.tolist()[:15],importance['重要性程度'].values.tolist()[:15])\n\ndf['class'].value_counts() #查看蘑菇类别分布\ndf.isnull().any() #查看数据是否有缺失值\n#查看蘑菇类别的占比(毒蘑菇：p，正常蘑菇：e)\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nplt.figure(figsize=(9,9))\nplt.rcParams['font.family'] = ['sans-serif'] \nplt.rcParams['font.sans-serif'] = ['SimHei']#画图时防止中文不显示\ndf['class'].value_counts().plot.pie(explode=[0,0.05],autopct='%.2f%%',textprops={'fontsize':20})\nplt.title('蘑菇类别占比',fontsize=20)\n\n#菌盖颜色进行直方图可视化，查看不同颜色菌盖的数量\nplt.rcParams['font.family'] = ['sans-serif'] \nplt.rcParams['font.sans-serif'] = ['SimHei']#画图时防止中文不显示\nplt.figure(figsize=(20,10))\n#设置条形图颜色\ndf['cap_color'].value_counts().plot.barh()\nplt.xlabel('数量',fontsize=20)\nplt.ylabel('菌盖颜色种类',fontsize=20)\n#可以看出在蘑菇届，棕、灰、红、黄、白的蘑菇占大多数\n\nplt.figure(figsize=(20,10))\nplt.rcParams['font.family'] = ['sans-serif'] \nplt.rcParams['font.sans-serif'] = ['SimHei']#画图时防止中文不显示\nsns.countplot(df['cap_color'],hue='class',data=df)\nplt.xlabel('菌盖颜色种类',fontsize=20)\nplt.ylabel('数量',fontsize=20)\n#(毒蘑菇：p，正常蘑菇：e)\n#可视化可以看出鲜艳的蘑菇有毒的可能性还是较高的，比如红色和棕色\n#但其他颜色的蘑菇并非就是完全可食用，棕色和灰色的蘑菇都是很常见的\n\n"
    }
]
    results = []
    for entry in json_data:
        if key.lower() in entry['content'].lower():
            results.append({'title': entry['title'], 'content': entry['content']})
    df = pd.DataFrame(results)
    df.style.set_properties(**{'white-space': 'pre-wrap', 'text-align': 'left'})
def daima_all():
    json_data=[
    {
        "title": "关联规则",
        "content": "# 读取数据并展示前五行\ndf = pd.read_csv('./data/online_retail_II.csv')\ndf.head()\n1.清理df数据：删除缺失值与重复值。\ndf.dropna(inplace=True)\ndf.drop_duplicates(inplace=True)\nfrom mlxtend.frequent_patterns import apriori, fpmax, fpgrowth\nfrom mlxtend.frequent_patterns import association_rules\n\n# 找到所有支持度超过0.03的项集\nbasket = df.groupby(['Invoice', 'StockCode'])['Quantity'].sum().unstack().fillna(0)\nbasket_sets = basket.applymap(lambda x: 1 if x > 0 else 0)\n使用 fpgrowth算法从数据basket_sets中计算频繁项集，并将最小支持度设置为 0.1。返回频繁项集frequent_itemsets\n\nfrequent_itemsets = fpgrowth(basket_sets, min_support=0.1, use_colnames=True)\nfrequent_itemsets\n使用association_rules从频繁项集frequent_itemsets中构建关联规则，metric为'confidence'， min_threshold为0.4。并将结果存储在rules中\n\nrules = association_rules(frequent_itemsets, metric='confidence', min_threshold=0.4)\n# 展示前五条关联规则\nrules.head()\n4.打印出同时满足置信度< 0.8，置信度>0.5，提升度>3.0的rules数据。\n/*此处由考生进行代码填写*/\nfiltered_rules = rules[(rules['confidence'] < 0.8) & (rules['confidence'] > 0.5) & (rules['lift'] > 3.0)]\n\n# 打印结果\nprint(filtered_rules)\n筛选规则rules，将规则中同时满足antecedent_sele为{'22111', '22271'}，consequent_sele为{'22272'}的规则选出来，并存储在final_sele中\n\n# 筛选 antecedents 和 consequents 满足条件的规则\nfinal_sele = (rules['antecedents'] == {'22111', '22271'}) & (rules['consequents'] == {'22272'})\n# 使用 final_sele 筛选出相应的规则\nrules.loc[final_sele ]"
    },
    {
        "title": "线性代数",
        "content": "生成一个包含整数0-11的向量\nx = np.arange(12)\n查看数组大小\nx.shape\n将x转换成二维矩阵，其中矩阵的第一个维度为1\nx = x.reshape(1,12)\n将x转换3x4的矩阵\nx = x.reshape(3,4)\n生成3*4的矩阵\nA = np.arange(12).reshape(3,4)\n转置\nA.T\n矩阵相乘：两个矩阵能够相乘的条件为第一个矩阵的列数等于第二个矩阵的行数。\nnp.matmul(A,B)\n矩阵对应运算：针对形状相同矩阵的运算统称，包括元素对应相乘、相加等，即对两个矩阵相同位置的元素进行加减乘除等运算。\n矩阵相乘：A*A\n矩阵相加：A + A\n逆矩阵实现：只有方阵才有逆矩阵\nA = np.arange(4).reshape(2,2)\nnp.linalg.inv(A)\n矩阵的特征值与特征向量\nA = [[1, 2],[2, 1]] #生成一个2*2的矩阵\nfrom scipy.linalg import eig\nevals, evecs = eig(A) #求A的特征值（evals）和特征向量(evecs)\n行列式\nnp.linalg.det(A)\n解线性方程组\nfrom scipy.linalg import solve\nx = solve(a, b)\n奇异值分解\n"
    },
    {
        "title": "概率论",
        "content": "ll = [[1,2,3,4,5,6],[3,4,5,6,7,8]]\nnp.mean(ll)  #全部元素求均值\nnp.mean(ll,0) #按列求均值，0代表列向量，1表示行向量\n求方差：\nnp.var(b)\nnp.var(ll,1)) #第二个参数为1，表示按行求方差\n标准差\nnp.std(ll)\n相关系数\nnp.corrcoef(a,b)\n二项分布\nfrom scipy.stats import binom, norm, beta, expon\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n#n,p对应二项式公式中的事件成功次数及其概率，size表示采样次数\nbinom_sim = binom.rvs(n=10, p=0.3, size=10000)\nprint('Data:',binom_sim)\nprint('Mean: %g' % np.mean(binom_sim))\nprint('SD: %g' % np.std(binom_sim, ddof=1))\n#生成直方图，x指定每个bin(箱子)分布的数据,对应x轴，binx是总共有几条条状图，normed值密度,也就是每个条状图的占比例比,默认为1\nplt.hist(binom_sim, bins=10, normed=True)\nplt.xlabel(('x'))\nplt.ylabel('density')\nplt.show()\n泊松分布\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n#产生10000个符合lambda=2的泊松分布的数\nX= np.random.poisson(lam=2, size=10000)  \n\na = plt.hist(X, bins=15, normed=True, range=[0, 15])\n#生成网格\nplt.grid()\nplt.show()\n正态分布\nimport matplotlib.pyplot as plt\nmu = 0\nsigma = 1\n#分布采样点\nx = np.arange(-5, 5, 0.1)\n#生成符合mu,sigma的正态分布\ny = norm.pdf(x, mu, sigma)\nplt.plot(x, y)\nplt.xlabel('x')\nplt.ylabel('density')\nplt.show()\n指数分布\nfrom scipy.stats import expon\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nlam = 0.5\n#分布采样点\nx = np.arange(0, 15, 0.1)\n#生成符合lambda为0.5的指数分布\ny = expon.pdf(x, lam)\nplt.plot(x, y)\nplt.title('Exponential: lam=%.2f' % lam)\nplt.xlabel('x')\nplt.ylabel('density')\nplt.show()\n\n中心极限定理\nimport numpy as np\nimport matplotlib.pyplot as plt\n#随机产生10000个范围为(1,6)的数\nramdon_data = np.random.randint(1,7,10000)\nprint(ramdon_data.mean())\nprint(ramdon_data.std())\n生成直方图\nplt.figure()\nplt.hist(ramdon_data,bins=6,facecolor='blue')\nplt.xlabel('x')\nplt.ylabel('n')\nplt.show()\n随机抽取1000组数据，每组50个\nsamples = []\nsamples_mean =[]\nsamples_std = []\n\n#从生成的1000个数中随机抽取1000组\nfor i in range(0,1000):\nsample = []\n#每组随机抽取50个数\n    for j in range(0,50):\n        sample.append(ramdon_data[int(np.random.random() * len(ramdon_data))])\n  #将这50个数组成一个array放入samples列表中\n    sample_ar = np.array(sample)\nsamples.append(sample_ar)\n#保存每50个数的均值和标准差\n    samples_mean.append(sample_ar.mean())\nsamples_std.append(sample_ar.std())\n#samples_std_ar = np.array(samples_std)\n#samples_mean_ar = np.array(samples_mean)\n# print(samples_mean_ar)\n梯度下降法\n训练集(x,y)共5个样本,每个样本点有3个分量 (x0,x1,x2)  \nx = [(1, 0., 3), (1, 1., 3), (1, 2., 3), (1, 3., 2), (1, 4., 4)]  \ny = [95.364, 97.217205, 75.195834, 60.105519, 49.342380]  y[i] 样本点对应的输出  \nepsilon = 0.0001  #迭代阀值，当两次迭代损失函数之差小于该阀值时停止迭代  \nalpha = 0.01  #学习率\ndiff = [0, 0]  \nmax_itor = 1000  \nerror1 = 0  \nerror0 = 0  \ncnt = 0  \nm = len(x)  \n#初始化参数  \ntheta0 = 0  \ntheta1 = 0  \ntheta2 = 0  \nwhile True:  \n    cnt += 1  \n\n    # 参数迭代计算  \n    for i in range(m):  \n        # 拟合函数为 \ny = theta0 * x[0] + theta1 * x[1] +theta2 * x[2]  \n        # 计算残差，即拟合函数值-真实值  \n        diff[0] = (theta0** x[i][0] + theta1 * x[i][1] + theta2 * x[i][2]) - y[i]  \n  \n        # 梯度 = diff[0] * x[i][j]。根据步长*梯度更新参数 \n        theta0 -= alpha * diff[0] * x[i][0]  \n        theta1 -= alpha * diff[0] * x[i][1]  \n        theta2 -= alpha * diff[0] * x[i][2]  \n  \n    # 计算损失函数  \n    error1 = 0  \n    for lp in range(len(x)):  \n        error1 += (y[lp]-(theta0 + theta1 * x[lp][1] + theta2 * x[lp][2]))**2/2  \n    #若当两次迭代损失函数之差小于该阀值时停止迭代，跳出循环；\n    if abs(error1-error0) < epsilon:  \n        break  \n    else:  \n        error0 = error1  \n  \n    print(' theta0 : %f, theta1 : %f, theta2 : %f, error1 : %f' % (theta0, theta1, theta2, error1) )  \n\nprint('Done: theta0 : %f, theta1 : %f, theta2 : %f' % (theta0, theta1, theta2)  )\nprint('迭代次数: %d' % cnt  )\n"
    },
    {
        "title": "斐波那契数列",
        "content": "def fibonacci(n):\nif n==1:\nreturn [0]\nif n==2:\nreturn [0,1]\n    fib = [0, 1]\n    for i in range(2, n):\n        next_value = fib[i-1] + fib[i-2]\n        fib.append(next_value)\n    return fib[:n]\n\n# 生成前10个斐波那契数列的值\nfibonacci_10 = fibonacci(10)"
    },
    {
        "title": "字典",
        "content": "# 创建字典并存储元素\ndict_data = {'name':'lee', 'age':22, 'gender':'male'}\n\n查看字典items\ndict_data.items()\n插入一个键值对\nd[\"mark\"]=99\n# 分别将字典中的键和值全部输出\nkeys = list(dict_data.keys())\nvalues = list(dict_data.values())\n\n# 获取字典中键'name'对应的值\nname_value = dict_data['name']\n# 1. 新建字典 Dict1\nDict1 = {'name': 'lee', 'age': 89, 'num': [1, 2, 8]}\n\n# 2. 浅拷贝 Dict1，副本命名为 Dict_copy\nDict_copy = Dict1.copy()\n\n# 3. 创建集合，命名为 sample_set，包含2个元素 \"Prince\", \"Techs\"\nsample_set = {\"Prince\", \"Techs\"}\n\n# 4. 检查集合 sample_set 中是否存在某一元素 \"Data\" 并打印结论\nexists_data = \"Data\" in sample_set\ncheck_conclusion = \"存在\" if exists_data else \"不存在\"\n\n# 5. 向集合 sample_set 中增加元素 \"Data\"\nsample_set.add(\"Data\")\n\n# 6. 将元素 'Techs' 从集合 sample_set 中移除\nsample_set.remove(\"Techs\")\n\n# 7. 将集合 sample_set 分别转换为元组和列表结构，并打印输出\nsample_set_tuple = tuple(sample_set)\nsample_set_list = list(sample_set)"
    },
    {
        "title": "列表，冒泡排序",
        "content": "# 创建列表并存储元素 [310, 7]\nlst = [310, 7]\n\n# 打印输出列表\nprint(lst)\n#在末尾添加一个元素3\nlst.append(3)\n# 在列表元素下标为2的位置插入元素5\nlst.insert(2, 5)\n#列表从小到大排序\nlst.sort()\n从大到小\nlst.sort(reverse=True)\n# 冒泡排序，从大到小\nfor i in range(len(lst)):\n    for j in range(0, len(lst)-i-1):\n        if lst[j] < lst[j+1]:\n            lst[j], lst[j+1] = lst[j+1], lst[j]\n\n# 打印排序后的列表\nprint(lst)\n\n# 在列表末尾位置添加元素 'fish'\nlst.append('fish')\n将最后一个元素替换为100\nlst[-1]=100\n\n\n# 将列表中的字符串元素 'fish' 转换为全部大写并替换原本的 'fish'\nlst[lst.index('fish')] = 'FISH'\n字符串a=\"Victory\"\na.upper()全改为大写\na.lower()全改为小写\na.title()字符串改为首字母大写\n"
    },
    {
        "title": "集合",
        "content": "#创建集合\ns=set([1,2])\ns={1,2}\ns.add(3)\n# 删除元素（如果元素不存在，会引发KeyError）\ns.remove(2)\n清空集合\nset1.clear()\nset1.update([6, 7, 8])  # 添加元素，可以是列表、元组、字典等\n# 2. 检查集合中是否在元素'numpy'\ncontains_numpy = 'numpy' in sample\n# 3. 删除集合中的元素'pandas'\nsample.discard('pandas')\n# 4. 将集合分别转化为列表和元组并输出\nlist_from_set = list(sample)\ntuple_from_set = tuple(sample)\n\n# 5. 使用 copy()对集合进行浅拷贝\nsample_copy = sample.copy()\n"
    },
    {
        "title": "总-代码总结",
        "content": "一、算法\n1、线性回归\nfrom sklearn.linear_model import LinearRegression\nmodel.coef_ w 系数 model.intercept_ 截距\n2、逻辑回归\nfrom sklearn.linear_model import LogisticRegression\n3、KNN\nfrom sklearn.neighbors import KneiborsClassifier\nKneiborsClassifier(n_neighbors =4, algorithm = “ball_tree”)\n4、朴素贝叶斯\nfrom sklearn.naive_bayes import BernoulliNB\n5、SVM\nfrom sklearn.svm import svc\nsvc(c = 1.0, kernel = “rbf”)\nc：惩罚系数，默认是 0，C 越小，泛化能力越强。\nKernel：核函数 rbf 是径向基（高斯）此外还有线性 linear、多项式、poly、sigmoid\n6、决策树\nfrom sklearn.tree import DecisionTreeClassifier\n7、集成算法\nfrom sklearn.ensemble import RandomForestClassifier,  BaggingClassifier，\nAdaBoostClassifier，GradientBosstingClassifier\n\n通用 参数 n_estimators = 10 ,基类学习器个数。\nGradientBosstingClassifier(max_depth = 5)\nmax_depth = 5 树最大深度\nfrom xgboost.sklearn import XGBClassifier\n8、Kmeans\nfrom sklearn.cluster import Kmeans\nKmeans(n_clusters = k, init=’k_means++’,max_iter = 300)\nmodel.labels_ 类标签\nmodel.cluster_centers_ 簇中心\n\nestimator = KMeans(n_clusters=3)\nestimator.fit(X)\nlabel_pred = estimator.labels_\nx0 = X[label_pred == 0]\nx1 = X[label_pred == 1]\nx2 = X[label_pred == 2]\n\n\n\n\n9、AgglomerativeClustering\nfrom sklearn.cluster import AgglomerativeClustering\nAgglomerativeClustering(n_clusters=2, affinity=“euclidean”, compute_full_tree,linkage = “ward”）\ncompute_full_tree = False 时，训练 n_cluesters 后，训练停止；True 则训练整颗树。\nlinkage 的参数为\n{“ward”, “complete”,“average”, “single”}\n\n10、Birch\nfrom sklearn.cluster import Birch\nBirch(threshold=0.5, branching_factor=50, n_clusters=3)\nthreshold：float，表示设定的半径阈值，默认 0.5。\nbranching_factor：int，默认=50，每个节点最大特征树子集群数。\nn_clusters：int，默认=3，最终聚类数目。\n11、DBSCAN\nfrom sklearn.cluster import DBSCAN\nDBSCAN(eps=0.5, min_samples=5, metric=’euclidean)\neps：两个样本被看作邻居节点的最大距离。\nmin_samples：最小簇的样本数。\nmetric：距离计算方式，euclidean 为欧氏距离计算。\n12、ariori\nfrom mlxtend.frequent_patterns import apriori\nApriori(df, min_support=0.5, use_colnames=False, max_len=None,\nn_jobs=1)\n其中 df 代表数据框数据集，min_support 表示指定的最小支持度，\nuse_colnames=True 表示使用元素名字，默认的 False 使用列名代表元素，max_len 表\n示生成的项目集的最大长度。如果为 None，则评估所有可能的项集长度。\n二、画图\nplt.xlabel('pca1')\nplt.ylabel('pca2')\nplt.title(\"PCA\")\nplt.legend(loc='lower left')\n1、条形图\n数据为：\ngrouped = data.groupby(['是否高质用户', '网络类型'])['用户标识符'].count().unstack()\n\n网络类型 2G 3G 4G\n是否高质用户   \n0 1630.0 1668.0 1699.0\n1 NaN 2530.0 2473.0\ndf.plot.bar(rot = 0) //直接 df 画图 rot 是下标志是否字体是立着的还是卧着的。\ngrouped.plot(kind='bar', alpha=1.0, rot=0)\n2、散点图\ndata.plot.scatter(x = '类目 2 消费金额', y = 6, c = 'br')\nx = 为列名 列名可以用字符串 或如 y 的序列序数\nc = 'br'是颜色， 如蓝色和红色。\nplt.scatter(data.loc[:, '类目 2 消费金额'], data.loc[:, '类目 3 消费金额'], c=y,alpha=0.5)\n# 生成数据可视化\ny = data.loc[:, '是否高质用户']\nplt.scatter(data.loc[:, '类目 2 消费金额'], data.loc[:, '类目 3 消费金额'], c=y,alpha=0.5)\n这种图的颜色点由 y 决定。\n3、饼图\ngrouped.plot.pie(autopct = '%0.01f', subplots = True)\nautopct 为是否在饼图画百分比；Subplots = True 为是否为每个列画单独子图。\n4、3D 画图\nfrom mpl_toolkits.mplot3d import Axes3D\nfig = plt.figure()\n方法一\nax = fig.gca(projection='3d')\n方法二\nax = Axes3D(fig)\nax.scatter(X[y==i ,0], X[y==i, 1], X[y==i,2], c=c,marker=m,\nlabel=l)\n#做三维曲面图，rstride 和 cstride 分别代表行和列的跨度，cmap:曲面颜色\nax.plot_surface(X, Y, Z1, rstride=1, cstride=1, cmap='rainbow')\n5、热力图\nfig = plt.figure(figsize = (25.10))\nplt.subplot(1,2,1)\n# 关系热力图\nmask_corr = np.zeros_like(df.corr())\nmast_corr[np.triu_indices_from(mask_corr)] = True\nsns.heatmap(df.corr(), mask = mask_corr)\n6、直方图\n#x 指定每个 bin(箱子)分布的数据,对应 x 轴，binx 是总共有几条条状图，\nnormed 值密度,也就是每个条状图的占比例比,默认为 1\nplt.hist(binom_sim, bins=10, normed=True)\n\ncond = data['是否高质用户'] == 1\ndata[cond]['类目1消费金额'].hist(alpha=0.5, label='高质用户')\ndata[~cond]['类目1消费金额'].hist(color='r', alpha=0.5, label='非高质用户')\nplt.legend()\n三、缺失值处理\n1、检测\nmissing_sum = df.isnull().sum().sorted_values(ascending=False) //检测缺失数量\nmissing_rate = missing_sum/df.shape[0].sort_values(ascending=False)\nmissing_stat = pd.concat([missing_sum,missing_rate], keys=['missing_sum','missing_rate', axis = ‘columns’])\n2、删除\ndf.drop(columns = cols_to_drop,inplace=True)\ndel df[cols_to_drop]\n3、填充\n3.1 非监督填充方法一\nfrom sklearn.preprocessing import Imputer\ndata = Imputer(missing_values='NaN', strategy='most_frequent',\naxis=0) //strategy =mean/median/most_frequent\ndataMode = data.fit_transform(df)\n方法二、\nvalues = {'A': 0, 'B': 1, 'C': 2, 'D': 3}\ndf.fillna(value=values)\n3.2 算法填充\n参见具体算法\n\n四、异常值处理\n1、散点图检测\n具体见绘图\n2、利用决策树等算法预测\n具体见算法\n3、 3σ原则\nmin_mask = df[\"weight\"] < (df[\"weight\"].mean() - 3 *  df[\"weight\"].std())\nmax_mask = df[\"weight\"] > (df[\"weight\"].mean() + 3 * df[\"weight\"].std())\n# 只要满足上诉表达式的任一个就为异常值，所以这里使用位与运算\nmask = min_mask | max_mask\nprint(df.loc[mask,\"weight\"])\n\n4、IQR\niqr = Ser.quantile(0.75)-Ser.quantile(0.25)\nLow = Ser.quantile(0.25)-1.5*iqr\nUp = Ser.quantile(0.75)+1.5*iqr\nindex = (Ser< Low) | (Ser>Up)\nreturn index\n\n五、特征缩放\n1、标准化\n方法一、\nfrom sklearn.preprocessing import scale\nX =  scale(X)\n方法二、\nfrom sklearn.preprocessing import StandardScaler\nX = StandardScaler().fit_transform(X)\n2、最小值-最大值归一化\nfrom sklearn.preprocessing import MinMaxScaler\nX = MinMaxScaler(feature_range=(0, 1), copy=True).fit_transform(X)\n3、均值归一化\nmean=np.mean(x)\nmin=np.min(x)\nmax=np.max(x)\nMeanNormalization=(x-mean)/(max-min)\n4、缩放成单位向量\nlinalg = np.linalg.norm(x, ord=1)\nX=x/linalg\n\n六、数值离散化\n1、聚类算法\n详细见算法\n2、等宽划分\nx=pd.cut(X,5)\n3、等频划分\nx=pd.qcut(X,5)\n\n七、特征编码\n1、独热编码\nfrom sklearn.preprocessing import OneHotEncoder\nenc = OneHotEncoder(sparse = False)\nx = enc.fit_transform(iris.data)\n2、哑编码\npd.get_dummies(X) //转完后全部记得变为 int\nX = X.astype(np.int)\nX.info()\n\n\n\n\n3、LabelEncoder\n方法一、\nle = preprocessing.LabelEncoder()\nle.fit([\"paris\", \"paris\", \"tokyo\", \"amsterdam\"])\nlist(le.classes_) #['amsterdam', 'paris', 'tokyo']\nle.transform([\"tokyo\", \"tokyo\", \"paris\"]) # array([2, 2, 1]...)\nlist(le.inverse_transform([2, 2, 1]))#['tokyo', 'tokyo', 'paris']\n方法二、\ndf['Sex'] = df['Sex'].map({'female': 1, 'male': 0})\n\n八、特征选择\n1、过滤法\n方法一、方差\nfrom sklearn.feature_selection import VarianceThreshold\nX_var=VarianceThreshold(threshold=0.5).fit_transform(X, y) \n #使用阈值0.5 进行选择，特征方差小于 0.5 的特征会被删除\n方法二、卡方\nfrom sklearn.feature_selection chi2,SelectKBest\nSelectKBest(chi2, k=2).fit_transform(iris.data, iris.target)\n方法三、互信息素\nfrom sklearn.feature_selection import mutual_info_classif\nX_mut = mutual_info_classif(X, y)\n\n2、包装法 RFE\nfrom sklearn.feature_selection import RFE\nfrom sklearn.linear_model import LogisticRegression\nx_rfe=RFE(estimator=LogisticRegression(), n_features_to_select=3).fit(X, y)\nprint(x_rfe.n_features_ ) # 所选特征的数量\nprint(x_rfe.support_ ) # 按特征对应位置展示所选特征，True 表示保留，False 表示剔除。\nprint(x_rfe.ranking_ ) # 特征排名，使得 ranking_[i]对应于第 i 个特征的排名位置。\nprint(x_rfe.estimator_ ) # 递归方法选择的基模型\n\n3、嵌入法\n方法一、逻辑回归\nprint(lr.coef_)\n方法二、L1 Lasso\nfrom sklearn.linear_model import Lasso\nls = Lasso()\nls.fit(X, Y)\nls.coef_\n方法三、随机森林\nrf = RandomForestRegressor(n_estimators=15, max_depth=6)\nboston_rf=rf.fit(X, y)\nboston_rf.feature_importances_\n\n九、降维\n1、PCA\nfrom sklearn.decomposition import PCA\nX_std = preprocessing.scale(X)\npca = PCA(n_components=2)\nX_pca =pca.fit(X_std).transform(X_std)\nprint(pca.explained_variance_ratio_)# 观测降维后特征信息量大小。\n2、LDA\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis\nas LDA\nlda = LDA(n_components=2)\nX_lda =lda.fit(X,y).transform(X)\nprint(lda.explained_variance_ratio_)\n\n十、模型评估调优\n1、常见普通\nfrom sklearn.metrics import accuracy_score, confusion_matrix, recall_score, f1_score,  fbeta_score\nprint(accuracy_score(y_true, y_pred))\nprint(fbeta_score(y_true, y_pred, beta=0.5))\n2、分类结果统计报告\nfrom sklearn.metrics import classification_report\nprint(classification_report(y_test, y_pred))\n3、样本划分\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y_labels, test_size=0.2)\n4、交叉验证\nfrom sklearn.model_selection import cross_val_score\ncv_scores = cross_val_score(rf_model, X_train,\ny_train,scoring=make_scorer(recall_score) ,cv=5)\nprint('mean f1_score socre of raw model{}'.format(np.mean(cv_scores)))\n\n\n\n5、网格调优\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.metrics import make_scorer\nmodel_gbr = GradientBoostingRegressor()\nparameters = {'loss': ['ls','lad','huber','quantile'],'min_samples_leaf':\n[1,2,3,4,5],'alpha': [0.1,0.3,0.6,0.9]}\nmodel_gs = GridSearchCV(estimator=model_gbr, param_grid=parameters, cv=5, scoring=make_scorer(f1_score))\nmodel_gs.fit(X_train,y_train)\nprint('Best score is:', model_gs.best_score_)\nprint('Best parameter is:', model_gs.best_params_)\n6、过采样\nfrom imblearn.over_sampling import SMOTE\nsmote_model = SMOTE(random_state=7, ratio=0.1)\nX_train_res_rf,y_train_res_rf = smote_model.fit_resample(X_train,y_train)\nprint('Resampled dataset shape {}'.format(y_train_res_rf))\n#过采样比例为 0.1，即目标的正负样本比例为 1:10\n\n十一、其他小知识点\n1、文件操作：\n写文件：\n# 使用 write 方法写文件\nwith open(\"f.txt\", \"w\") as f:\nf.write( \"www.huawei.com\")\n读取文件：\n# 使用 read 方法读取\nwith open(\"f.txt\", \"r\") as f:\nprint(f.read())\n2、实验模型的导出以及导入\nimport pickle\nfrom sklearn.externals import joblib\njoblib.dump(svm, 'svm.pkl')\nsvm = joblib.load('svm.pkl')\n3、间隔取值\nlist[::2 ] #list 间隔取值\ndf = df[[i%2==0 for i in range(len(df.index))]] #df 按行取样本\ndf=df.iloc[:,[i%2==0 for i in range(len(df.columns))]]#df按列取样本\n\n\n\n\n4、取序号值\narr = np.array([3, 22, 4, 11, 2, 44, 9])\nprint(np.max(arr))  # 44\nprint(np.argmax(arr))  # 5\nprint(np.argmin(arr))  # 4\nprint(np.where(arr > 4, arr - 10, arr * 10))  # [10 20 30 40 -5 -4 -3 -2]\n\ndef modthree(x):\n    return x % 3 ==0\nprint(np.where(modthree(arr), arr - 10, arr * 10))  # [ -7 220  40 110  20 440  -1]\n\narr = np.array([3, 4, 5, 6, 7])\nprint(np.argwhere(arr % 2 != 0))\nprint(np.argwhere(arr % 2 != 0).flatten())  # [0 2 4]  可以用来取序号\n3、浮点数两位\ndf.round(2)\n5、斐波那契数列\n# 位置参数\ndef fibs(num):\nresult = [0,1]# 新建列表存储数列的值\nfor i in range(2,num):# 循环 num-2 次\na = result[i-1] + result[i-2]\n# 将值追加至列表\nresult.append(a)\n# 返回列表\nreturn result\nfibs(5)\n# 输出：[0, 1, 1, 2, 3]\n\n十二、概率论：\n1、二项分布贝努力\nfrom scipy.stats import binom\nbinom_sim = binom.rvs(n=10, p=0.3, size=10000) #\n2、泊松分布\nX= np.random.poisson(lam=2, size=10000)\n3、正态分布\nfrom scipy.stats import norm\nx = np.arange(-5, 5, 0.1)\ny = norm.pdf(x, mu, sigma)\n4、指数分布\nfrom scipy.stats import expon\nx = np.arange(0, 15, 0.1)\ny = expon.pdf(x, lam)\n"
    },
    {
        "title": "电信用户分析,聚类",
        "content": "import pandas as pd\nimport numpy as np\nimport matplotlib.pylab as plt\n%matplotlib inline\n\nX = pd.read_csv('./telecom.csv', encoding='utf-8')\nprint(X.shape)\nX.head()\n\n# 数据预处理\nfrom sklearn import preprocessing\nX_scaled = preprocessing.scale(X)  # scale操作之后的数据零均值，单位方差（方差为1）\nX_scaled[0:5]\n\n# 进行PCA数据降维\nfrom sklearn.decomposition import PCA\n \n# 生成PCA实例\npca = PCA(n_components=3)  # 把维度降至3维\n# 进行PCA降维\nX_pca = pca.fit_transform(X_scaled)\n# 生成降维后的dataframe\nX_pca_frame = pd.DataFrame(X_pca, columns=['pca_1', 'pca_2', 'pca_3'])  # 原始数据由(30000, 7)降维至(30000, 3)\nX_pca_frame.head()\n\n# 训练简单模型\nfrom sklearn.cluster import KMeans\n \n# KMeans算法实例化，将其设置为K=10\nest = KMeans(n_clusters=10)\n \n# 作用到降维后的数据上\nest.fit(X_pca)\n\n# 取出聚类后的标签\nkmeans_clustering_labels = pd.DataFrame(est.labels_, columns=['cluster'])  # 0-9,一共10个标签\n \n# 生成有聚类后的dataframe\nX_pca_frame = pd.concat([X_pca_frame, kmeans_clustering_labels], axis=1)\n \nX_pca_frame.head()\n\n# 对不同的k值进行计算，筛选出最优的K值\nfrom mpl_toolkits.mplot3d import Axes3D  # 绘制3D图形\nfrom sklearn import metrics\n \n# KMeans算法实例化，将其设置为K=range(2, 14)\nd = {}\nfig_reduced_data = plt.figure(figsize=(12, 12))  #画图之前首先设置figure对象，此函数相当于设置一块自定义大小的画布，使得后面的图形输出在这块规定了大小的画布上，其中参数figsize设置画布大小\nfor k in range(2, 14):\n    est = KMeans(n_clusters=k, random_state=111)\n    # 作用到降维后的数据上\n    y_pred = est.fit_predict(X_pca)\n    # 评估不同k值聚类算法效果\n    calinski_harabaz_score = metrics.calinski_harabasz_score(X_pca_frame, y_pred)  # X_pca_frame：表示要聚类的样本数据，一般形如（samples，features）的格式。y_pred：即聚类之后得到的label标签，形如（samples，）的格式\n    d.update({k: calinski_harabaz_score})\n    print('calinski_harabaz_score with k={0} is {1}'.format(k, calinski_harabaz_score))  # CH score的数值越大越好\n    # 生成三维图形，每个样本点的坐标分别是三个主成分的值\n    ax = plt.subplot(4, 3, k - 1, projection='3d') #将figure设置的画布大小分成几个部分，表示4(row)x3(colu),即将画布分成4x3，四行三列的12块区域，k-1表示选择图形输出的区域在第k-1块，图形输出区域参数必须在“行x列”范围\n    ax.scatter(X_pca_frame.pca_1, X_pca_frame.pca_2, X_pca_frame.pca_3, c=y_pred)  # pca_1、pca_2、pca_3为输入数据，c表示颜色序列\n    ax.set_xlabel('pca_1')\n    ax.set_ylabel('pca_2')\n    ax.set_zlabel('pca_3')\n\n# 绘制不同k值对应的score，找到最优的k值\nx = []\ny = []\nfor k, score in d.items():\n    x.append(k)\n    y.append(score)\n \nplt.plot(x, y)\nplt.xlabel('k value')\nplt.ylabel('calinski_harabaz_score')\n\nX.index = X_pca_frame.index  # 返回：RangeIndex(start=0, stop=30000, step=1)\n \n# 合并原数据和三个主成分的数据\nX_full = pd.concat([X, X_pca_frame], axis=1)\nX_full.head()\n\n# 按每个聚类分组\ngrouped = X_full.groupby('cluster')\n \nresult_data = pd.DataFrame()\n# 对分组做循环，分别对每组进行去除异常值处理\nfor name, group in grouped:\n    # 每组去除异常值前的个数\n    print('Group:{0}, Samples before:{1}'.format(name, group['pca_1'].count()))\n\n    desp = group[['pca_1', 'pca_2', 'pca_3']].describe() # 返回每组的数量、均值、标准差、最小值、最大值等数据\n    for att in ['pca_1', 'pca_2', 'pca_3']:\n        # 去异常值：箱形图\n        lower25 = desp.loc['25%', att]\n        upper75 = desp.loc['75%', att]\n        IQR = upper75 - lower25\n        min_value = lower25 - 1.5 * IQR\n        max_value = upper75 + 1.5 * IQR\n        # 使用统计中的1.5*IQR法则，删除每个聚类中的噪音和异常点\n        group = group[(group[att] > min_value) & (group[att] < max_value)]\n    result_data = pd.concat([result_data, group], axis=0)\n    # 每组去除异常值后的个数\n    print('Group:{0}, Samples after:{1}'.format(name, group['pca_1'].count()))\nprint('Remain sample:', result_data['pca_1'].count())\n\n# 设置每个簇对应的颜色\ncluster_2_color = {0: 'red', 1: 'green', 2: 'blue', 3: 'yellow', 4: 'cyan', 5: 'black', 6: 'magenta', 7: '#fff0f5',\n                   8: '#ffdab9', 9: '#ffa500'}\n \ncolors_clustered_data = X_pca_frame.cluster.map(cluster_2_color)  # 簇名和颜色映射\nfig_reduced_data = plt.figure()\nax_clustered_data = plt.subplot(111, projection='3d')\n \n# 聚类算法之后的不同簇数据的映射为不同颜色\nax_clustered_data.scatter(X_pca_frame.pca_1.values, X_pca_frame.pca_2.values, X_pca_frame.pca_3.values,\n                          c=colors_clustered_data)\nax_clustered_data.set_xlabel('Component_1')\nax_clustered_data.set_ylabel('Component_2')\nax_clustered_data.set_zlabel('Component_3')\n\n# 筛选后的数据聚类可视化\ncolors_filtered_data = result_data.cluster.map(cluster_2_color)\nfig = plt.figure(figsize=(12,12))\nax = plt.subplot(111, projection='3d')\nax.scatter(result_data.pca_1.values, result_data.pca_2.values, result_data.pca_3.values, c=colors_filtered_data)\nax.set_xlabel('Component_1')\nax.set_ylabel('Component_2')\nax.set_zlabel('Component_3')\n\n# 查看各族中的每月话费情况\nmonthly_Fare = result_data.groupby('cluster').describe().loc[:, u'每月话费']\nmonthly_Fare\n\n# mean：均值；std：标准差\nmonthly_Fare[['mean', 'std']].plot(kind='bar', rot=0, legend=True)  # rot可以控制轴标签的旋转度数。legend是否在图上显示图例\n\n# 查看各族中的入网时间情况\naccess_time = result_data.groupby('cluster').describe().loc[:, u'入网时间']\naccess_time\naccess_time[['mean', 'std']].plot(kind='bar', rot=0, legend=True, title='Access Time')\n\n# 查看各族中的欠费金额情况\narrearage = result_data.groupby('cluster').describe().loc[:, u'欠费金额']\narrearage[['mean', 'std']].plot(kind='bar', rot=0, legend=True, title='Arrearage')\n\n# 综合描述\nnew_column = ['Access_time', u'套餐价格', u'每月流量', 'Monthly_Fare', u'每月通话时长', 'Arrearage', u'欠费月份数', u'pca_1', u'pca_2',\n              u'pca_3', u'cluster']\nresult_data.columns = new_column\nresult_data.groupby('cluster')[['Monthly_Fare', 'Access_time', 'Arrearage']].mean().plot(kind='bar')  # 每个簇的Monthly_Fare、Access_time、Arrearag的均值放在一块比较"
    },
    {
        "title": "手写线性回归",
        "content": "import numpy as np\n\ndef simple_linear_regression(x, y):\n    # 计算x和y的平均值\n    x_mean = np.mean(x)\n    y_mean = np.mean(y)\n    \n    # 计算权重w\n    w = np.sum((x - x_mean) * (y - y_mean)) / np.sum((x - x_mean) ** 2)\n    \n    # 计算偏置b\n    b = y_mean - w * x_mean\n    \n    return w, b\n\ndef predict(x, w, b):\n    # 使用模型参数进行预测\n    return w * x + b\n\n# 示例数据\nx = np.array([10, 4, 6])\ny = np.array([8, 2, 5])\n\n# 训练模型并获取参数\nw, b = simple_linear_regression(x, y)\n\n# 使用模型进行预测\nx_new = np.array([12])\ny_pred = predict(x_new, w, b)\n\nprint(f\"预测结果: y = {w:.2f} * x + {b:.2f}\")\nprint(f\"对于x = {x_new[0]}, 预测的y值是: {y_pred[0]:.2f}\")\n\n\n*基于线性回归算法实现\nfrom sklearn.linear_model import LinearRegression\nmodel=LinearRegression()\nmodel.fit(data,y)\nprint('基于线性逻辑回归算法',model.coef_,model.intercept_)"
    },
    {
        "title": "方差选择模型（VarianceThreshold）",
        "content": "* 题目说阈值为1，但是hreshold=填空，有如下的备注，我就调整了hreshold，用了另外的判断方法，输出的结果为True False。要看True的值。\nfrom sklearn.feature_selection import VarianceThreshold\n\n# 定义方差选择模型，定义方差系数\nmodel_vt = VarianceThreshold(threshold=0.7) #，输出超过3个小于6个。"
    },
    {
        "title": "SMOTE过采样",
        "content": "pip install imbalanced-learn\nfrom imblearn.over_sampling import SMOTE\nfrom sklearn.datasets import make_classification\nfrom sklearn.model_selection import train_test_split\n\n# 创建一个不平衡的数据集\nX, y = make_classification(n_classes=2, class_sep=2,\n                           weights=[0.1, 0.9], n_informative=3, n_redundant=1, flip_y=0,\n                           n_features=20, n_clusters_per_class=1, n_samples=1000, random_state=10)\n\n# 划分数据集\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=0)\n\n# 实例化 SMOTE\nsm = SMOTE(random_state=42)\n\n# 应用 SMOTE\nX_res, y_res = sm.fit_resample(X_train, y_train)\n\n# 现在 X_res 和 y_res 包含了过采样后的训练数据\n"
    },
    {
        "title": "最小二乘法",
        "content": "import numpy as np  \nimport scipy as sp  \nimport pylab as pl  \nfrom scipy.optimize import leastsq  # 引入最小二乘函数  \n\nn = 9  # 多项式次数  \n定义目标函数：  \ndef real_func(x):  \n  #目标函数：sin(2*pi*x)\n    return np.sin(2 * np.pi * x)  \n定义多项式函数，用多项式去拟合数据:  \ndef fit_func(p, x):  \n    f = np.poly1d(p)  \n    return f(x)  \n定义残差函数，残差函数值为多项式拟合结果与真实值的差值：  \ndef residuals_func(p, y, x):  \n    ret = fit_func(p, x) - y  \n    return ret  \n\nx = np.linspace(0, 1, 9)  # 随机选择9个点作为x  \nx_points = np.linspace(0, 1, 1000)  # 画图时需要的连续点  \ny0 = real_func(x)  # 目标函数  \ny1 = [np.random.normal(0, 0.1) + y for y in y0]  # 在目标函数上添加符合正态分布噪声后的函数  \np_init = np.random.randn(n)  # 随机初始化多项式参数  \n# 调用scipy.optimize中的leastsq函数，通过最小化误差的平方和来寻找最佳的匹配函数\n#func 是一个残差函数，x0 是计算的初始参数值，把残差函数中除了初始化以外的参数打包到args中\nplsq = leastsq(func=residuals_func, x0=p_init, args=(y1, x))  \n\nprint('Fitting Parameters: ', plsq[0])  # 输出拟合参数  \n\npl.plot(x_points, real_func(x_points), label='real')  \npl.plot(x_points, fit_func(plsq[0], x_points), label='fitted curve')  \npl.plot(x, y1, 'bo', label='with noise')  \npl.legend()  \npl.show()"
    },
    {
        "title": "美国人口收入分析",
        "content": "import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import LabelEncoder, StandardScaler\nfrom sklearn.neighbors import KNN\nfrom sklearn.metrics import classification_report, confusion_matrix\nfrom sklearn.model_selection import GridSearchCV\n\n# 1、加载数据，查看数据行列分布情况\ndata = pd.read_csv('adult_inconn.csv')\ndata.shape\n\n# 2、查看数据特征，最大值，最小值，中位数，平均数以及四分位数\ndata.describe()\n\n# 3、查看数据缺失值分布，并用柱形图表示\nmissing_values = data.isnull().sum()\nmissing_values.plot(kind='bar')\nmissing_values[missing_values > 0].plot(kind='bar', color='skyblue')\nplt.title(\"缺失值分布\")\nplt.xlabel(\"特征\")\nplt.ylabel(\"缺失值数量\")\nplt.show()\n# 4、创建新数据集、对Predclass 标签进行转化\n# 这里假设Predclass是目标变量，我们需要将其转换为数值型\ndata['Predclass'] = data['Predclass'].map({'<=50K': 0, '>50K': 1})\n\n# 5、使用cut方法对数据进行分箱，分10个箱\n# 这里以年龄为例进行分箱\ndata['age_bins'] = pd.cut(data['age'], bins=10, right=False)\n\n# 6、绘制分箱后的数据分布图\nage_distribution = data['age_bins'].value_counts().sort_index().plot(kind='bar')\n\n# 7、属性衍生 - 这里我们以年龄和收入为例创建一个新属性\ndata['age_income'] = data['age'] * data['Predclass']\n创建新的“年龄组”或“收入水平”特征。\n# 举例，创建“年龄组”特征\ndata['age_group'] = pd.cut(data['age'], bins=[0, 30, 60, 90], labels=['青年', '中年', '老年'])\n\n# 8、查看sex-marital 分布图\nsex_marital_distribution = sns.countplot(data=data, x='sex', hue='marital_status')\n# 性别和婚姻状况分布图\nsns.countplot(data=data, x='sex', hue='marital-status')\nplt.title(\"性别与婚姻状况分布\")\nplt.show()\n# 9、数据特征值处理，属性编码\n# 对类别特征进行编码\nlabel_encoders = {}\nfor column in data.select_dtypes(include=['object']).columns:\n    le = LabelEncoder()\n    data[column] = le.fit_transform(data[column])\n    label_encoders[column] = le\n\n# 10、将df 转为str - 这一步似乎没有必要，可能是有误解\ndata = data.astype(str)\n\n# 11、引入KNN 对数据进行预测\n# 首先划分数据集\nX = data.drop('Predclass', axis=1)\ny = data['Predclass']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n# 标准化特征\nscaler = StandardScaler()\nX_train_scaled = scaler.fit_transform(X_train)\nX_test_scaled = scaler.transform(X_test)\n\n# 12、选择模型最优参数\nknn = KNN()\nparam_grid = {'n_neighbors': np.arange(1, 30)}\nknn_gscv = GridSearchCV(knn, param_grid, cv=5)\nknn_gscv.fit(X_train_scaled, y_train)\nbest_params = knn_gscv.best_params_\n\n# 13、重新生成模型knn_final\nknn_final = KNN(n_neighbors=best_params['n_neighbors'])\n\n# 14、对数据进行拟合\nknn_final.fit(X_train_scaled, y_train)\n\n# 15、重新对X_test 进行预测\ny_pred = knn_final.predict(X_test_scaled)\n\n# 16、导入分类模块，画出分布图\nconf_matrix = confusion_matrix(y_test, y_pred)\nsns.heatmap(conf_matrix, annot=True, fmt='d')\n\n# 输出相关结果\ndata_shape, data_description, missing_values, best_params, conf_matrix\n绘制预测结果分布图\nsns.countplot(x=y_pred)\nplt.title(\"预测结果分布\")\nplt.show()\n"
    },
    {
        "title": "黑色星期五",
        "content": "# 检查缺失值\nprint(data.isnull().sum())\n\n# 填充缺失值或删除缺失行（根据数据分析需求决定策略）\n# 假设我们选择填充缺失值\ndata.fillna(method='ffill', inplace=True)\n\n# 生成user_info表\nuser_info = data[['User_ID', 'Gender', 'Age', 'Occupation', 'City_Category']]\nprint(user_info.head())\n# 年龄和性别分布\nage_distribution = user_info['Age'].value_counts()\ngender_distribution = user_info['Gender'].value_counts()\n\nprint(\"年龄分布:\\n\", age_distribution)\nprint(\"性别分布:\\n\", gender_distribution)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# 按年龄和性别分组计算消费金额总和\nage_gender_purchase = data.groupby(['Age', 'Gender'])['Purchase'].sum().unstack()\n\n# 绘制条形图\nage_gender_purchase.plot(kind='bar', stacked=True, figsize=(12, 8))\nplt.title(\"消费情况按年龄和性别分布\")\nplt.xlabel(\"年龄\")\nplt.ylabel(\"总消费金额\")\nplt.legend(title=\"性别\")\nplt.show()"
    },
    {
        "title": "信用违约预测",
        "content": "1. 读取数据并查看前5行信息\ndata = pd.read_csv(\"credit-default.csv\")\nprint(data.head())\n2. 查看Target分布\nprint(data['Target'].value_counts())\ndata['Target'].value_counts().plot(kind='bar', title='Target分布')\n3. 相关系数矩阵及热力图\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n# 相关系数矩阵\ncorr_matrix = data.corr()\n\n# 绘制热力图\nplt.figure(figsize=(12, 10))\nsns.heatmap(corr_matrix, annot=True, cmap='coolwarm', fmt='.2f')\nplt.title(\"相关系数矩阵热力图\")\nplt.show()\n4. 删除相关性超过0.8的特征\n删除高相关性的特征，以减少多重共线性问题。\n# 找出相关性超过0.8的特征对并删除一个特征\nhigh_corr = corr_matrix[(corr_matrix.abs() > 0.8) & (corr_matrix.abs() < 1.0)]\ncolumns_to_drop = [column for column in high_corr.columns if any(high_corr[column])]\ndata = data.drop(columns=columns_to_drop)\n5. 统计各特征的缺失率\n# 统计缺失率\nmissing_rate = data.isnull().mean()\nprint(missing_rate)\n6. 名义型变量的缺失值处理\n针对名义型变量的缺失值，可以选择填充方式。\n# 填充缺失值 - 以mode填充名义型变量缺失值\nfor col in data.select_dtypes(include=['object']).columns:\n    data[col].fillna(data[col].mode()[0], inplace=True)\n7. 数据填充\n针对数值型变量缺失值，可以用均值或中位数填充。\n# 填充数值型变量缺失值\nfor col in data.select_dtypes(exclude=['object']).columns:\n    data[col].fillna(data[col].median(), inplace=True)\n8. 名义型变量独热编码\n对名义型变量进行独热编码，以增强表达能力。\n# 独热编码\ndata = pd.get_dummies(data, drop_first=True)\n9. 数据拆分\n将数据集拆分为训练集和测试集。\nfrom sklearn.model_selection import train_test_split\n\nX = data.drop(columns=['Target'])\ny = data['Target']\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n10. 引入随机森林算法并进行预测\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import f1_score\n\n# 初始化模型并训练\nrf_model = RandomForestClassifier(random_state=42)\nrf_model.fit(X_train, y_train)\n\n# 预测并计算f1 score\ny_pred = rf_model.predict(X_test)\nprint(\"初始模型 F1 Score:\", f1_score(y_test, y_pred))\n11. 使用SMOTE进行过采样\n使用SMOTE对数据进行过采样以平衡类别。\nfrom imblearn.over_sampling import SMOTE\n# 过采样\nsmote = SMOTE(random_state=42)\nX_train_res, y_train_res = smote.fit_resample(X_train, y_train)\n12. 重新训练随机森林模型\n# 重新训练模型\nrf_model.fit(X_train_res, y_train_res)\ny_pred_resampled = rf_model.predict(X_test)\nprint(\"SMOTE后模型 F1 Score:\", f1_score(y_test, y_pred_resampled))\n13. 模型调优\n可以使用网格搜索调优超参数。\nfrom sklearn.model_selection import GridSearchCV\n\n# 参数网格\nparam_grid = {\n    'n_estimators': [50, 100, 200],\n    'max_depth': [10, 20, 30],\n    'min_samples_split': [2, 5, 10]\n}\n\n# 网格搜索\ngrid_search = GridSearchCV(RandomForestClassifier(random_state=42), param_grid, cv=5, scoring='f1')\ngrid_search.fit(X_train_res, y_train_res)\n\n# 最优参数\nprint(\"最优参数:\", grid_search.best_params_)\n14. 输出最终模型的F1 Score\n# 使用最优参数重新训练模型\nrf_final = RandomForestClassifier(**grid_search.best_params_, random_state=42)\nrf_final.fit(X_train_res, y_train_res)\ny_pred_final = rf_final.predict(X_test)\n\n# 计算f1 score\nprint(\"最终模型 F1 Score:\", f1_score(y_test, y_pred_final))"
    },
    {
        "title": "data_cluster.csv\n 1、导入数据，统计行、列、时间日期\n 2、查看数据中的最大值，最小值，中位数，平均数以及四分位数\n 3、选择日期为20200319的数据判读空值处理并生成新数据集data_new\n 4、按用户ID浏量生成数据集data_new2\n 5、选择用户ID为5的流量信息分布绘制柱形图\n 6、选择用户ID为10的流量信息分布绘制饼状图",
        "content": "1. 导入数据，统计行、列及时间日期信息\ndata.shape\n# 假设时间日期列为'Date'，统计日期信息\ndata['Date'] = pd.to_datetime(data['Date'])\nprint(\"日期范围:\", data['Date'].min(), \"到\", data['Date'].max())\n2. 查看数据的最大值、最小值、中位数、平均数及四分位数\ndata.describe()\n3. 筛选日期为20200319的数据，处理空值并生成data_new\ndata_new = data[data['Date'] == '2020-03-19']\n\n# 空值处理，可以选择删除或填充\ndata_new = data_new.dropna()  # 或者使用data_new.fillna(0)填充\n\n# 显示空值处理后的数据集\nprint(data_new.head())\n4. 按用户ID生成data_new2数据集\n\n# 假设用户ID列为'UserID'，按UserID分组计算流量总和\ndata_new2 = data_new.groupby('UserID').sum().reset_index()\nprint(data_new2.head())\n5. 绘制用户ID为5的流量信息柱形图\nimport matplotlib.pyplot as plt\n\n# 筛选用户ID为5的数据\nuser_5_data = data_new[data_new['UserID'] == 5]\n\n# 绘制柱形图\nuser_5_data.plot(kind='bar', x='Date', y='Flow', title=\"用户ID为5的流量信息\")\nplt.xlabel(\"日期\")\nplt.ylabel(\"流量\")\nplt.show()\n6. 绘制用户ID为10的流量信息饼状图\nuser_10_data = data_new[data_new['UserID'] == 10]\n\n# 绘制饼状图\nuser_10_data.set_index('Date')['Flow'].plot(kind='pie', autopct='%1.1f%%', title=\"用户ID为10的流量分布\")\nplt.ylabel(\"\")  # 去掉y轴标签\nplt.show()"
    },
    {
        "title": "使用数据datamining01.csv\n导入数据集文件，查看除表头外的前5行\n统计每列数据的最大值，最小值，中位数，平均数以及四分位数\n计算每列缺失值所占百分比，并输出结果，输出结果包含缺失值数量，缺失值占比\n将缺失值超过70%列删除，并选取适当方法对缺失值进行填充",
        "content": "2. 统计每列数据的最大值、最小值、中位数、平均数及四分位数\n# 使用 describe 方法进行统计\nstats = data.describe().T  # 转置方便查看\nstats['median'] = data.median()\nprint(stats[['min', 'max', 'median', 'mean', '25%', '50%', '75%']])\n3. 计算每列缺失值的数量和占比\n# 计算每列缺失值数量\nmissing_counts = data.isnull().sum()\n# 计算每列缺失值占比\nmissing_percentage = (missing_counts / len(data)) * 100\n# 输出缺失值数量和占比\nmissing_info = pd.DataFrame({'缺失值数量': missing_counts, '缺失值占比': missing_percentage})\nprint(missing_info)\n4. 删除缺失值超过70%的列并对剩余缺失值进行填\n# 删除缺失值超过70%的列\ndata_cleaned = data.loc[:, missing_percentage <= 70]\n\n# 对剩余列的缺失值进行填充，使用适当的填充方法\n# 数值型列填充中位数，分类变量填充众数\nfor col in data_cleaned.columns:\n    if data_cleaned[col].dtype == 'object':  # 分类变量\n        data_cleaned[col].fillna(data_cleaned[col].mode()[0], inplace=True)\n    else:  # 数值型变量\n        data_cleaned[col].fillna(data_cleaned[col].median(), inplace=True)\n\n# 查看填充后的数据\nprint(data_cleaned.head())"
    },
    {
        "title": "使用数据train_data.csv,datamining01.csv\n导入数据查看除表头外的前5行head()\n统计每列数据中的最大值，最小值，中位数，平均数以及四分位数describe()\n计算每列中的缺失值占比，并输出结果，输出结果包含缺失值数量，缺失值占比isnull().\n删除无意义数据（unique=1列），删除无特征变化数据（值为0的列）\n将缺失值超过80%的列进行删除，\n绘制AGE 直方图",
        "content": "1. 导入数据并查看前5行\ndata = pd.read_csv(\"train_data.csv\")\ndata.head()\n2. 统计每列数据的最大值、最小值、中位数、平均数及四分位数\n# 统计每列的描述性统计信息\ndata.describe()\n3. 计算每列缺失值数量和占比\n# 计算每列的缺失值数量和缺失值占比\nmissing_counts = data.isnull().sum()\nmissing_percentage = (missing_counts / len(data)) * 100\n# 创建包含缺失值数量和占比的数据框\nmissing_info = pd.DataFrame({'缺失值数量': missing_counts, '缺失值占比': missing_percentage})\nprint(missing_info)\n4. 删除无意义数据列（唯一值为1的列）和无特征变化数据（全为0的列）\n# 删除唯一值为1的列\nunique_cols = data.columns[data.nunique() == 1]\ndata = data.drop(columns=unique_cols)\n\n# 删除所有值为0的列\nzero_cols = data.columns[(data == 0).all()]\ndata = data.drop(columns=zero_cols)\n\n# 查看删除后的数据\nprint(\"删除无意义列和无变化列后的数据:\", data.shape)\n5. 删除缺失值超过80%的列\n# 筛选出缺失值占比小于等于80%的列\ndata = data.loc[:, missing_percentage <= 80]\n\nprint(\"删除缺失值超过80%的列后的数据:\", data.shape)\n6. 绘制AGE的直方图\nimport matplotlib.pyplot as plt\nplt.hist(data['AGE'].dropna(), bins=20, color='skyblue', edgecolor='black')\nplt.title(\"AGE 直方图\")\nplt.xlabel(\"AGE\")\nplt.ylabel(\"频数\")\nplt.show()"
    },
    {
        "title": "电影数据集，推荐",
        "content": NaN
    },
    {
        "title": "数据集diabetes.csv",
        "content": "1. 导入数据并查看前5行\ndata = pd.read_csv(\"diabetes.csv\")\n\n2. 查看OutCome列的数据分布\noutcome_distribution = data['OutCome'].value_counts()\n3. 使用train_test_split对数据进行XY拆分\nfrom sklearn.model_selection import train_test_split\n# 特征和目标变量拆分\nX = data.drop('OutCome', axis=1)\ny = data['OutCome']\n\n# 使用80-20比例拆分数据集\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n4. 使用XGBoost分类算法XGBClassifier进行预测并优化参数\nfrom xgboost import XGBClassifier\nfrom sklearn.metrics import roc_auc_score\n# 定义初始模型\nmodel = XGBClassifier(max_depth=3, learning_rate=0.1, random_state=42)\n\n# 训练模型\nmodel.fit(X_train, y_train)\n\n# 预测\ny_pred = model.predict_proba(X_test)[:, 1]\n\n# 计算AUC得分\nauc_score = roc_auc_score(y_test, y_pred)\nprint(\"初始模型的AUC得分:\", auc_score)\n\n# 参数调优，提高AUC得分\nbest_auc = auc_score\nbest_params = {'max_depth': 3, 'learning_rate': 0.1}\n\n# 尝试不同的参数组合\nfor depth in range(3, 8):\n    for lr in [0.01, 0.05, 0.1, 0.2]:\n        model = XGBClassifier(max_depth=depth, learning_rate=lr, random_state=42)\n        model.fit(X_train, y_train)\n        y_pred = model.predict_proba(X_test)[:, 1]\n        auc = roc_auc_score(y_test, y_pred)\n        \n        if auc > best_auc:\n            best_auc = auc\n            best_params = {'max_depth': depth, 'learning_rate': lr}\n\nprint(\"最佳AUC得分:\", best_auc)\nprint(\"最佳参数:\", best_params)"
    },
    {
        "title": "bigdata",
        "content": "data = [50,46,33,27,55,36,73,101] \nfor i in range(len(data)): #建立for循环，循环整个data列表\n    for j in range(0,len(data)-i-1):\n        if data[j]>data[j+1]: #if语句设定条件\n            data[j],data[j+1] = data[j+1],data[j] #结果，比较两个数，比较后按照大小互换位置\nprint(data) #输出排序后的列表\n\nimport numpy as np\nimport scipy as sp\n#1、生成一个0-14向量\nx = np.arange(15)\nprint(x)\n#2、将x转换为二维矩阵，矩阵的第一维度为1\ny = x.reshape(1,15)\nprint(y)\n#3、生成3*4矩阵\nA = np.arange(12).reshape(3,4)\nprint(A)\n#4、转置3中的矩阵\nprint(A.T)\n#5、给定线性方程组求解\n#10x_1 + 8x_2 + 12x_3 = 20\n#4x_1 + 4x_2 + 2x_3 = 8\n#2x_1 - 4x_2- 2x_3 = -5\nfrom scipy.linalg import solve\na = np.array([[10, 8, 12], [4, 4, 2], [2, -4, -2]])\nb = np.array([10,8,-5])\nx = solve(a, b)\nprint(x)\n\n#计算每列缺失值所占百分比，并输出结果，输出结果包含缺失值数量，缺失值占比\ncolumns = data.columns\nfor i in columns:\n    null_count = data[i].isnull().sum()\n    qszb = null_count/10000*100\n    print(i,\"缺失值数量为:{}\".format(null_count),\"-->缺失值占比为:{}%\".format(round(qszb,2)))\n\n#将缺失值超过70%列删除，并选取适当方法对缺失值进行填充\ndata_drop = data.dropna(axis=1,thresh=len(data)*0.3)\ndata_drop\n\n#各列均值填充各列缺失值\nfor i in data_drop.columns:\n    data_drop[i] = data_drop[i].fillna(np.mean(data_drop[i]))\n\n#再次查看数据集确实情况\ncolumns = data_drop.columns\nfor i in columns:\n    null_count = data_drop[i].isnull().sum()\n    qszb = null_count/10000*100\n    print(i,\"缺失值数量为:{}\".format(null_count),\"-->缺失值占比为:{}%\".format(round(qszb,2)))\n    #缺失全部填充完毕\n\nprint(df.isnull().any()) #查看数据是否含有缺失值，false表示不含缺失值\n\nprint(df.info()) #查看数据类型，浮点型数据共22列，整型数据共2列\n\nimport matplotlib.pyplot as plt #导入画图库\nimport seaborn as sns #导入seaborn画图库\nimport warnings \nwarnings.filterwarnings('ignore') #忽略告警提示\nplt.rcParams['font.family'] = ['sans-serif'] \nplt.rcParams['font.sans-serif'] = ['SimHei']#画图时防止中文不显示\n\nimport matplotlib\nmatplotlib.rcParams['axes.unicode_minus']=False #设置负号显示\nplt.figure(figsize=(20,10))\ndf_corr = df.corr(method='pearson') #特征相关性矩阵\nsns.heatmap(df_corr,annot=True) #特征热力图\nplt.show()\n#颜色越浅表示正相关越强，颜色越深表示负相关越强\n\ndrop = [] #设置一个列表，存储筛选出的相关性高的特征对\nfor i in df_corr.index: #遍历相关系数矩阵所有的行\n    for j in df_corr.columns:#遍历相关系数矩阵所有的列\n        if df_corr.loc[i,j]>0.8 and i!=j and (j,i) not in drop: #设定if语句条件，筛选出相关性大于0.8的特征对\n            drop.append((i,j)) #将筛选出的特征对加入drop列表中\nprint(drop) #输出存储的特征对\n\n#丢弃特征对中的一个，进行特征选择\ncols_to_drop = np.unique([col[1] for col in drop]) #对特征对进行去重，筛选出需要剔除的特征，包括Var4,10,16,17,18,19,20,21,22,23列\ndf.drop(cols_to_drop,axis=1,inplace=True)\nprint(df.head())\n\nfrom sklearn.tree import DecisionTreeClassifier #决策树\nfrom sklearn.neighbors import KNeighborsClassifier #KNN\nfrom sklearn.preprocessing import StandardScaler #标准化函数\nfrom sklearn.model_selection import cross_val_score #交叉验证得分函数\nfrom sklearn.metrics import confusion_matrix #混淆矩阵\nfrom sklearn import metrics\nfrom sklearn.metrics import roc_auc_score,f1_score,accuracy_score,precision_score,recall_score #评价指标\n\n\n#三个评估函数的集合\ndef eva(x,y):\n    print('f1_score:',round(f1_score(y,x),2),'查准率:',round(precision_score(y,x),2),'查全率',round(recall_score(y,x),2))\n    return\n\n#绘制ROC曲线\ndef plot_roc_curve(y_test,preds):\n    fpr,tpr,threshold = metrics.roc_curve(y_test,preds)\n    roc_auc = metrics.auc(fpr,tpr)\n    plt.title('ROC曲线')\n    plt.plot(fpr,tpr,'b',label = 'AUC = %0.2f'%roc_auc)\n    plt.legend(loc='lower right')\n    plt.plot([0,1],[0,1],'r--')\n    plt.xlim([-0.01,1.01])\n    plt.ylim([-0.01,1.01])\n    plt.ylabel('真正例率',fontsize=16)\n    plt.xlabel('假正例率',fontsize=16)\nplt.show()\n\n\n#划分训练集和测试集\nfrom sklearn.model_selection import train_test_split\nX = df.loc[:,df.columns!='lab'] #属性特征列\nY = df['lab'] #目标列\nx_train,x_test,y_train,y_test = train_test_split(X,Y,test_size=0.3,random_state=12) #训练集/测试集=7：3，随机种子数为12\n\n\nx_train_std = StandardScaler().fit_transform(x_train) #标准化数据\nx_test_std = StandardScaler().fit_transform(x_test)\n\ndtc = DecisionTreeClassifier(criterion='gini',max_depth=8)  #建立决策树模型，特征划分准则采用gini，最大树深度为8\ndtc.fit(x_train,y_train) #模型训练\nprint(dtc.score(x_test,y_test)) #模型得分\n\nplt.rcParams['font.family'] = ['sans-serif'] \nplt.rcParams['font.sans-serif'] = ['SimHei']#画图时防止中文不显示\ntest=[] #设定一个列表，存储模型得分\nplt.figure(figsize=(15,5)) #设置画布大小\nfor i in range(13): #for循环遍历0,12\n    dtc = DecisionTreeClassifier(max_depth=i+1,criterion='gini',random_state=30) #循环不同的max_depth来建立决策树模型\n    dtc.fit(x_train,y_train) #模型训练\n    score = dtc.score(x_test,y_test) #模型得分\n    test.append(score) #存储不同max_depth建立的决策树的得分\nplt.plot(range(1,14),test,color='blue',label='max_depth') #折线图可视化\nplt.xlabel('数值设置')\nplt.ylabel('模型准确率')\nplt.legend()\nplt.title('最大树深度max_depth影响图')\nplt.show()\n#可以看出max_depth为13时模型准确率最高，下一步基于max_depth为13时建立新的决策树模型\n\nnew_dtc = DecisionTreeClassifier(max_depth=13,criterion='gini',random_state=30) #建立新的决策树模型，max_depth为13\nnew_dtc.fit(x_train,y_train) #模型训练\nprint(new_dtc.score(x_test,y_test)) #模型得分\n\ny_pre_dtc = new_dtc.predict(x_test) #决策树模型预测值\n\ny_pro_dtc = new_dtc.predict_proba(x_test)[:,1] #模型预测概率\n\nprint(confusion_matrix(y_test,y_pre_dtc))#决策树模型混淆矩阵\n\nprint(eva(y_pre_dtc,y_test)) #决策树模型评估\n\n#决策树模型ROC曲线及AUC值\nplt.figure(figsize=(20,8))\nplt.xticks(fontsize=18)\nplt.yticks(fontsize=18)\nplot_roc_curve(y_test,y_pro_dtc)\nplt.show()\n\n#运用KNN算法建模\nK = np.arange(1,14) #K值选取范围为1到13\nacc=[] #存储不同K值建立的KNN模型准确率的列表\nfor i in K:\n    #交叉验证得分，使用accuracy准确率判定，cv=10是10折交叉验证\n    cv_score = cross_val_score(KNeighborsClassifier(n_neighbors=i,weights='uniform'),x_train_std,y_train,scoring='accuracy',cv=10)\n    acc.append(cv_score.mean()) #将n_neighbors为1到13所有情况下得分的均值存储在列表acc中\nplt.figure(figsize=(15,5)) #设置画布大小\nplt.plot(K,acc) #绘制K值-acc折线图\narg_max = np.array(acc).argmax() #取平均准确率中最大值的对应的下标\nplt.text(K[arg_max], acc[arg_max], '最佳k值为{}'.format(K[arg_max])) #设置文字说明，找到最佳K值\nplt.xlabel('k值')\nplt.ylabel('模型准确率')\nplt.title('不同K值下模型的平均准确率')\nplt.show()\n\n#建立最佳K值下的KNN模型\nknn = KNeighborsClassifier(n_neighbors=3,weights='uniform') #建立KNN模型\nknn.fit(x_train_std,y_train) #模型训练\nprint(knn.score(x_test_std,y_test)) #模型准确率得分\n\ny_pre_knn = knn.predict(x_test_std)#模型预测\ny_pro_knn = knn.predict_proba(x_test_std)[:,1] #KNN模型预测概率\n\nprint(confusion_matrix(y_test,y_pre_knn))#KNN模型混淆矩阵\nprint(eva(y_pre_knn,y_test)) #KNN模型评估\n#KNN模型ROC曲线及AUC值\nplt.figure(figsize=(20,8))\nplt.xticks(fontsize=18)\nplt.yticks(fontsize=18)\nplot_roc_curve(y_test,y_pro_knn)\nplt.show()\n\n#运用SVM建模\nfrom sklearn.svm import SVC\nsvm = SVC(C=1.0) #建立SVM模型，惩罚项C为1，其余参数默认\nsvm.fit(x_train_std,y_train) #模型训练\nprint(svm.score(x_test_std,y_test)) #模型准确率得分\nfrom sklearn.model_selection import GridSearchCV #导入网格搜索算法\nparam_svm = {'C':[0.05,0.1,0.3,0.5,1,2]} #搜索参数C\ngsearch_svm = GridSearchCV(estimator=svm,param_grid=param_svm,cv=5) #构建网格搜索，5折交叉验证\ngsearch_svm.fit(x_train_std,y_train) #模型训练\nprint(gsearch_svm.best_params_ )#搜索得到的模型最佳参数\nnew_svm = SVC(C=2.0,probability=True) #建立搜索得到的最优参数C的SVM模型\nnew_svm.fit(x_train_std,y_train) #模型训练\nprint(new_svm.score(x_test_std,y_test)) #模型得分\n\ny_pre_svm = new_svm.predict(x_test_std)#SVM模型预测\ny_pro_svm = new_svm.predict_proba(x_test_std)[:,1]#SVM模型预测概率\nprint(confusion_matrix(y_test,y_pre_svm)) #SVM模型的混淆矩阵\nprint(eva(y_pre_svm,y_test)) #SVM模型评估\n#SVM模型ROC曲线及AUC值\nplt.figure(figsize=(20,8))\nplt.xticks(fontsize=18)\nplt.yticks(fontsize=18)\nplot_roc_curve(y_test,y_pro_svm)\nplt.show()\n\n#保存模型到本地\nimport joblib\n#保存KNN模型\njoblib.dump(knn,'./knn.pkl')\n\nmodel_knn = joblib.load('./knn.pkl') #加载上述保存的KNN模型\nknn_yuce = model_knn.predict(x_test_std) #使用KNN模型对标准化后的测试集x_test_std进行预测\nprint(knn_yuce) #输出预测数据\n\ndata = [] #设定一个存储预测数据的列表data\nfor i in list(knn_yuce): #遍历模型预测出的所有数据\n    data.append(i) #向列表中加入数据\n\nf = open('C:\\\\yuce.txt','w') #在桌面上写入一个保存预测数据的txt文件，名称为yuce.txt\nfor i in data: #遍历data中的数据\n    f.write(str(i)+'\\n') #向txt文本中写入所有预测的数据，稍后打开桌面中的yuce.txt文档即可看到预测出的所有数据\n\ndf.describe(include='O') #统计离散型特征信息\n\ndf.describe() #查看连续型特征信息\n\n#各特征缺失值可视化\nimport missingno as mnso\nmnso.bar(df)\n\ndf.isnull().any() #不存在缺失值\nplt.rcParams['font.family'] = ['sans-serif'] \nplt.rcParams['font.sans-serif'] = ['SimHei']#画图时防止中文不显示\nplt.subplot(2,2,1)\ndf['Churn'].value_counts().plot.pie(autopct='%.1f%%',figsize =(15,15),fontsize=20)\nplt.subplot(2,2,2)\ndf['SeniorCitizen'].value_counts().plot.pie(autopct='%.1f%%',figsize =(15,15),fontsize=20)\n#可以看出流失用户占整体用户的26.5%，大约16%的人群是老年人，84%的人群是年轻人\n\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nplt.figure(figsize=(20,8),dpi=80)\nsns.countplot(df['Churn'],hue='gender',data=df)\nplt.xticks(fontsize=18)\nplt.yticks(fontsize=18)#可视化看出客户流失中男女分布均匀，性别属性对结果并无明显决策作用\n\n#查看年龄与客户流失之间的关系，发现年轻人群体保留率占比很高,在流失用户中，老年用户占比明显比年轻用户更高\nplt.figure(figsize=(20,8),dpi=80)\nsns.countplot(df['Churn'],hue='SeniorCitizen',data=df)\nplt.xticks(fontsize=18)\nplt.yticks(fontsize=18)\n\nplt.figure(figsize=(15,10))\nplt.subplot(2,2,1)\nsns.kdeplot(df.TotalCharges[(df[\"Churn\"] == 'No')],color=\"Red\", shade =True)\nsns.kdeplot(df.TotalCharges[(df[\"Churn\"] == 'Yes')],color=\"Blue\", shade= True)\nplt.legend([\"Not Churn\",\"Churn\"],loc='upper right')\nplt.xlabel('总费用')\nplt.ylabel('密度')\nplt.title('客户人群是否流失与总费用分布') #可以发现总费用在500左右时用户流失率最高\nplt.subplot(2,2,2)\nsns.kdeplot(df.MonthlyCharges[(df[\"Churn\"] == 'No')],color=\"Red\", shade = True)\nsns.kdeplot(df.MonthlyCharges[(df[\"Churn\"] == 'Yes')],color=\"Blue\", shade = True)\nplt.legend([\"Not Churn\",\"Churn\"],loc='upper right')\nplt.xlabel('月费用')\nplt.ylabel('密度')\nplt.title('客户人群是否流失与月费用分布')#可以发现月费用较高时，客户流失率也会增高\n\n#可视化PhoneService(是否有电话服务)与客户是否流失之间的关系，看出拥有电话服务的人群占比高，但是该特征对用户是否流失并无明显决策作用\nplt.figure(figsize=(20,8),dpi=80)\nsns.countplot(df['Churn'],hue='PhoneService',data=df)\nplt.xticks(fontsize=18)\nplt.yticks(fontsize=18)\n\n#命名属性集合为list_all\nlist_all = ['InternetService','OnlineSecurity', 'OnlineBackup', 'DeviceProtection', 'TechSupport','StreamingTV']\nplt.figure(figsize=(15,15))\nfor i,m in enumerate(list_all):\n    plt.subplot(3,3,(i+1))\n    sns.countplot(x=m,hue='Churn',data=df)\n    plt.xlabel(str(m))\n    plt.title('Churn by '+str(m))\n    i+=1\nplt.show() #可以看出No internet service(无互联网服务)用户的流失率相同，说明这几个因素对于不使用互联网服务的用户是否流失不具影响\n#而在网络服务中，不使用网络服务的用户流失占比较低，对于使用DSL和Fiber optic的用户来说，使用Fiber optic的用户更容易流失\n\n#删除customerID,使用LabelEncoder编码，将数据转换为连续型数值变量\nfrom sklearn.preprocessing import LabelEncoder\ndel df['customerID']\nle = LabelEncoder()\nnew_data = df.astype(str)\nnew_data_le = new_data.apply(le.fit_transform)\n\ndf_corr = new_data_le.corr(method=\"spearman\")\nplt.figure(figsize=(20,10))\nplt.xticks(fontsize=16)\nplt.yticks(fontsize=16)\nsns.heatmap(df_corr,annot=True)\n\n#数据集划分为训练集和测试集，拆分比例为0.3\nfrom sklearn.model_selection import train_test_split\nX = new_data_le.loc[:,new_data_le.columns!='Churn']\nY = new_data_le['Churn']\nx_train,x_test,y_train,y_test = train_test_split(X,Y,test_size=0.3)\n\nfrom sklearn import metrics \nfrom sklearn.preprocessing import StandardScaler #导入标准化函数\nfrom sklearn.metrics import roc_auc_score,f1_score,accuracy_score,precision_score,recall_score #评价指标\nx_train_std = StandardScaler().fit_transform(x_train) #标准化训练集\nx_test_std = StandardScaler().fit_transform(x_test)\n\n\n#三个评估函数的集合\ndef eva(x,y):\n    print('f1_score:',f1_score(y,x),'查准率:',precision_score(y,x),'查全率',recall_score(y,x))\n    return\n\n#绘制ROC曲线\ndef plot_roc_curve(y_test,preds):\n    fpr,tpr,threshold = metrics.roc_curve(y_test,preds)\n    roc_auc = metrics.auc(fpr,tpr)\n    plt.title('Receiver Operating Characteristic')\n    plt.plot(fpr,tpr,'b',label = 'AUC = %0.2f'%roc_auc)\n    plt.legend(loc='lower right')\n    plt.plot([0,1],[0,1],'r--')\n    plt.xlim([-0.01,1.01])\n    plt.ylim([-0.01,1.01])\n    plt.ylabel('True Positive Rate',fontsize=16)\n    plt.xlabel('False Positive Rate',fontsize=16)\nplt.show()\n\n#随机森林算法建模\nfrom sklearn.ensemble import RandomForestClassifier\nrf = RandomForestClassifier()\nrf.fit(x_train_std,y_train)\nrf.score(x_test_std,y_test)\n\n#预测值y_pre_rf与预测概率y_pro_rf\ny_pre_rf = rf.predict(x_test_std)\ny_pro_rf = rf.predict_proba(x_test_std)[:,1]\neva(y_pre_rf,y_test)#模型评估\n#ROC曲线图及AUC值\nplt.figure(figsize=(20,8))\nplt.xticks(fontsize=18)\nplt.yticks(fontsize=18)\nplot_roc_curve(y_test,y_pro_rf)\n\n#最后查看各个特征重要性，可视化并按照重要性数值大小排序\n#特征重要性图\nplt.rcParams['font.family'] = ['sans-serif'] \nplt.rcParams['font.sans-serif'] = ['SimHei']#画图时防止中文不显示\nimportance = pd.DataFrame() #建立一个DataFrame\nimportance['变量'] = x_train.columns\n#查看属性重要性\nimportance['重要性程度'] = rf.feature_importances_ #得到随机森林对特征的评价重要度\n#根据数值大小排序\nimportance = importance.sort_values(by = '重要性程度',ascending=False)\nplt.figure(figsize=(16,10))\nplt.yticks(fontsize=14)\nplt.xticks(fontsize=16,rotation=90)\nplt.ylabel('重要性程度',fontsize=14)\nplt.bar(importance['变量'].values.tolist()[:15],importance['重要性程度'].values.tolist()[:15])\n\ndf['class'].value_counts() #查看蘑菇类别分布\ndf.isnull().any() #查看数据是否有缺失值\n#查看蘑菇类别的占比(毒蘑菇：p，正常蘑菇：e)\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nplt.figure(figsize=(9,9))\nplt.rcParams['font.family'] = ['sans-serif'] \nplt.rcParams['font.sans-serif'] = ['SimHei']#画图时防止中文不显示\ndf['class'].value_counts().plot.pie(explode=[0,0.05],autopct='%.2f%%',textprops={'fontsize':20})\nplt.title('蘑菇类别占比',fontsize=20)\n\n#菌盖颜色进行直方图可视化，查看不同颜色菌盖的数量\nplt.rcParams['font.family'] = ['sans-serif'] \nplt.rcParams['font.sans-serif'] = ['SimHei']#画图时防止中文不显示\nplt.figure(figsize=(20,10))\n#设置条形图颜色\ndf['cap_color'].value_counts().plot.barh()\nplt.xlabel('数量',fontsize=20)\nplt.ylabel('菌盖颜色种类',fontsize=20)\n#可以看出在蘑菇届，棕、灰、红、黄、白的蘑菇占大多数\n\nplt.figure(figsize=(20,10))\nplt.rcParams['font.family'] = ['sans-serif'] \nplt.rcParams['font.sans-serif'] = ['SimHei']#画图时防止中文不显示\nsns.countplot(df['cap_color'],hue='class',data=df)\nplt.xlabel('菌盖颜色种类',fontsize=20)\nplt.ylabel('数量',fontsize=20)\n#(毒蘑菇：p，正常蘑菇：e)\n#可视化可以看出鲜艳的蘑菇有毒的可能性还是较高的，比如红色和棕色\n#但其他颜色的蘑菇并非就是完全可食用，棕色和灰色的蘑菇都是很常见的\n\n"
    }
]
    results = []
    for dict_item in json_data:
        results.append({'title': dict_item['title']})
    df = pd.DataFrame(results)
    df.style.set_properties(**{'white-space': 'pre-wrap', 'text-align': 'left'})

def d_by_t(key):
    json_data=[
    {
        "title": "关联规则1fruit",
        "content": "df = pd.read_csv('./data/fruit.csv',sep=';')\n#1.以“;”为分割符读取数据中的ID和fruit列，将fruit列通过lambda函数，以逗号为分割符进行切分并去除数据集中的空格，生成矩阵 df_mtix\ndf_mtix = df['fruit'].map(lambda x:x.replace(\" \",'').split(','))\n#2.使用TransactionEncoder函数对矩阵进行拟合df_mtix,生成数据df_te。将数据df_te转换成DataFrame,使用df_te.columns为列名\nfrom mlxtend.preprocessing import TransactionEncoder\nte = TransactionEncoder()\ndf_te = te.fit_transform(df_mtix)\ndf_te = pd.DataFrame(data=df_te,columns=te.columns_)\n#3.使用fpgrowth算法建模发现数据中的关联规则，指定min_support=0.5,use_colname=True,使用mlxtend库中的assocication_rules 函数来找出关联规则。指定metric为'confidence',min_threshold为0.3\nfrom mlxtend.frequent_patterns import fpgrowth\nfrom mlxtend.frequent_patterns import association_rules\nmodel = fpgrowth(df_te,min_support=0.5,use_colnames=True)\nrules = association_rules(model,metric='confidence',min_threshold=0.3)\n#4.输出关联规则rules中，前验置信度>0.6,置信度大于0.5的项集\nrules[(rules['antecedent support']>0.6) & (rules['support']>0.5)]\n#5.输出关联规则rules中，antecedents为{苹果,香蕉}的内容\nrules[rules['antecedents'] == {'apple','banana'}]"
    },
    {
        "title": "关联规则2transactions",
        "content": "df = pd.read_csv('./data/transactions.csv')\n#1.对df中的items列应用lambda函数，将每行中的值按照逗号分割的字符串拆分成列表，并去掉空格，将结果存储在transactions列表中\ntransactions = []\ntransactions = df['items'].map(lambda x:x.replace(\" \",'').split(','))\n#2.使用TransctionEncoder将数transactions转换为合适的Apriori算法的格式数据te_data,使用Pandas的DataFrame()函数将te_data转换为DataFrame对象df\nfrom mlxtend.preprocessing import TransactionEncoder\nte = TransactionEncoder()\nte_data = te.fit_transform(transactions)\ndf = pd.DataFrame(te_data,columns=te.columns_)\nfrom mlxtend.frequent_patterns import fpgrowth\nfrequent_items = fpgrowth(df,min_support=0.3,use_colnames=True)\n#3在频繁项集frequent_items中构建关联规则rules,度量指标为confidence,最小阀值为0.5\nfrom mlxtend.frequent_patterns import association_rules\nrules = association_rules(frequent_items,metric='confidence',min_threshold=0.5)\n#4.打印同时满足先验支持度>=0.6,支持度>0.3,提升度>0.8的rules数据\nrules[(rules['antecedent support']>=0.6) & (rules['support']>=0.3) & (rules['lift']>=0.8)]\n#5.筛选规则rules,将规则中同时满足antecedents_sale为Banana,consequents_sale为Apple规则选出来，并存储在final_sale中，使得rules.loc[fina_sale]输出\nantecedents_sale = rules['antecedents'] == frozenset({'Banana'})  #返回true和false的集合\nconsequents_sale = rules['consequents'] == frozenset({'Apple'})\nfinal_sale = (antecedents_sale&consequents_sale)\n#考试的时候要注意这里，考题有可能是删除，也有可能是筛选\n# as_rule.loc[~final_sale]  #加了波浪符号或者负号表示筛选出相反的数据，也就是选择不同时含有上述条件的选项 ，即删除以上条件的数据\nrules.loc[final_sale]  #不加了波浪符号或者负号表示筛选出以上条件的数据   "
    },
    {
        "title": "关联规则3GoodsOrder_eng.csv",
        "content": "pip install mlxtend\nimport numpy as np \nimport pandas as pd\nfrom mlxtend.frequent_patterns import apriori,fpgrowth,association_rules\nimport warnings \nwarnings.filterwarnings('ignore')\norder_data = pd.read_csv('./data/GoodsOrder_eng.csv',sep=',',header=0,encoding='gbk')\n#转换数据格式\norder_data['Goods'] = order_data['Goods'].apply(lambda x : '' + x)\n#将order_data按id分组求和，并重置索引\ndata = order_data.groupby(by='id',as_index=False)['Goods'].sum().reset_index()\ndata['Goods'] = data['Goods'].apply(lambda x:[x[1:]])\ndata_list = list(data['Goods'])\n#2.分割商品名为每一个元素。使得dataset最终输出如以下格式：\ndata_trans = []\nfor i in data_list:\n    p = i[0].split(',')\n    data_trans.append(p)\ndata_set = data_trans\ncolumn_list = []\nfor var in data_set:\n    column_list = set(column_list)|set(var)\n#3.遍历data_set中的每一个商品，并将data中对应位置的值加1，即购买一次则在相应物品上加1，使得data输出为以下形式：\ndata = pd.DataFrame(np.zeros((len(data_set),6)),columns=list(column_list))\nfor i in range(len(data_set)):\n    for j in data_set[i]:\n        data[j][i]+=1\ndata = data.applymap(lambda x:1 if x > 0 else 0)\n#4.使用Apriori算法从数据中计算频繁项集，并将最小支持度设置为0.02.然后根据支持度倒排序，最后返回频繁项集frequent_itemts,使得frequent_items部分输出为：\nfrom mlxtend.frequent_patterns import apriori\nfrom mlxtend.frequent_patterns import fpgrowth\nfrequent_itemts = apriori(data,min_support=0.02,use_colnames=True)\nfrequent_itemts = frequent_itemts.sort_values(by='support',ascending=False)\nfrequent_itemts\n#5.使用association_rules从频繁项集frequent_items中构建关联规则，metric为'confidence',min_threshold=0.35。对关联规则按confidence值进行降序排列，并将排序结果存储在association_rule中，使得df_association_rule部分输出为:\nfrom mlxtend.preprocessing import TransactionEncoder\nfrom mlxtend.frequent_patterns import association_rules\nassociation_rule = association_rules(frequent_itemts,metric='confidence',min_threshold=0.35)\nassociation_rule.sort_values(by='confidence',ascending=False,inplace=True)\nassociation_rule\n"
    },
    {
        "title": "pyspark2,creditcard",
        "content": "from pyspark.context import SparkContext\nfrom pyspark.sql import SparkSession\nsc = SparkContext('local','test')\nfrom pyspark.sql.functions import explode\nfrom pyspark.sql.functions import split\nspark = SparkSession(sc)\nsc\ndata_O = spark.read.load('./data/creditcard.csv',format='csv',header='true',inferSchema='true')\ndata_O.show(1)\n#1.data_O按Class列分组统计每个类别的个数，输出为classFreq,打印classFreq输出为\nclassFreq = data_O.groupBy('Class').count()\nclassFreq.show()\ndata = data_O.toPandas()\ndata = data.sample(frac=1)\n#欺诈样本492条\nfraud_df = data.loc[data['Class']==1]\nfraud_df   #492*31\nnon_fraud_df = data.loc[data['Class']==0][:492]\nnormal_distributed_df = pd.concat([fraud_df,non_fraud_df])\nnormal_distributed_df\nnew_df = normal_distributed_df.sample(frac=1,random_state=42)\nnew_df.shape\nimport seaborn as sns\nimport matplotlib.pyplot as plt\ncolors = ['#BF9C5','#f9c5b3']\n#2.以Class为X轴，V10为Y轴，绘制出箱线图，用于查看new_df数据集中V10变量在不同班级之间的分布情况。输出如下：\nV10_sns = sns.boxplot(x='Class',y='V10',data=new_df)\nV10_sns.set_title('V10 vs Class Negative Correlation')\nplt.show()\ndfff = spark.createDataFrame(new_df)\nfrom pyspark.sql.functions import *\nfrom pyspark.sql.window import Window\n#3.运用withColumn将dfff添加一列idx,该列是按照窗口win中的Time字段排序后的行号row_number.\nwin = Window().orderBy('Time')\ndfff = dfff.withColumn('idx',row_number().over(win))\ndfff.show(1)\nfrom pyspark.ml import Pipeline\nfrom pyspark.ml.classification import GBTClassifier\nfrom pyspark.ml.feature import VectorIndexer,VectorAssembler\nfrom pyspark.ml.evaluation import BinaryClassificationEvaluator\nfrom pyspark.ml.linalg import DenseVector\n#4.将dfff中的rdd映射为元组，元组中只有一个元素DenseVector,DenseVector的第一个值由dfff的前30列组成，第二个值为dfff的第31列，第三个值为dfff的第32列\ntraining_df = dfff.rdd.map(lambda x:(DenseVector(x[0:29]),x[30],x[31]))\ntraining_df = spark.createDataFrame(training_df,[\"features\",\"label\",\"index\"])\ntraining_df.show(1)\ntraining_df = training_df.select('index','features','label')\ntraining_df.show(1)\ntrain_data,test_data = training_df.randomSplit([.8,.2],seed=1234)\ngbt = GBTClassifier(featuresCol='features',maxIter=100,maxDepth=8)\nmodel = gbt.fit(train_data)\npredictions = model.transform(test_data)\nevaluator = BinaryClassificationEvaluator()\nevaluator.evaluate(predictions)\n混淆矩阵： P(正例/正元组)：感兴趣的类别 N(负例/负元组)：不感兴趣的类别 TP真正例：原本是正例也被预测为正例 TN真负例：原本为负例也被预测为负例 FP假正例：原本为负例被预测为正例 FN假负例：原本为正例被预测为负例 分类的偏差考察角度： 准确率(精度)：(TP+TN)/P+N 错误率(汉明损失)：(FN+FP)/P+N 查准率：TP/(TP+FP),主要关注对负例的预测是否精准 查全率：TP/(TP+FN),主要关注对正例的预测是否精准，也叫召回率 F1值(常用)：F1值是查准率和查全率的一个调和平均值。其综合考虑了查准率和查全率\ntp = predictions[(predictions.label == 1) & (predictions.prediction == 1)].count()\ntn = predictions[(predictions.label == 0) & (predictions.prediction == 0)].count()\nfp = predictions[(predictions.label == 0) & (predictions.prediction == 1)].count()\nfn = predictions[(predictions.label == 1) & (predictions.prediction == 0)].count()\n#5.打印召回率和精确率\nprint(\"Recall:\",tp/(tp+fn))\nprint(\"Precision:\",tp/(tp+fp))"
    },
    {
        "title": "pyspark3,houses_data",
        "content": "from pyspark.context import SparkContext\nfrom pyspark.sql import SparkSession\nsc = SparkContext('local','test')\nspark = SparkSession(sc)\nsc\nfrom pyspark.sql.types import *\nfrom pyspark.sql import Row\nhouses_data = spark.read.format('org.apache.spark.sql.execution.datasources.csv.CSVFileFormat')\\\n.option('header','true')\\\n.option('inferSchema','true')\\\n.load('./data/houses_data.csv')\nrdd = sc.textFile('./data/houses_data.csv')\nhouses_data.show(5)\nprint(rdd.take(5))\n#1.对数据集的每一行用逗号进行分割\nrdd = rdd.map(lambda x:x.split(','))\nrdd.take(2)\nheader = rdd.first()\nheader\n#2.使用‘filter’删除包含标题的行\nrdd = rdd.filter(lambda x:x!=header)\nrdd.take(2)\ndf = rdd.map(lambda line:Row(street=line[0],city=line[1],zip=line[2],beds=line[4],baths=line[5],sqft=line[6],price=line[9])).toDF()\ndf.show(5)\ndf.toPandas().head()\n#3.df按照'beds'字段分组，并计算每个分组中记录的数量，并显示结果\ndf.groupBy('beds').count().show()\ndf.describe(['baths','beds','price','sqft']).show()\nimport pyspark.mllib\nimport pyspark.mllib.regression\nfrom pyspark.mllib.regression import LabeledPoint\nfrom pyspark.sql.functions import  *\ndf = df.select('price','baths','beds','sqft')\ndf = df[df.baths>0]\ndf = df[df.beds>0]\ndf = df[df.sqft>0]\ndf.show(5)\ndf.describe(['baths','beds','price','sqft']).show()\ntemp = df.rdd.map(lambda line:LabeledPoint(line[0],[line[1:]]))\ntemp.take(5)\nfrom pyspark.mllib.util import MLUtils\nfrom pyspark.mllib.linalg import Vectors\nfrom pyspark.mllib.feature import StandardScaler\nfeatures = df.rdd.map(lambda row:row[1:])\nfeatures.take(5)\nstandardizer = StandardScaler()\nmodel = standardizer.fit(features)\nfeature_transform = model.transform(features)\nfeature_transform.take(5)\nlab = df.rdd.map(lambda row:row[0])\nlab.take(5)\n#4.将标签lab和特征feature_transform进行zip操作\ntransformedData = lab.zip(feature_transform)  \ntransformedData.take(5)\n#5.将transformedData转换为LabeledPoint类型，其中row[0]作为标签，row[1]作为特征向量\ntransformedData = transformedData.map(lambda row:LabeledPoint(row[0],row[1]))\ntransformedData.take(5)\ntrainingData,testingData = transformedData.randomSplit([.8,.2],seed=1234)\nfrom pyspark.mllib.regression import LinearRegressionWithSGD\nlinearModel = LinearRegressionWithSGD.train(trainingData,1000,.2)\nlinearModel.weights\ntestingData.take(10)\nlinearModel.predict([1.49297445326,3.52055958053,1.73535287287])"
    },
    {
        "title": "pyspark1,iris",
        "content": "import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom sklearn.datasets import load_iris\nfrom pyspark.conf import SparkConf\nfrom pyspark.ml.feature import VectorAssembler,StandardScaler,PCA\nfrom pyspark.context import SparkContext\nfrom pyspark.sql import SparkSession\nsc = SparkContext('local','test')\nspark = SparkSession(sc)\nsc\niris = load_iris()\nX = iris['data']\ny = iris['target']\ndata = pd.DataFrame(X,columns=iris.feature_names)\n#1.将Pandas数据框转换为Spark数据框dataset,并将其列名设置为iris特征名称，使得dataset输出为：\ndataset = spark.createDataFrame(data)\ndataset.show(6)\n#2.使用VectorAssembler将dataset多列数据转化为单列的向量‘features’,使得df输出为：\ncol = ['sepal length (cm)','sepal width (cm)','petal length (cm)','petal width (cm)']\nmodel_ve = VectorAssembler(inputCols=col, outputCol=\"features\")\ndf = model_ve.transform(dataset).select('features')\ndf.show(6)\n#3.创建一个StandarScaler对象，命名为scaler,用来对特征列‘features’进行标准化，将标准化后的结果输出到新的特征列'scaledFeatures',withMean和withStd分别为True,最后在df上进行fit操作\nscaler = StandardScaler(inputCol='features',outputCol='scaledFeatures',withMean=True,withStd=True).fit(df)\ndf_scaled = scaler.transform(df)\nn_components = 3\npca = PCA(k=n_components,inputCol='scaledFeatures',outputCol='pcaFeatures').fit(df_scaled)\ndf_pca = pca.transform(df_scaled)\nprint('Expained Variance Ratio',pca.explainedVariance.toArray())\ndf_pca.show(6)\ndf_pca.rdd.collect()[0:2]\nlist1 = df_pca.rdd.map(lambda x:(x[2][0],x[2][1],x[2][2])).collect()\ntype(list1[1][1])\ndf_origin = spark.createDataFrame([(float(tup[0]),float(tup[1]),float(tup[2])) for tup in list1],['x','y','z'])\ndf_origin.show(5)\n#4.将Spark数据框df_origin转换为Pandas数据框df_Pandas\ndf_Pandas = df_origin.toPandas()\ndf_Pandas.head()\ndf_Pandas['id'] = 'row' + df_Pandas.index.astype(str)\ncols = list(df_Pandas)\ncols\ncols.insert(0,cols.pop(cols.index('id')))\ndf_Pandas = df_Pandas.loc[:,cols]\ndf_Pandas.head(5)\npath = './data/input.csv'\n#5.将df_Pandas的数据保存到指定的path下，不保存index\ndf_Pandas.to_csv(path,index=False)\ndf = spark.read.csv(path,header=True)\ndf.show(5)"
    },
    {
        "title": "推荐1，BX-Book-Ratings",
        "content": "user_artists = pd.read_csv('./data/book/BX-Book-Ratings.csv',sep=';',encoding='latin-1',usecols=['User-ID','ISBN','Book-Rating'])\n#1.构建评分矩阵rating_matric,将缺失值填充为0\nrating_matric = user_artists.pivot_table(index='User-ID',columns='ISBN',values='Book-Rating')\nrating_matric.fillna(0,inplace=True)\nrating_matric.head(5)\n#2.使用cosine_similarity函数计算项目相似度矩阵\n#3.用户评分过的物品中获取与item相似的物品\nfrom sklearn.metrics.pairwise import cosine_similarity\nitem_similarity_matric = cosine_similarity(rating_matric.T)\n#将rating_matric 的索引转换为字符串\nrating_matric.index = rating_matric.index.map(str)\n#定义预测用户对书籍的评分函数\ndef predict_rating(user,item):\n    # 获取item的索引\n    item_index = rating_matric.columns.get_loc(item)\n    #获取item的相似度分数\n    similarity_scores = item_similarity_matric[item_index]\n    #获取用户的评分\n    user_ratings = rating_matric.loc[user]\n    #获取用户评分过的的物品\n    rated_items = user_ratings[user_ratings > 0].index\n    #在用户评分过的物品中获取与item相似的物品   ,由考生填写 .这里有点难理解\n    similar_items = [item for item in rated_items if item_similarity_matric[item_index][rating_matric.columns.get_loc(item)]>0]\n    similar_items_index = []\n    for i in range(len(similar_items)):\n        similar_items_index.append(rating_matric.columns.get_loc(similar_items[i]))\n        #如果没有相似的物品，则返回0\n    if len(similar_items) == 0:\n        return 0\n    #否则，计算用户对item的预测评分\n    else:\n        #a = np.array([1,2]),b = np.array([3,4])要求元素个数相同，相当于求内积，对应元素相乘再相加，“1*3 + 2*4 = 11”\n        return user_ratings[similar_items].dot(similarity_scores[similar_items_index])/similarity_scores[similar_items_index].sum()\n        #上面其实是求均值，也可以这么写\n        # return user_ratings[similar_items].mean()\n# 测试，计算用户276729对书籍0064401367的预测评分\nuser = '276729'\nitem = '0064401367'\npredict_rating = predict_rating(user,item)\nprint('预测用户{}对书籍{}的评分为:{}'.format(user,item,predict_rating))"
    },
    {
        "title": "推荐2，artists.dat",
        "content": "user_artists = pd.read_csv('./data/music/user_artists.dat',sep='\\t') #优用'\\t'分隔符读取文件\nartists = pd.read_csv('./data/music/artists.dat',sep='\\t',usecols=['id','name']) #仅读取id,name列\n#1.将user_artists和artist两个dataFrame进行合并，以user_artists中的artistID和artists中的id为键值进行连接\ndata = pd.merge(user_artists,artists,left_on='artistID',right_on='id')\ndata = data.drop(['id','artistID'],axis=1)    \ndata\n#2.去除评分次数少于50次的音乐和用户\ncounts = data['userID'].value_counts()   #计算每个用户的评分次数\ndata = data[data['userID'].isin(counts[counts >=50].index)]  #去除评分次数少于50次的用户\ndata\ncounts = data['name'].value_counts()   #计算每个音乐的评分次数\ndata = data[data['name'].isin(counts[counts >=50].index)]   #去除评分次数少于50次的音乐\ndata\n#将音乐和用户ID转换为数字\nname_to_index = {} #创建name_to_index字典\nindex_to_name = {} #创建index_to_name字典\nfor i,name in enumerate(data['name'].unique()):  #遍历data中的name列\n    name_to_index[name] = i   #将name映射到数字\n    index_to_name[i] = name   #将数字映射到name\n\nuser_id_to_index = {} #创建user_id_to_index字典\nindex_to_user_id = {} #创建index_to_user_id字典\nfor i,user_id in enumerate(data['userID'].unique()):  #遍历data中的name列\n    user_id_to_index[user_id] = i   #将user_id映射到数字\n    index_to_user_id[i] = user_id   #将数字映射到user_id\ndata['name_index'] = data['name'].apply(lambda x:name_to_index[x]) #将data中的name映射到数字\ndata['user_index'] = data['userID'].apply(lambda x:user_id_to_index[x]) #将data中的user_id映射到数字\n\n# 构建用户-音乐评分矩阵\nn_users = len(user_id_to_index)  #获取用户数量\nn_artists = len(name_to_index)   #获取音乐数量\nratings_matrix = np.zeros((n_users,n_artists))  #创建用户-音乐评分矩阵\n#3.将data中的评分填入用户-音乐评分矩阵，使得ratingsmatrix的输出为：\nfor row in data.itertuples(): #遍历data\n    ratings_matrix[row[5],row[4]] = row[1]  #将data中的评分填入用户-音乐评分矩阵\n#计算用户之间的相似度\nfrom sklearn.metrics.pairwise import cosine_similarity   #导入sklearn中的余弦相似度函数\nuser_simlarity = cosine_similarity(ratings_matrix)  #计算用户之间的相似度\nuser_simlarity\nuser_index = 0 #设置用户索引\n#4.获取与用户最相似的用户索引similar_users(不包括自己),使得similar_users的输出为：\n#获取与用户索引为0的用户最相似的用户的索引\n#index_u = np.argsort(-user_similarity[user_index])   #np.argsort：排序，但是返回的是数据的下标\n# 方式1\n# similar_users = np.argsort(-user_simlarity[user_index])[np.argsort(-user_simlarity[user_index]) != 0]\n# 方式2\nsimilar_users = np.argsort(-user_simlarity[user_index])[1:] #从1开始就是把0去掉，0是自己\n\nsimilar_users\nrecommended_artists = []   #创建推荐音乐列表\n#5.填补以下程序，获取相似用户评分的音乐索引artists_rated\nfor i in similar_users: #遍历相似用户索引\n#     np.where(rating_matrix[i]>0)返回的是一个元组，第一个值为下标索引的列表，第二个无值\n    artists_rated = np.where(ratings_matrix[i] > 0)[0] #获取相似用户评分的音乐,要选出相似用户听过的，也就是评分>0\n    for j in artists_rated:\n        if ratings_matrix[user_index][j] == 0: #用户未评分\n            recommended_artists.append((j,ratings_matrix[i][j])) #将音乐加入推荐音乐列表  (j,ratings_matrix[i][j]):(音乐,评分)\nrecommended_artists[0:10]\nrecommended_artists = sorted(recommended_artists,key=lambda x:x[1],reverse=True)[:10] #筛选出推荐音乐中的前10首\nartists['name'].values\nfor artist in recommended_artists: #遍历推荐音乐\n#     以下两种方式都行\n    print(artists[artists['name'] == index_to_name[artist[0]]]['name'].values[0])  #打印推荐音乐 values的结果为一个数组，但数组里面只有一个值\n    print(index_to_name[artist[0]])"
    },
    {
        "title": "分类1，pima-indians-diabetes.data",
        "content": "df = pd.read_csv('./data/pima-indians-diabetes.data',sep=',')\ndf['Outcome'].value_counts() #将数据集按照输入特征和目标特征进行划分，分别命名为X,y，且保证其类型为numpy.ndarry\nx_cols = [col for col in df.columns if col!='Outcome']\ny_col = 'Outcome'\nX = df[x_cols].values #将dataframe转化为ndarry，才能进入下面的标准化\ny = df[y_col].values\n#1.划分数据集X,y,分别命名为X_train,X_test,y_train,y_test,测试集比例为10%，固定随机数种子为42，打乱顺序，并以df[y_col]做分层抽样\n#横向拆分数据\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import  StandardScaler\nfrom collections import Counter\nx_train, x_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=42)\n#查看划分后的训练集和测试集中两类目标值的数量\nprint('Distribution of y train {}'.format(Counter(y_train)))\nprint('Distribution of y test {}'.format(Counter(y_test)))\n#标准化\nstd_scaler = StandardScaler().fit(X)\nx_train = std_scaler.fit_transform(x_train)\nx_test = std_scaler.fit_transform(x_test)\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.svm import  SVC\nfrom sklearn.linear_model import  LogisticRegression\nfrom sklearn.tree import  DecisionTreeClassifier\nfrom sklearn.naive_bayes import  GaussianNB\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.ensemble import GradientBoostingClassifier\nmodels = []\nmodels.append(('KNN',KNeighborsClassifier()))\nmodels.append(('SVC',SVC()))\nmodels.append(('LR',LogisticRegression()))\nmodels.append(('DT',DecisionTreeClassifier()))\nmodels.append(('GNB',GaussianNB()))\nmodels.append(('RF',RandomForestClassifier()))\nmodels.append(('GB',GradientBoostingClassifier()))\nnames = []\nscores = []\n#2.遍历models中的每个模型，分别使用训练集(xtrain,ytrain)进行训练，然后用测试集xtest进行预测，计算准确率accuracy score,并将模型名称name和准确率accuracy score分别存储在列表names和scores中，最后将列表names和scores转换成DataFrame格式，DataFrame命名为tr split,tr split打印输出结果如下：\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import make_scorer\nfrom sklearn.model_selection import cross_val_score\nfor name,model in models:\n    model.fit(x_train,y_train)\n    score = accuracy_score(y_test,model.predict(x_test))\n    names.append(name)\n    scores.append(score)\n# 方式1:\n# tr_split = pd.DataFrame(data={'name':names,'score':scores})\n# 方式2:\ntr_split = pd.DataFrame(data=list(zip(names,scores)),columns=['name','score'])\ntr_split\nfrom sklearn.model_selection import GridSearchCV\nc_values = list(np.arange(1,10))\n#3.使用GridSearchCV对GradientBoostingClassifier进行网格搜索，参数为param_prid,使用5折交叉验证，评分标准为'accuracy'.使用训练集(xtrain,ytrain)训练网格搜索模型，打印出最佳参数和最佳模型estimator.并采样最佳estimator创建模型new_model\nrom sklearn.ensemble import GradientBoostingClassifier\nGradientBoostingClassifier()\nparam_grid = {'n_estimators':[5,50,250,500],'max_depth':[1,3,5,7,9],'learning_rate':[0.01,0.1,1,10,100]}\ngrid = GridSearchCV(estimator=GradientBoostingClassifier(),param_grid=param_grid)  #默认为5折\ngrid.fit(x_train,y_train)\nprint(grid.best_params_) #打印最佳参数\nprint(grid.best_estimator_) #打印最佳estimator\nnew_model = grid.best_estimator_\nnew_model.fit(x_train,y_train)\nprint(accuracy_score(y_test,new_model.predict(x_test)))\n#4.初始化StackingCVClassfier,要求至少定义3个基础分类器，LogisticRegression作为元分类器，使用3折交叉验证，要求StackingCVClassfier的得分accuracy score至少大于0.79。accuracy score打印输出如下：\nfrom mlxtend.classifier import StackingCVClassifier\nsclf = StackingCVClassifier(classifiers=[LogisticRegression(),DecisionTreeClassifier(),SVC()],meta_classifier=LogisticRegression(),cv=3,use_features_in_secondary=True)\nsclf.fit(x_train,y_train)\nprint('accuracy score: {}'.format(accuracy_score(y_test,sclf.predict(x_test))))"
    },
    {
        "title": "分类2，Chicago_Crimes.csv",
        "content": "%matplotlib inline\n#读取数据\ndf = pd.read_csv('./第一套题数据/data/Chicago_Crimes.csv',error_bad_lines=False)\n#随机抽样，抽取1%\ndf_sample = df.sample(frac=0.01)\ndel df_sample['IUCR']\ndel df_sample['Case Number']\ndel df_sample['ID']\ndel df_sample['FBI Code']\ndel df_sample['Updated On']\ndel df_sample['Arrest']\ndel df_sample['Domestic']\ndel df_sample['Unnamed: 0']\ndel df_sample['Latitude']\ndf_sample.info()\ndf_sample.isnull().sum()\ndf_sample.describe(include='O')\ndf_na = pd.DataFrame(data=df_sample.isnull().sum()/df.shape[0],columns=['miss_rate']).sort_values(by='miss_rate',ascending=False)\ndf_sample.dropna(inplace=True)\ndf_sample.isnull().sum()\n#1.先将字段Date转为datetime类型，再扩展字段，提取年、月、周、日、小时信息。同时删除Date字段\ndf_sample['Date'] = df_sample['Date'].astype(np.datetime64)\ndf_sample['year'] = df_sample['Date'].dt.r\ndf_sample['month'] = df_sample['Date'].dt.month\ndf_sample['week'] = df_sample['Date'].dt.weekday   \ndf_sample['day'] = df_sample['Date'].dt.day\ndf_sample['hour'] = df_sample['Date'].dt.hour\n\n#删除Date列\ndel df_sample['Date']\n#2.字符串类型字段'Block','Primary Type','Description','Location Description','Location',在进行数据分析之前需要数值化，提高运行效率。factorize 函数可以将字符串类型数据映射为一组数字，相同的字符串类型映射为想通的数字。\nlist_col = ['Block','Description','Location Description','Location']\nfor col in list_col:\n    df_sample[col]= pd.factorize(df_sample[col])[0]\n#由考生填写\n#如果考题中要求把'Primary Type列名改成'Primary_Type,那么可以拿出来单独处理\ndf_sample['Primary_Type'] = pd.factorize(df_sample['Primary Type'])[0]\ndel df_sample['Primary Type']\nfrom sklearn.preprocessing import MinMaxScaler\ndf_sample['X Coordinate'] = df_sample['X Coordinate'].astype(float)\ndf_sample['Y Coordinate'] = df_sample['Y Coordinate'].astype(float)\ndf_sample['X Coordinate'] = MinMaxScaler().fit_transform(df_sample['X Coordinate'].values.reshape(-1,1))\ndf_sample['Y Coordinate'] = MinMaxScaler().fit_transform(df_sample['Y Coordinate'].values.reshape(-1,1))\n\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(df_sample.loc[:,df_sample.columns!='Primary_Type'], df_sample['Primary_Type'], test_size=0.3, random_state=42)\n#3.使用GradientBoostingClassifier分类器进行训练模型model_gbdt\nfrom sklearn.ensemble import GradientBoostingClassifier\nfrom sklearn.metrics import f1_score\n#由考生填写\nmodel_gbdt = GradientBoostingClassifier(n_estimators=8)  #设置参数， 如果题目没指定就用默认的，不用填\nmodel_gbdt.fit(X=X_train,y=y_train)\ny_prel = model_gbdt.predict(X_test)\nf1_score1 = f1_score(y_test,y_prel,average='micro')\nprint('f1_score为{}'.format(f1_score1))\n#4.使用网格搜索交叉验证对模型model_gbdt进行优化，调整参数learn_rate建议值为[0.1,0.2,0.3,0.4,0.5],cv采用5折进行模型训练，得到最优模型、最优参数和最优评分\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.metrics import make_scorer\n\nparam_grids = {'learning_rate':[0.1,0.2,0.3,0.4,0.5]}\nmodel_gs = GridSearchCV(estimator=model_gbdt,param_grid=param_grids,cv=5,scoring=make_scorer(f1_score))\nmodel_gs.fit(X=X_train,y=y_train)\n# 最优模型\nmodel_gs.best_estimator_\n# #最优参数\nmodel_gs.best_params_\n# # 最优评分\nmodel_gs.best_score_\n#5.使用VotingClassifier聚合了多个基础模型的预测结果。通过硬投票，软投票和自定义权重的软投票三种方式进行比较，确定最后的结果\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.ensemble import VotingClassifier\nfrom sklearn.ensemble import AdaBoostClassifier\nfrom xgboost import XGBClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nimport warnings\nwarnings.filterwarnings('ignore')\n\nclf1 = XGBClassifier(learning_rate=0.1,n_estimators=150,max_depth=3,min_child_weight=2,subsample=0.7,colsample_bytree=0.6,objective='binary:logistic')\nclf2 = RandomForestClassifier(n_estimators=50,max_depth=1,min_samples_split=4,min_samples_leaf=63,oob_score=True)\nclf3 = SVC(C=0.1,probability=True)  #软投票的时候,probability必须指定且为True\n\nclf = VotingClassifier(estimators=[('xgb',clf1),('rf',clf2),('svc',clf3)],voting='hard')\nclf\n#硬投票\neclf = VotingClassifier(estimators=[('xgb',clf1),('rf',clf2),('svc',clf3)],voting='hard')\nX_train.info()\nindex = 0\nfor clf,label in zip([clf1,clf2,clf3,eclf],['XGBBoosting','Random Forest','SVM','Voting']):\n        scores = cross_val_score(clf,X_train,y_train,cv=5,scoring='accuracy')\n        print(\"Accuracy:%0.2f(+/- %0.2f) [%s]\"%(scores.mean(),scores.std(),label))\n#软投票只需要设置voting='soft'即可\neclf = VotingClassifier(estimators=[('xgb',clf1),('rf',clf2),('svc',clf3)],voting='soft')\nX_train.info()\nfor clf,label in zip([clf1,clf2,clf3,eclf],['XGBBoosting','Random Forest','SVM','Voting']):\n    scores = cross_val_score(clf,X_train,y_train,cv=5,scoring='accuracy')\n    print(\"Accuracy:%0.2f(+/- %0.2f) [%s]\" %(scores.mean(),scores.std(),label))\n#软投票自定义权重\neclf = VotingClassifier(estimators=[('xgb',clf1),('rf',clf2),('svc',clf3)],voting='soft',weights=[0,1,9])\nX_train.info()\nfor clf,label in zip([clf1,clf2,clf3,eclf],['XGBBoosting','Random Forest','SVM','Voting']):\n    scores = cross_val_score(clf,X_train,y_train,cv=5,scoring='accuracy')\n    print(\"Accuracy:%0.2f(+/- %0.2f) [%s]\" %(scores.mean(),scores.std(),label))\n"
    },
    {
        "title": "聚类1，data_cluster.csv,MinMaxScaler",
        "content": "df = pd.read_csv('./data/data_cluster.csv',encoding='gbk',header=0)\n#1.使用MinMaxScaler函数对数据进行归一化处理，将数据存储为df_norm.\nfrom sklearn.preprocessing import MinMaxScaler\nmodel = MinMaxScaler()\ndf_norm = model.fit_transform(df)\ndf_norm = pd.DataFrame(df_norm,columns=df.columns)\n#2.使用SpecteralClustering算法选择K值2-12对数据进行聚类分析，使用轮廓系数进行评分，将轮廓系数和K值存储在列表k和silhouette_s 中，并绘制折线图\nfrom sklearn.cluster import SpectralClustering\nfrom sklearn.metrics import silhouette_score \nk,silhouette_s = [],[]\nfor i in range(2,13):\n    sc_model = SpectralClustering(n_clusters=i)    #这里本地运行很慢，在云环境就好很多\n    sc_model.fit(X=df_norm)\n    k.append(i)\n    print(i)\n    s = silhouette_score(X=df_norm,labels=sc_model.labels_)\n    silhouette_s.append(s)\nimport matplotlib.pyplot as plt\nplt.figure()\nplt.rc('font',family='YouYuan')   #这个记得加上\nplt.title('SpectralClustering')\nplt.xlabel(\"聚类的簇数\")\nplt.ylabel('silhouette_score')\nplt.plot(k,silhouette_s)\nplt.show()\n"
    },
    {
        "title": "聚类2，data_cluster.csv,sample()",
        "content": "import seaborn as sns\nfrom sklearn import metrics\nsns.set()\npd.options.display.max_columns = None\nimport warnings \nwarnings.filterwarnings('ignore')\norigin_data = pd.read_csv('./data/data_cluster.csv')\n#1.使用sample()方法从origin_data中随机抽取10000个样本，并允许重复抽样。重置索引，使得输出data为以下格式\ndata = origin_data.sample(n=10000)\ndata.reset_index()  #重置索引\ndf_cluster = data.copy()\n#2.从df_cluster中选择所有np.number类型的列，并将其保存在df_X中\ndf_X = df_cluster.select_dtypes(include=np.number)\ndf_X\ncols = df_X.columns\ncols\nfrom sklearn.preprocessing import MinMaxScaler\nscaler = MinMaxScaler()\nsclaed = pd.DataFrame(scaler.fit_transform(df_X),columns=cols)\nsclaed.head()\nfrom sklearn.cluster import KMeans\nk,silhouette,sse = [],[],[]\n#3.使用K-means模型，设置聚类数为i,初始算法为k-means++,最大迭代次数为500，初始化次数为10，随机种子为0，并将模型拟合到scaled数据上，计算使用欧几里得距离度量标准的轮廓系数s1和模型的质心间距离的总和s2。\nfrom sklearn.metrics import silhouette_score\nfor i in range(2,20):\n    kmeans_model = KMeans(n_clusters=i,init='k-means++',max_iter=500,n_init=10,random_state=0)\n    kmeans_model.fit(sclaed)\n    result = kmeans_model.labels_\n    k.append(i)\n    s1 = silhouette_score(sclaed,result)\n    ss = kmeans_model.inertia_ \n    sse.append(ss)\n    silhouette.append(s1)\n    print('K=',i,'Silhouette=',s1,'SSE=',ss)\nprint(len(silhouette),len(range(2,20)))\nplt.figure()\nplt.title('K-means silhouette')\n# plt.plot(range(2,20),silhouette,'*')  ##第三个参数，'o'代表折线的每一处拐角处用什么符号标记，o代表用原点，*代表用星号好表\nplt.plot(k,silhouette,'*')\n# plt.plot(range(2,20),silhouette,'-',alpha=0.5)\nplt.plot(k,silhouette,'-',alpha=0.5)\nplt.xlabel('Number of Cluster')\nplt.ylabel('silhouette')\nplt.show()\n\nplt.figure()\nplt.title('K-means SSE')\nplt.plot(k,sse)\nplt.plot(k,sse,'o')  #第三个参数，'o'代表折线的每一处拐角处用什么符号标记，o代表用原点，*代表用星号好表\nplt.xlabel('Number of cluster')\nplt.ylabel('SSE')\nplt.show()\n"
    },
    {
        "title": "回归student-info.csv",
        "content": "import matplotlib.pyplot as plt\nimport seaborn as sns\ndf = pd.read_csv('./data/student-info.csv')\n#1.查看年龄和性别的分布柱状图，输出如下：\nsns.countplot(x='age',hue='sex',data=df)\nplt.xlabel('age')\nplt.ylabel('sex')\nplt.title('Distribution bar chart of age and gender')\nplt.show()\n#2.查看年龄和成绩分布箱线图，输出如下：\nsns.boxplot(x='age',y='G3',data=df)\nplt.xlabel('age')\nplt.ylabel('G3')\nplt.title('Box plot of age and grade distribution')\nplt.show()\n#3.使用swarmplot函数查看年龄和成绩分布图：输出如下\nsns.swarmplot(x='age',y='G3',data=df)\nplt.xlabel('age')\nplt.ylabel('G3')\nplt.title('Age and grade distribution chart')\nplt.show()\n\ndel df['G1']\ndel df['G2']\ndf_oh = pd.get_dummies(df)\n#4.分析数据相关性系数，并取出和G3相关性最高的9个属性\ncro = df_oh.corr()\na = cro['G3'].sort_values(ascending=True)[:9]\nfrom sklearn.model_selection import train_test_split\nX = df_oh.loc[:,df_oh.columns != 'G3']\ny = df_oh['G3']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)\n#5.使用lightBGM算法进行建模，并使用网格搜索对模型进行cv使用5，输出模型评分的mae,rmse,abs\nfrom lightgbm import LGBMRegressor\n# 配置最优模型参数的模型\nmodel = LGBMRegressor()\nmodel.fit(X=X_train,y=y_train)\n# 使用偏差进行预测\nfrom sklearn.metrics import mean_absolute_error,mean_squared_log_error,mean_squared_error\nfrom sklearn.metrics import make_scorer\nprint(mean_absolute_error(y_test,model.predict(X_test)))\nprint(mean_squared_log_error(y_test,model.predict(X_test)))\ny_pred = model.predict(X_test)\nprint(np.median(np.abs(y_test - y_pred)/y_pred))\n\nfrom sklearn.model_selection import GridSearchCV\nparam_grid = {'learning_rate':[0.1,0.2,0.3,0.4,0.5]}\nmodel_gs = GridSearchCV(estimator=model,param_grid=param_grid,cv=5,scoring=make_scorer(mean_absolute_error))\nmodel_gs.fit(X_train,y_train)\nmodel_gs.best_score_\n#6.使用Voting算法聚合RandomForestRegressor,GradientBoostingRegressor,LinearRegression模型，预测成绩G3\nfrom sklearn.ensemble import VotingRegressor\nfrom sklearn.ensemble import RandomForestRegressor,GradientBoostingRegressor\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.metrics import r2_score\nreg1 = RandomForestRegressor(random_state=1)\nreg1.fit(X_train,y_train)\nprint('rf R2:',r2_score(y_test,reg1.predict(X_test)))\nreg2 = GradientBoostingRegressor(random_state=1)\nreg1.fit(X_train,y_train)\nprint('gbdt R2:',r2_score(y_test,reg1.predict(X_test)))\nreg3 = LinearRegression()\nreg3.fit(X_train,y_train)\nprint('lr R2:',r2_score(y_test,reg1.predict(X_test)))\nfrom sklearn.ensemble import VotingClassifier\nfrom mlxtend.classifier import StackingCVClassifier\nVotingClassifier()\nereg = VotingRegressor([('gb',reg1),('rf',reg2),('lr',reg3)],weights=[1,5,10])\nereg.fit(X_train,y_train)\nr2_score(y_test,ereg.predict(X_test))\npre_ereg = ereg.predict(X_test)\nprint(\"VotingRegressor R2:\",r2_score(y_test,pre_ereg))\nprint(pre_ereg)\n\n"
    },
    {
        "title": "特征train_forest_covertype.csv",
        "content": "import warnings\nwarnings.filterwarnings('ignore')\nimport numpy as np\nimport pandas as pd\nimport time\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom subprocess import check_output \nfrom lightgbm import LGBMClassifier\nimport lightgbm as lgb\nfrom tqdm import tqdm_notebook as tqdm\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import confusion_matrix,log_loss\n#!pip install xgboost\nfrom xgboost import XGBClassifier\nfrom lightgbm import LGBMClassifier\nfrom sklearn.pipeline import Pipeline\n#读取数据\ndf_forest = pd.read_csv('./data/train_forest_covertype.csv')\n\n#随机采样15120条数据\ndf_forest_sample = df_forest.sample(n=15120)\n\n# 删除Soil_Type7和Soil_Type15\ndf_forest_sample.drop(['Soil_Type7','Soil_Type15'],axis=1,inplace=True)\n\nXGB = XGBClassifier()\nlgbm = LGBMClassifier()\nfirst_models = [XGB,lgbm]\nfirst_model_names = ['XGB','lgbm']\nseed = 42\nskf = 5\n\n#1.ShuffleSplit函数进行切分，指定参数plitting iterations为skf,test_size为0.3，random_state为seed\nfrom sklearn.model_selection import ShuffleSplit\nfrom sklearn.preprocessing import StandardScaler\nn_folds = 5\nShuffleSplit(n_splits=skf, random_state=seed, test_size=0.3, train_size=0.6)  #切分数据\nstd_sca = StandardScaler()\nX = df_forest_sample.drop(columns=['Cover_Type'])\ny = pd.factorize(df_forest_sample['Cover_Type'])[0]\n\nMLA_columns = ['MLA Name','MLA Parameters','MLA Train Accuracy Mean','MLA Test Accuracy Mean','MLA Time']\nMLA_compare = pd.DataFrame(columns=MLA_columns)\n#create table to compare MLA predictions\nMLA_predict = df_forest_sample[['Id']]\ntrain_size = X.shape[0]\nn_models = len(first_models)\noof_pred = np.zeros((train_size,n_models))\nscores = []\nrow_index = 0\n\n#2.使用Pipline进行模型训练set中指定标准为('Scaler',std_sca),模型为('Estimator',model)3.使用cross_validate函数对模型进行评分，模型使用model,数据集使用X，y,指定return_train_score为True\nfrom sklearn.model_selection import cross_validate\ncross_validate()\nfrom sklearn.model_selection import cross_validate\nfor n,model in enumerate(first_models):\n    model_pipeline = Pipeline(steps=[('Scaler', StandardScaler()), ('Estimator', model)])\n    MLA_name = model.__class__.__name__\n    MLA_compare.loc[row_index,'MLA_name'] = MLA_name\n    MLA_compare.loc[row_index,'MLA Parameters'] = str(model_pipeline.get_params())\n    cv_result = cross_validate(estimator=model,X=X,y=y,cv=skf,return_train_score=True)\n    MLA_compare.loc[row_index,'MLA Time'] = cv_result['fit_time'].mean()\n    MLA_compare.loc[row_index,'MLA Train Accuracy Mean'] = cv_result['train_score'].mean()\n    MLA_compare.loc[row_index,'MLA Test Accuracy Mean'] = cv_result['test_score'].mean()\n    model_pipeline.fit(X,y)\n    MLA_predict[MLA_name] = model_pipeline.predict(X)\n    row_index+=1\n\nMLA_compare\n\ncv_result = cross_validate(estimator=first_models[0],X=X,y=y,cv=skf,return_train_score=True)\n\n#4.使用sort_values对按照集 MLA Test Accuracy Mean这一列倒序排序，指定inplace为True，覆盖原本数据\nMLA_compare.sort_values(by=['MLA Test Accuracy Mean'],ascending=True,inplace=True)\nMLA_compare\nMLA_compare.index[-20:-1]\n\n#5.删除MLA_compare后20位数据\nMLA_compare.drop(axis=0,index=MLA_compare.index[-20:-1])"
    }
]
    results = []
    for entry in json_data:
        if key.lower() in entry['title'].lower():
            results.append({'title': entry['title'], 'content': entry['content']})
    df = pd.DataFrame(results)
    df.style.set_properties(**{'white-space': 'pre-wrap', 'text-align': 'left'})
def d_by_c(key):
    json_data=[
    {
        "title": "关联规则1fruit",
        "content": "df = pd.read_csv('./data/fruit.csv',sep=';')\n#1.以“;”为分割符读取数据中的ID和fruit列，将fruit列通过lambda函数，以逗号为分割符进行切分并去除数据集中的空格，生成矩阵 df_mtix\ndf_mtix = df['fruit'].map(lambda x:x.replace(\" \",'').split(','))\n#2.使用TransactionEncoder函数对矩阵进行拟合df_mtix,生成数据df_te。将数据df_te转换成DataFrame,使用df_te.columns为列名\nfrom mlxtend.preprocessing import TransactionEncoder\nte = TransactionEncoder()\ndf_te = te.fit_transform(df_mtix)\ndf_te = pd.DataFrame(data=df_te,columns=te.columns_)\n#3.使用fpgrowth算法建模发现数据中的关联规则，指定min_support=0.5,use_colname=True,使用mlxtend库中的assocication_rules 函数来找出关联规则。指定metric为'confidence',min_threshold为0.3\nfrom mlxtend.frequent_patterns import fpgrowth\nfrom mlxtend.frequent_patterns import association_rules\nmodel = fpgrowth(df_te,min_support=0.5,use_colnames=True)\nrules = association_rules(model,metric='confidence',min_threshold=0.3)\n#4.输出关联规则rules中，前验置信度>0.6,置信度大于0.5的项集\nrules[(rules['antecedent support']>0.6) & (rules['support']>0.5)]\n#5.输出关联规则rules中，antecedents为{苹果,香蕉}的内容\nrules[rules['antecedents'] == {'apple','banana'}]"
    },
    {
        "title": "关联规则2transactions",
        "content": "df = pd.read_csv('./data/transactions.csv')\n#1.对df中的items列应用lambda函数，将每行中的值按照逗号分割的字符串拆分成列表，并去掉空格，将结果存储在transactions列表中\ntransactions = []\ntransactions = df['items'].map(lambda x:x.replace(\" \",'').split(','))\n#2.使用TransctionEncoder将数transactions转换为合适的Apriori算法的格式数据te_data,使用Pandas的DataFrame()函数将te_data转换为DataFrame对象df\nfrom mlxtend.preprocessing import TransactionEncoder\nte = TransactionEncoder()\nte_data = te.fit_transform(transactions)\ndf = pd.DataFrame(te_data,columns=te.columns_)\nfrom mlxtend.frequent_patterns import fpgrowth\nfrequent_items = fpgrowth(df,min_support=0.3,use_colnames=True)\n#3在频繁项集frequent_items中构建关联规则rules,度量指标为confidence,最小阀值为0.5\nfrom mlxtend.frequent_patterns import association_rules\nrules = association_rules(frequent_items,metric='confidence',min_threshold=0.5)\n#4.打印同时满足先验支持度>=0.6,支持度>0.3,提升度>0.8的rules数据\nrules[(rules['antecedent support']>=0.6) & (rules['support']>=0.3) & (rules['lift']>=0.8)]\n#5.筛选规则rules,将规则中同时满足antecedents_sale为Banana,consequents_sale为Apple规则选出来，并存储在final_sale中，使得rules.loc[fina_sale]输出\nantecedents_sale = rules['antecedents'] == frozenset({'Banana'})  #返回true和false的集合\nconsequents_sale = rules['consequents'] == frozenset({'Apple'})\nfinal_sale = (antecedents_sale&consequents_sale)\n#考试的时候要注意这里，考题有可能是删除，也有可能是筛选\n# as_rule.loc[~final_sale]  #加了波浪符号或者负号表示筛选出相反的数据，也就是选择不同时含有上述条件的选项 ，即删除以上条件的数据\nrules.loc[final_sale]  #不加了波浪符号或者负号表示筛选出以上条件的数据   "
    },
    {
        "title": "关联规则3GoodsOrder_eng.csv",
        "content": "pip install mlxtend\nimport numpy as np \nimport pandas as pd\nfrom mlxtend.frequent_patterns import apriori,fpgrowth,association_rules\nimport warnings \nwarnings.filterwarnings('ignore')\norder_data = pd.read_csv('./data/GoodsOrder_eng.csv',sep=',',header=0,encoding='gbk')\n#转换数据格式\norder_data['Goods'] = order_data['Goods'].apply(lambda x : '' + x)\n#将order_data按id分组求和，并重置索引\ndata = order_data.groupby(by='id',as_index=False)['Goods'].sum().reset_index()\ndata['Goods'] = data['Goods'].apply(lambda x:[x[1:]])\ndata_list = list(data['Goods'])\n#2.分割商品名为每一个元素。使得dataset最终输出如以下格式：\ndata_trans = []\nfor i in data_list:\n    p = i[0].split(',')\n    data_trans.append(p)\ndata_set = data_trans\ncolumn_list = []\nfor var in data_set:\n    column_list = set(column_list)|set(var)\n#3.遍历data_set中的每一个商品，并将data中对应位置的值加1，即购买一次则在相应物品上加1，使得data输出为以下形式：\ndata = pd.DataFrame(np.zeros((len(data_set),6)),columns=list(column_list))\nfor i in range(len(data_set)):\n    for j in data_set[i]:\n        data[j][i]+=1\ndata = data.applymap(lambda x:1 if x > 0 else 0)\n#4.使用Apriori算法从数据中计算频繁项集，并将最小支持度设置为0.02.然后根据支持度倒排序，最后返回频繁项集frequent_itemts,使得frequent_items部分输出为：\nfrom mlxtend.frequent_patterns import apriori\nfrom mlxtend.frequent_patterns import fpgrowth\nfrequent_itemts = apriori(data,min_support=0.02,use_colnames=True)\nfrequent_itemts = frequent_itemts.sort_values(by='support',ascending=False)\nfrequent_itemts\n#5.使用association_rules从频繁项集frequent_items中构建关联规则，metric为'confidence',min_threshold=0.35。对关联规则按confidence值进行降序排列，并将排序结果存储在association_rule中，使得df_association_rule部分输出为:\nfrom mlxtend.preprocessing import TransactionEncoder\nfrom mlxtend.frequent_patterns import association_rules\nassociation_rule = association_rules(frequent_itemts,metric='confidence',min_threshold=0.35)\nassociation_rule.sort_values(by='confidence',ascending=False,inplace=True)\nassociation_rule\n"
    },
    {
        "title": "pyspark2,creditcard",
        "content": "from pyspark.context import SparkContext\nfrom pyspark.sql import SparkSession\nsc = SparkContext('local','test')\nfrom pyspark.sql.functions import explode\nfrom pyspark.sql.functions import split\nspark = SparkSession(sc)\nsc\ndata_O = spark.read.load('./data/creditcard.csv',format='csv',header='true',inferSchema='true')\ndata_O.show(1)\n#1.data_O按Class列分组统计每个类别的个数，输出为classFreq,打印classFreq输出为\nclassFreq = data_O.groupBy('Class').count()\nclassFreq.show()\ndata = data_O.toPandas()\ndata = data.sample(frac=1)\n#欺诈样本492条\nfraud_df = data.loc[data['Class']==1]\nfraud_df   #492*31\nnon_fraud_df = data.loc[data['Class']==0][:492]\nnormal_distributed_df = pd.concat([fraud_df,non_fraud_df])\nnormal_distributed_df\nnew_df = normal_distributed_df.sample(frac=1,random_state=42)\nnew_df.shape\nimport seaborn as sns\nimport matplotlib.pyplot as plt\ncolors = ['#BF9C5','#f9c5b3']\n#2.以Class为X轴，V10为Y轴，绘制出箱线图，用于查看new_df数据集中V10变量在不同班级之间的分布情况。输出如下：\nV10_sns = sns.boxplot(x='Class',y='V10',data=new_df)\nV10_sns.set_title('V10 vs Class Negative Correlation')\nplt.show()\ndfff = spark.createDataFrame(new_df)\nfrom pyspark.sql.functions import *\nfrom pyspark.sql.window import Window\n#3.运用withColumn将dfff添加一列idx,该列是按照窗口win中的Time字段排序后的行号row_number.\nwin = Window().orderBy('Time')\ndfff = dfff.withColumn('idx',row_number().over(win))\ndfff.show(1)\nfrom pyspark.ml import Pipeline\nfrom pyspark.ml.classification import GBTClassifier\nfrom pyspark.ml.feature import VectorIndexer,VectorAssembler\nfrom pyspark.ml.evaluation import BinaryClassificationEvaluator\nfrom pyspark.ml.linalg import DenseVector\n#4.将dfff中的rdd映射为元组，元组中只有一个元素DenseVector,DenseVector的第一个值由dfff的前30列组成，第二个值为dfff的第31列，第三个值为dfff的第32列\ntraining_df = dfff.rdd.map(lambda x:(DenseVector(x[0:29]),x[30],x[31]))\ntraining_df = spark.createDataFrame(training_df,[\"features\",\"label\",\"index\"])\ntraining_df.show(1)\ntraining_df = training_df.select('index','features','label')\ntraining_df.show(1)\ntrain_data,test_data = training_df.randomSplit([.8,.2],seed=1234)\ngbt = GBTClassifier(featuresCol='features',maxIter=100,maxDepth=8)\nmodel = gbt.fit(train_data)\npredictions = model.transform(test_data)\nevaluator = BinaryClassificationEvaluator()\nevaluator.evaluate(predictions)\n混淆矩阵： P(正例/正元组)：感兴趣的类别 N(负例/负元组)：不感兴趣的类别 TP真正例：原本是正例也被预测为正例 TN真负例：原本为负例也被预测为负例 FP假正例：原本为负例被预测为正例 FN假负例：原本为正例被预测为负例 分类的偏差考察角度： 准确率(精度)：(TP+TN)/P+N 错误率(汉明损失)：(FN+FP)/P+N 查准率：TP/(TP+FP),主要关注对负例的预测是否精准 查全率：TP/(TP+FN),主要关注对正例的预测是否精准，也叫召回率 F1值(常用)：F1值是查准率和查全率的一个调和平均值。其综合考虑了查准率和查全率\ntp = predictions[(predictions.label == 1) & (predictions.prediction == 1)].count()\ntn = predictions[(predictions.label == 0) & (predictions.prediction == 0)].count()\nfp = predictions[(predictions.label == 0) & (predictions.prediction == 1)].count()\nfn = predictions[(predictions.label == 1) & (predictions.prediction == 0)].count()\n#5.打印召回率和精确率\nprint(\"Recall:\",tp/(tp+fn))\nprint(\"Precision:\",tp/(tp+fp))"
    },
    {
        "title": "pyspark3,houses_data",
        "content": "from pyspark.context import SparkContext\nfrom pyspark.sql import SparkSession\nsc = SparkContext('local','test')\nspark = SparkSession(sc)\nsc\nfrom pyspark.sql.types import *\nfrom pyspark.sql import Row\nhouses_data = spark.read.format('org.apache.spark.sql.execution.datasources.csv.CSVFileFormat')\\\n.option('header','true')\\\n.option('inferSchema','true')\\\n.load('./data/houses_data.csv')\nrdd = sc.textFile('./data/houses_data.csv')\nhouses_data.show(5)\nprint(rdd.take(5))\n#1.对数据集的每一行用逗号进行分割\nrdd = rdd.map(lambda x:x.split(','))\nrdd.take(2)\nheader = rdd.first()\nheader\n#2.使用‘filter’删除包含标题的行\nrdd = rdd.filter(lambda x:x!=header)\nrdd.take(2)\ndf = rdd.map(lambda line:Row(street=line[0],city=line[1],zip=line[2],beds=line[4],baths=line[5],sqft=line[6],price=line[9])).toDF()\ndf.show(5)\ndf.toPandas().head()\n#3.df按照'beds'字段分组，并计算每个分组中记录的数量，并显示结果\ndf.groupBy('beds').count().show()\ndf.describe(['baths','beds','price','sqft']).show()\nimport pyspark.mllib\nimport pyspark.mllib.regression\nfrom pyspark.mllib.regression import LabeledPoint\nfrom pyspark.sql.functions import  *\ndf = df.select('price','baths','beds','sqft')\ndf = df[df.baths>0]\ndf = df[df.beds>0]\ndf = df[df.sqft>0]\ndf.show(5)\ndf.describe(['baths','beds','price','sqft']).show()\ntemp = df.rdd.map(lambda line:LabeledPoint(line[0],[line[1:]]))\ntemp.take(5)\nfrom pyspark.mllib.util import MLUtils\nfrom pyspark.mllib.linalg import Vectors\nfrom pyspark.mllib.feature import StandardScaler\nfeatures = df.rdd.map(lambda row:row[1:])\nfeatures.take(5)\nstandardizer = StandardScaler()\nmodel = standardizer.fit(features)\nfeature_transform = model.transform(features)\nfeature_transform.take(5)\nlab = df.rdd.map(lambda row:row[0])\nlab.take(5)\n#4.将标签lab和特征feature_transform进行zip操作\ntransformedData = lab.zip(feature_transform)  \ntransformedData.take(5)\n#5.将transformedData转换为LabeledPoint类型，其中row[0]作为标签，row[1]作为特征向量\ntransformedData = transformedData.map(lambda row:LabeledPoint(row[0],row[1]))\ntransformedData.take(5)\ntrainingData,testingData = transformedData.randomSplit([.8,.2],seed=1234)\nfrom pyspark.mllib.regression import LinearRegressionWithSGD\nlinearModel = LinearRegressionWithSGD.train(trainingData,1000,.2)\nlinearModel.weights\ntestingData.take(10)\nlinearModel.predict([1.49297445326,3.52055958053,1.73535287287])"
    },
    {
        "title": "pyspark1,iris",
        "content": "import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom sklearn.datasets import load_iris\nfrom pyspark.conf import SparkConf\nfrom pyspark.ml.feature import VectorAssembler,StandardScaler,PCA\nfrom pyspark.context import SparkContext\nfrom pyspark.sql import SparkSession\nsc = SparkContext('local','test')\nspark = SparkSession(sc)\nsc\niris = load_iris()\nX = iris['data']\ny = iris['target']\ndata = pd.DataFrame(X,columns=iris.feature_names)\n#1.将Pandas数据框转换为Spark数据框dataset,并将其列名设置为iris特征名称，使得dataset输出为：\ndataset = spark.createDataFrame(data)\ndataset.show(6)\n#2.使用VectorAssembler将dataset多列数据转化为单列的向量‘features’,使得df输出为：\ncol = ['sepal length (cm)','sepal width (cm)','petal length (cm)','petal width (cm)']\nmodel_ve = VectorAssembler(inputCols=col, outputCol=\"features\")\ndf = model_ve.transform(dataset).select('features')\ndf.show(6)\n#3.创建一个StandarScaler对象，命名为scaler,用来对特征列‘features’进行标准化，将标准化后的结果输出到新的特征列'scaledFeatures',withMean和withStd分别为True,最后在df上进行fit操作\nscaler = StandardScaler(inputCol='features',outputCol='scaledFeatures',withMean=True,withStd=True).fit(df)\ndf_scaled = scaler.transform(df)\nn_components = 3\npca = PCA(k=n_components,inputCol='scaledFeatures',outputCol='pcaFeatures').fit(df_scaled)\ndf_pca = pca.transform(df_scaled)\nprint('Expained Variance Ratio',pca.explainedVariance.toArray())\ndf_pca.show(6)\ndf_pca.rdd.collect()[0:2]\nlist1 = df_pca.rdd.map(lambda x:(x[2][0],x[2][1],x[2][2])).collect()\ntype(list1[1][1])\ndf_origin = spark.createDataFrame([(float(tup[0]),float(tup[1]),float(tup[2])) for tup in list1],['x','y','z'])\ndf_origin.show(5)\n#4.将Spark数据框df_origin转换为Pandas数据框df_Pandas\ndf_Pandas = df_origin.toPandas()\ndf_Pandas.head()\ndf_Pandas['id'] = 'row' + df_Pandas.index.astype(str)\ncols = list(df_Pandas)\ncols\ncols.insert(0,cols.pop(cols.index('id')))\ndf_Pandas = df_Pandas.loc[:,cols]\ndf_Pandas.head(5)\npath = './data/input.csv'\n#5.将df_Pandas的数据保存到指定的path下，不保存index\ndf_Pandas.to_csv(path,index=False)\ndf = spark.read.csv(path,header=True)\ndf.show(5)"
    },
    {
        "title": "推荐1，BX-Book-Ratings",
        "content": "user_artists = pd.read_csv('./data/book/BX-Book-Ratings.csv',sep=';',encoding='latin-1',usecols=['User-ID','ISBN','Book-Rating'])\n#1.构建评分矩阵rating_matric,将缺失值填充为0\nrating_matric = user_artists.pivot_table(index='User-ID',columns='ISBN',values='Book-Rating')\nrating_matric.fillna(0,inplace=True)\nrating_matric.head(5)\n#2.使用cosine_similarity函数计算项目相似度矩阵\n#3.用户评分过的物品中获取与item相似的物品\nfrom sklearn.metrics.pairwise import cosine_similarity\nitem_similarity_matric = cosine_similarity(rating_matric.T)\n#将rating_matric 的索引转换为字符串\nrating_matric.index = rating_matric.index.map(str)\n#定义预测用户对书籍的评分函数\ndef predict_rating(user,item):\n    # 获取item的索引\n    item_index = rating_matric.columns.get_loc(item)\n    #获取item的相似度分数\n    similarity_scores = item_similarity_matric[item_index]\n    #获取用户的评分\n    user_ratings = rating_matric.loc[user]\n    #获取用户评分过的的物品\n    rated_items = user_ratings[user_ratings > 0].index\n    #在用户评分过的物品中获取与item相似的物品   ,由考生填写 .这里有点难理解\n    similar_items = [item for item in rated_items if item_similarity_matric[item_index][rating_matric.columns.get_loc(item)]>0]\n    similar_items_index = []\n    for i in range(len(similar_items)):\n        similar_items_index.append(rating_matric.columns.get_loc(similar_items[i]))\n        #如果没有相似的物品，则返回0\n    if len(similar_items) == 0:\n        return 0\n    #否则，计算用户对item的预测评分\n    else:\n        #a = np.array([1,2]),b = np.array([3,4])要求元素个数相同，相当于求内积，对应元素相乘再相加，“1*3 + 2*4 = 11”\n        return user_ratings[similar_items].dot(similarity_scores[similar_items_index])/similarity_scores[similar_items_index].sum()\n        #上面其实是求均值，也可以这么写\n        # return user_ratings[similar_items].mean()\n# 测试，计算用户276729对书籍0064401367的预测评分\nuser = '276729'\nitem = '0064401367'\npredict_rating = predict_rating(user,item)\nprint('预测用户{}对书籍{}的评分为:{}'.format(user,item,predict_rating))"
    },
    {
        "title": "推荐2，artists.dat",
        "content": "user_artists = pd.read_csv('./data/music/user_artists.dat',sep='\\t') #优用'\\t'分隔符读取文件\nartists = pd.read_csv('./data/music/artists.dat',sep='\\t',usecols=['id','name']) #仅读取id,name列\n#1.将user_artists和artist两个dataFrame进行合并，以user_artists中的artistID和artists中的id为键值进行连接\ndata = pd.merge(user_artists,artists,left_on='artistID',right_on='id')\ndata = data.drop(['id','artistID'],axis=1)    \ndata\n#2.去除评分次数少于50次的音乐和用户\ncounts = data['userID'].value_counts()   #计算每个用户的评分次数\ndata = data[data['userID'].isin(counts[counts >=50].index)]  #去除评分次数少于50次的用户\ndata\ncounts = data['name'].value_counts()   #计算每个音乐的评分次数\ndata = data[data['name'].isin(counts[counts >=50].index)]   #去除评分次数少于50次的音乐\ndata\n#将音乐和用户ID转换为数字\nname_to_index = {} #创建name_to_index字典\nindex_to_name = {} #创建index_to_name字典\nfor i,name in enumerate(data['name'].unique()):  #遍历data中的name列\n    name_to_index[name] = i   #将name映射到数字\n    index_to_name[i] = name   #将数字映射到name\n\nuser_id_to_index = {} #创建user_id_to_index字典\nindex_to_user_id = {} #创建index_to_user_id字典\nfor i,user_id in enumerate(data['userID'].unique()):  #遍历data中的name列\n    user_id_to_index[user_id] = i   #将user_id映射到数字\n    index_to_user_id[i] = user_id   #将数字映射到user_id\ndata['name_index'] = data['name'].apply(lambda x:name_to_index[x]) #将data中的name映射到数字\ndata['user_index'] = data['userID'].apply(lambda x:user_id_to_index[x]) #将data中的user_id映射到数字\n\n# 构建用户-音乐评分矩阵\nn_users = len(user_id_to_index)  #获取用户数量\nn_artists = len(name_to_index)   #获取音乐数量\nratings_matrix = np.zeros((n_users,n_artists))  #创建用户-音乐评分矩阵\n#3.将data中的评分填入用户-音乐评分矩阵，使得ratingsmatrix的输出为：\nfor row in data.itertuples(): #遍历data\n    ratings_matrix[row[5],row[4]] = row[1]  #将data中的评分填入用户-音乐评分矩阵\n#计算用户之间的相似度\nfrom sklearn.metrics.pairwise import cosine_similarity   #导入sklearn中的余弦相似度函数\nuser_simlarity = cosine_similarity(ratings_matrix)  #计算用户之间的相似度\nuser_simlarity\nuser_index = 0 #设置用户索引\n#4.获取与用户最相似的用户索引similar_users(不包括自己),使得similar_users的输出为：\n#获取与用户索引为0的用户最相似的用户的索引\n#index_u = np.argsort(-user_similarity[user_index])   #np.argsort：排序，但是返回的是数据的下标\n# 方式1\n# similar_users = np.argsort(-user_simlarity[user_index])[np.argsort(-user_simlarity[user_index]) != 0]\n# 方式2\nsimilar_users = np.argsort(-user_simlarity[user_index])[1:] #从1开始就是把0去掉，0是自己\n\nsimilar_users\nrecommended_artists = []   #创建推荐音乐列表\n#5.填补以下程序，获取相似用户评分的音乐索引artists_rated\nfor i in similar_users: #遍历相似用户索引\n#     np.where(rating_matrix[i]>0)返回的是一个元组，第一个值为下标索引的列表，第二个无值\n    artists_rated = np.where(ratings_matrix[i] > 0)[0] #获取相似用户评分的音乐,要选出相似用户听过的，也就是评分>0\n    for j in artists_rated:\n        if ratings_matrix[user_index][j] == 0: #用户未评分\n            recommended_artists.append((j,ratings_matrix[i][j])) #将音乐加入推荐音乐列表  (j,ratings_matrix[i][j]):(音乐,评分)\nrecommended_artists[0:10]\nrecommended_artists = sorted(recommended_artists,key=lambda x:x[1],reverse=True)[:10] #筛选出推荐音乐中的前10首\nartists['name'].values\nfor artist in recommended_artists: #遍历推荐音乐\n#     以下两种方式都行\n    print(artists[artists['name'] == index_to_name[artist[0]]]['name'].values[0])  #打印推荐音乐 values的结果为一个数组，但数组里面只有一个值\n    print(index_to_name[artist[0]])"
    },
    {
        "title": "分类1，pima-indians-diabetes.data",
        "content": "df = pd.read_csv('./data/pima-indians-diabetes.data',sep=',')\ndf['Outcome'].value_counts() #将数据集按照输入特征和目标特征进行划分，分别命名为X,y，且保证其类型为numpy.ndarry\nx_cols = [col for col in df.columns if col!='Outcome']\ny_col = 'Outcome'\nX = df[x_cols].values #将dataframe转化为ndarry，才能进入下面的标准化\ny = df[y_col].values\n#1.划分数据集X,y,分别命名为X_train,X_test,y_train,y_test,测试集比例为10%，固定随机数种子为42，打乱顺序，并以df[y_col]做分层抽样\n#横向拆分数据\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import  StandardScaler\nfrom collections import Counter\nx_train, x_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=42)\n#查看划分后的训练集和测试集中两类目标值的数量\nprint('Distribution of y train {}'.format(Counter(y_train)))\nprint('Distribution of y test {}'.format(Counter(y_test)))\n#标准化\nstd_scaler = StandardScaler().fit(X)\nx_train = std_scaler.fit_transform(x_train)\nx_test = std_scaler.fit_transform(x_test)\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.svm import  SVC\nfrom sklearn.linear_model import  LogisticRegression\nfrom sklearn.tree import  DecisionTreeClassifier\nfrom sklearn.naive_bayes import  GaussianNB\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.ensemble import GradientBoostingClassifier\nmodels = []\nmodels.append(('KNN',KNeighborsClassifier()))\nmodels.append(('SVC',SVC()))\nmodels.append(('LR',LogisticRegression()))\nmodels.append(('DT',DecisionTreeClassifier()))\nmodels.append(('GNB',GaussianNB()))\nmodels.append(('RF',RandomForestClassifier()))\nmodels.append(('GB',GradientBoostingClassifier()))\nnames = []\nscores = []\n#2.遍历models中的每个模型，分别使用训练集(xtrain,ytrain)进行训练，然后用测试集xtest进行预测，计算准确率accuracy score,并将模型名称name和准确率accuracy score分别存储在列表names和scores中，最后将列表names和scores转换成DataFrame格式，DataFrame命名为tr split,tr split打印输出结果如下：\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import make_scorer\nfrom sklearn.model_selection import cross_val_score\nfor name,model in models:\n    model.fit(x_train,y_train)\n    score = accuracy_score(y_test,model.predict(x_test))\n    names.append(name)\n    scores.append(score)\n# 方式1:\n# tr_split = pd.DataFrame(data={'name':names,'score':scores})\n# 方式2:\ntr_split = pd.DataFrame(data=list(zip(names,scores)),columns=['name','score'])\ntr_split\nfrom sklearn.model_selection import GridSearchCV\nc_values = list(np.arange(1,10))\n#3.使用GridSearchCV对GradientBoostingClassifier进行网格搜索，参数为param_prid,使用5折交叉验证，评分标准为'accuracy'.使用训练集(xtrain,ytrain)训练网格搜索模型，打印出最佳参数和最佳模型estimator.并采样最佳estimator创建模型new_model\nrom sklearn.ensemble import GradientBoostingClassifier\nGradientBoostingClassifier()\nparam_grid = {'n_estimators':[5,50,250,500],'max_depth':[1,3,5,7,9],'learning_rate':[0.01,0.1,1,10,100]}\ngrid = GridSearchCV(estimator=GradientBoostingClassifier(),param_grid=param_grid)  #默认为5折\ngrid.fit(x_train,y_train)\nprint(grid.best_params_) #打印最佳参数\nprint(grid.best_estimator_) #打印最佳estimator\nnew_model = grid.best_estimator_\nnew_model.fit(x_train,y_train)\nprint(accuracy_score(y_test,new_model.predict(x_test)))\n#4.初始化StackingCVClassfier,要求至少定义3个基础分类器，LogisticRegression作为元分类器，使用3折交叉验证，要求StackingCVClassfier的得分accuracy score至少大于0.79。accuracy score打印输出如下：\nfrom mlxtend.classifier import StackingCVClassifier\nsclf = StackingCVClassifier(classifiers=[LogisticRegression(),DecisionTreeClassifier(),SVC()],meta_classifier=LogisticRegression(),cv=3,use_features_in_secondary=True)\nsclf.fit(x_train,y_train)\nprint('accuracy score: {}'.format(accuracy_score(y_test,sclf.predict(x_test))))"
    },
    {
        "title": "分类2，Chicago_Crimes.csv",
        "content": "%matplotlib inline\n#读取数据\ndf = pd.read_csv('./第一套题数据/data/Chicago_Crimes.csv',error_bad_lines=False)\n#随机抽样，抽取1%\ndf_sample = df.sample(frac=0.01)\ndel df_sample['IUCR']\ndel df_sample['Case Number']\ndel df_sample['ID']\ndel df_sample['FBI Code']\ndel df_sample['Updated On']\ndel df_sample['Arrest']\ndel df_sample['Domestic']\ndel df_sample['Unnamed: 0']\ndel df_sample['Latitude']\ndf_sample.info()\ndf_sample.isnull().sum()\ndf_sample.describe(include='O')\ndf_na = pd.DataFrame(data=df_sample.isnull().sum()/df.shape[0],columns=['miss_rate']).sort_values(by='miss_rate',ascending=False)\ndf_sample.dropna(inplace=True)\ndf_sample.isnull().sum()\n#1.先将字段Date转为datetime类型，再扩展字段，提取年、月、周、日、小时信息。同时删除Date字段\ndf_sample['Date'] = df_sample['Date'].astype(np.datetime64)\ndf_sample['year'] = df_sample['Date'].dt.r\ndf_sample['month'] = df_sample['Date'].dt.month\ndf_sample['week'] = df_sample['Date'].dt.weekday   \ndf_sample['day'] = df_sample['Date'].dt.day\ndf_sample['hour'] = df_sample['Date'].dt.hour\n\n#删除Date列\ndel df_sample['Date']\n#2.字符串类型字段'Block','Primary Type','Description','Location Description','Location',在进行数据分析之前需要数值化，提高运行效率。factorize 函数可以将字符串类型数据映射为一组数字，相同的字符串类型映射为想通的数字。\nlist_col = ['Block','Description','Location Description','Location']\nfor col in list_col:\n    df_sample[col]= pd.factorize(df_sample[col])[0]\n#由考生填写\n#如果考题中要求把'Primary Type列名改成'Primary_Type,那么可以拿出来单独处理\ndf_sample['Primary_Type'] = pd.factorize(df_sample['Primary Type'])[0]\ndel df_sample['Primary Type']\nfrom sklearn.preprocessing import MinMaxScaler\ndf_sample['X Coordinate'] = df_sample['X Coordinate'].astype(float)\ndf_sample['Y Coordinate'] = df_sample['Y Coordinate'].astype(float)\ndf_sample['X Coordinate'] = MinMaxScaler().fit_transform(df_sample['X Coordinate'].values.reshape(-1,1))\ndf_sample['Y Coordinate'] = MinMaxScaler().fit_transform(df_sample['Y Coordinate'].values.reshape(-1,1))\n\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(df_sample.loc[:,df_sample.columns!='Primary_Type'], df_sample['Primary_Type'], test_size=0.3, random_state=42)\n#3.使用GradientBoostingClassifier分类器进行训练模型model_gbdt\nfrom sklearn.ensemble import GradientBoostingClassifier\nfrom sklearn.metrics import f1_score\n#由考生填写\nmodel_gbdt = GradientBoostingClassifier(n_estimators=8)  #设置参数， 如果题目没指定就用默认的，不用填\nmodel_gbdt.fit(X=X_train,y=y_train)\ny_prel = model_gbdt.predict(X_test)\nf1_score1 = f1_score(y_test,y_prel,average='micro')\nprint('f1_score为{}'.format(f1_score1))\n#4.使用网格搜索交叉验证对模型model_gbdt进行优化，调整参数learn_rate建议值为[0.1,0.2,0.3,0.4,0.5],cv采用5折进行模型训练，得到最优模型、最优参数和最优评分\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.metrics import make_scorer\n\nparam_grids = {'learning_rate':[0.1,0.2,0.3,0.4,0.5]}\nmodel_gs = GridSearchCV(estimator=model_gbdt,param_grid=param_grids,cv=5,scoring=make_scorer(f1_score))\nmodel_gs.fit(X=X_train,y=y_train)\n# 最优模型\nmodel_gs.best_estimator_\n# #最优参数\nmodel_gs.best_params_\n# # 最优评分\nmodel_gs.best_score_\n#5.使用VotingClassifier聚合了多个基础模型的预测结果。通过硬投票，软投票和自定义权重的软投票三种方式进行比较，确定最后的结果\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.ensemble import VotingClassifier\nfrom sklearn.ensemble import AdaBoostClassifier\nfrom xgboost import XGBClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nimport warnings\nwarnings.filterwarnings('ignore')\n\nclf1 = XGBClassifier(learning_rate=0.1,n_estimators=150,max_depth=3,min_child_weight=2,subsample=0.7,colsample_bytree=0.6,objective='binary:logistic')\nclf2 = RandomForestClassifier(n_estimators=50,max_depth=1,min_samples_split=4,min_samples_leaf=63,oob_score=True)\nclf3 = SVC(C=0.1,probability=True)  #软投票的时候,probability必须指定且为True\n\nclf = VotingClassifier(estimators=[('xgb',clf1),('rf',clf2),('svc',clf3)],voting='hard')\nclf\n#硬投票\neclf = VotingClassifier(estimators=[('xgb',clf1),('rf',clf2),('svc',clf3)],voting='hard')\nX_train.info()\nindex = 0\nfor clf,label in zip([clf1,clf2,clf3,eclf],['XGBBoosting','Random Forest','SVM','Voting']):\n        scores = cross_val_score(clf,X_train,y_train,cv=5,scoring='accuracy')\n        print(\"Accuracy:%0.2f(+/- %0.2f) [%s]\"%(scores.mean(),scores.std(),label))\n#软投票只需要设置voting='soft'即可\neclf = VotingClassifier(estimators=[('xgb',clf1),('rf',clf2),('svc',clf3)],voting='soft')\nX_train.info()\nfor clf,label in zip([clf1,clf2,clf3,eclf],['XGBBoosting','Random Forest','SVM','Voting']):\n    scores = cross_val_score(clf,X_train,y_train,cv=5,scoring='accuracy')\n    print(\"Accuracy:%0.2f(+/- %0.2f) [%s]\" %(scores.mean(),scores.std(),label))\n#软投票自定义权重\neclf = VotingClassifier(estimators=[('xgb',clf1),('rf',clf2),('svc',clf3)],voting='soft',weights=[0,1,9])\nX_train.info()\nfor clf,label in zip([clf1,clf2,clf3,eclf],['XGBBoosting','Random Forest','SVM','Voting']):\n    scores = cross_val_score(clf,X_train,y_train,cv=5,scoring='accuracy')\n    print(\"Accuracy:%0.2f(+/- %0.2f) [%s]\" %(scores.mean(),scores.std(),label))\n"
    },
    {
        "title": "聚类1，data_cluster.csv,MinMaxScaler",
        "content": "df = pd.read_csv('./data/data_cluster.csv',encoding='gbk',header=0)\n#1.使用MinMaxScaler函数对数据进行归一化处理，将数据存储为df_norm.\nfrom sklearn.preprocessing import MinMaxScaler\nmodel = MinMaxScaler()\ndf_norm = model.fit_transform(df)\ndf_norm = pd.DataFrame(df_norm,columns=df.columns)\n#2.使用SpecteralClustering算法选择K值2-12对数据进行聚类分析，使用轮廓系数进行评分，将轮廓系数和K值存储在列表k和silhouette_s 中，并绘制折线图\nfrom sklearn.cluster import SpectralClustering\nfrom sklearn.metrics import silhouette_score \nk,silhouette_s = [],[]\nfor i in range(2,13):\n    sc_model = SpectralClustering(n_clusters=i)    #这里本地运行很慢，在云环境就好很多\n    sc_model.fit(X=df_norm)\n    k.append(i)\n    print(i)\n    s = silhouette_score(X=df_norm,labels=sc_model.labels_)\n    silhouette_s.append(s)\nimport matplotlib.pyplot as plt\nplt.figure()\nplt.rc('font',family='YouYuan')   #这个记得加上\nplt.title('SpectralClustering')\nplt.xlabel(\"聚类的簇数\")\nplt.ylabel('silhouette_score')\nplt.plot(k,silhouette_s)\nplt.show()\n"
    },
    {
        "title": "聚类2，data_cluster.csv,sample()",
        "content": "import seaborn as sns\nfrom sklearn import metrics\nsns.set()\npd.options.display.max_columns = None\nimport warnings \nwarnings.filterwarnings('ignore')\norigin_data = pd.read_csv('./data/data_cluster.csv')\n#1.使用sample()方法从origin_data中随机抽取10000个样本，并允许重复抽样。重置索引，使得输出data为以下格式\ndata = origin_data.sample(n=10000)\ndata.reset_index()  #重置索引\ndf_cluster = data.copy()\n#2.从df_cluster中选择所有np.number类型的列，并将其保存在df_X中\ndf_X = df_cluster.select_dtypes(include=np.number)\ndf_X\ncols = df_X.columns\ncols\nfrom sklearn.preprocessing import MinMaxScaler\nscaler = MinMaxScaler()\nsclaed = pd.DataFrame(scaler.fit_transform(df_X),columns=cols)\nsclaed.head()\nfrom sklearn.cluster import KMeans\nk,silhouette,sse = [],[],[]\n#3.使用K-means模型，设置聚类数为i,初始算法为k-means++,最大迭代次数为500，初始化次数为10，随机种子为0，并将模型拟合到scaled数据上，计算使用欧几里得距离度量标准的轮廓系数s1和模型的质心间距离的总和s2。\nfrom sklearn.metrics import silhouette_score\nfor i in range(2,20):\n    kmeans_model = KMeans(n_clusters=i,init='k-means++',max_iter=500,n_init=10,random_state=0)\n    kmeans_model.fit(sclaed)\n    result = kmeans_model.labels_\n    k.append(i)\n    s1 = silhouette_score(sclaed,result)\n    ss = kmeans_model.inertia_ \n    sse.append(ss)\n    silhouette.append(s1)\n    print('K=',i,'Silhouette=',s1,'SSE=',ss)\nprint(len(silhouette),len(range(2,20)))\nplt.figure()\nplt.title('K-means silhouette')\n# plt.plot(range(2,20),silhouette,'*')  ##第三个参数，'o'代表折线的每一处拐角处用什么符号标记，o代表用原点，*代表用星号好表\nplt.plot(k,silhouette,'*')\n# plt.plot(range(2,20),silhouette,'-',alpha=0.5)\nplt.plot(k,silhouette,'-',alpha=0.5)\nplt.xlabel('Number of Cluster')\nplt.ylabel('silhouette')\nplt.show()\n\nplt.figure()\nplt.title('K-means SSE')\nplt.plot(k,sse)\nplt.plot(k,sse,'o')  #第三个参数，'o'代表折线的每一处拐角处用什么符号标记，o代表用原点，*代表用星号好表\nplt.xlabel('Number of cluster')\nplt.ylabel('SSE')\nplt.show()\n"
    },
    {
        "title": "回归student-info.csv",
        "content": "import matplotlib.pyplot as plt\nimport seaborn as sns\ndf = pd.read_csv('./data/student-info.csv')\n#1.查看年龄和性别的分布柱状图，输出如下：\nsns.countplot(x='age',hue='sex',data=df)\nplt.xlabel('age')\nplt.ylabel('sex')\nplt.title('Distribution bar chart of age and gender')\nplt.show()\n#2.查看年龄和成绩分布箱线图，输出如下：\nsns.boxplot(x='age',y='G3',data=df)\nplt.xlabel('age')\nplt.ylabel('G3')\nplt.title('Box plot of age and grade distribution')\nplt.show()\n#3.使用swarmplot函数查看年龄和成绩分布图：输出如下\nsns.swarmplot(x='age',y='G3',data=df)\nplt.xlabel('age')\nplt.ylabel('G3')\nplt.title('Age and grade distribution chart')\nplt.show()\n\ndel df['G1']\ndel df['G2']\ndf_oh = pd.get_dummies(df)\n#4.分析数据相关性系数，并取出和G3相关性最高的9个属性\ncro = df_oh.corr()\na = cro['G3'].sort_values(ascending=True)[:9]\nfrom sklearn.model_selection import train_test_split\nX = df_oh.loc[:,df_oh.columns != 'G3']\ny = df_oh['G3']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)\n#5.使用lightBGM算法进行建模，并使用网格搜索对模型进行cv使用5，输出模型评分的mae,rmse,abs\nfrom lightgbm import LGBMRegressor\n# 配置最优模型参数的模型\nmodel = LGBMRegressor()\nmodel.fit(X=X_train,y=y_train)\n# 使用偏差进行预测\nfrom sklearn.metrics import mean_absolute_error,mean_squared_log_error,mean_squared_error\nfrom sklearn.metrics import make_scorer\nprint(mean_absolute_error(y_test,model.predict(X_test)))\nprint(mean_squared_log_error(y_test,model.predict(X_test)))\ny_pred = model.predict(X_test)\nprint(np.median(np.abs(y_test - y_pred)/y_pred))\n\nfrom sklearn.model_selection import GridSearchCV\nparam_grid = {'learning_rate':[0.1,0.2,0.3,0.4,0.5]}\nmodel_gs = GridSearchCV(estimator=model,param_grid=param_grid,cv=5,scoring=make_scorer(mean_absolute_error))\nmodel_gs.fit(X_train,y_train)\nmodel_gs.best_score_\n#6.使用Voting算法聚合RandomForestRegressor,GradientBoostingRegressor,LinearRegression模型，预测成绩G3\nfrom sklearn.ensemble import VotingRegressor\nfrom sklearn.ensemble import RandomForestRegressor,GradientBoostingRegressor\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.metrics import r2_score\nreg1 = RandomForestRegressor(random_state=1)\nreg1.fit(X_train,y_train)\nprint('rf R2:',r2_score(y_test,reg1.predict(X_test)))\nreg2 = GradientBoostingRegressor(random_state=1)\nreg1.fit(X_train,y_train)\nprint('gbdt R2:',r2_score(y_test,reg1.predict(X_test)))\nreg3 = LinearRegression()\nreg3.fit(X_train,y_train)\nprint('lr R2:',r2_score(y_test,reg1.predict(X_test)))\nfrom sklearn.ensemble import VotingClassifier\nfrom mlxtend.classifier import StackingCVClassifier\nVotingClassifier()\nereg = VotingRegressor([('gb',reg1),('rf',reg2),('lr',reg3)],weights=[1,5,10])\nereg.fit(X_train,y_train)\nr2_score(y_test,ereg.predict(X_test))\npre_ereg = ereg.predict(X_test)\nprint(\"VotingRegressor R2:\",r2_score(y_test,pre_ereg))\nprint(pre_ereg)\n\n"
    },
    {
        "title": "特征train_forest_covertype.csv",
        "content": "import warnings\nwarnings.filterwarnings('ignore')\nimport numpy as np\nimport pandas as pd\nimport time\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom subprocess import check_output \nfrom lightgbm import LGBMClassifier\nimport lightgbm as lgb\nfrom tqdm import tqdm_notebook as tqdm\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import confusion_matrix,log_loss\n#!pip install xgboost\nfrom xgboost import XGBClassifier\nfrom lightgbm import LGBMClassifier\nfrom sklearn.pipeline import Pipeline\n#读取数据\ndf_forest = pd.read_csv('./data/train_forest_covertype.csv')\n\n#随机采样15120条数据\ndf_forest_sample = df_forest.sample(n=15120)\n\n# 删除Soil_Type7和Soil_Type15\ndf_forest_sample.drop(['Soil_Type7','Soil_Type15'],axis=1,inplace=True)\n\nXGB = XGBClassifier()\nlgbm = LGBMClassifier()\nfirst_models = [XGB,lgbm]\nfirst_model_names = ['XGB','lgbm']\nseed = 42\nskf = 5\n\n#1.ShuffleSplit函数进行切分，指定参数plitting iterations为skf,test_size为0.3，random_state为seed\nfrom sklearn.model_selection import ShuffleSplit\nfrom sklearn.preprocessing import StandardScaler\nn_folds = 5\nShuffleSplit(n_splits=skf, random_state=seed, test_size=0.3, train_size=0.6)  #切分数据\nstd_sca = StandardScaler()\nX = df_forest_sample.drop(columns=['Cover_Type'])\ny = pd.factorize(df_forest_sample['Cover_Type'])[0]\n\nMLA_columns = ['MLA Name','MLA Parameters','MLA Train Accuracy Mean','MLA Test Accuracy Mean','MLA Time']\nMLA_compare = pd.DataFrame(columns=MLA_columns)\n#create table to compare MLA predictions\nMLA_predict = df_forest_sample[['Id']]\ntrain_size = X.shape[0]\nn_models = len(first_models)\noof_pred = np.zeros((train_size,n_models))\nscores = []\nrow_index = 0\n\n#2.使用Pipline进行模型训练set中指定标准为('Scaler',std_sca),模型为('Estimator',model)3.使用cross_validate函数对模型进行评分，模型使用model,数据集使用X，y,指定return_train_score为True\nfrom sklearn.model_selection import cross_validate\ncross_validate()\nfrom sklearn.model_selection import cross_validate\nfor n,model in enumerate(first_models):\n    model_pipeline = Pipeline(steps=[('Scaler', StandardScaler()), ('Estimator', model)])\n    MLA_name = model.__class__.__name__\n    MLA_compare.loc[row_index,'MLA_name'] = MLA_name\n    MLA_compare.loc[row_index,'MLA Parameters'] = str(model_pipeline.get_params())\n    cv_result = cross_validate(estimator=model,X=X,y=y,cv=skf,return_train_score=True)\n    MLA_compare.loc[row_index,'MLA Time'] = cv_result['fit_time'].mean()\n    MLA_compare.loc[row_index,'MLA Train Accuracy Mean'] = cv_result['train_score'].mean()\n    MLA_compare.loc[row_index,'MLA Test Accuracy Mean'] = cv_result['test_score'].mean()\n    model_pipeline.fit(X,y)\n    MLA_predict[MLA_name] = model_pipeline.predict(X)\n    row_index+=1\n\nMLA_compare\n\ncv_result = cross_validate(estimator=first_models[0],X=X,y=y,cv=skf,return_train_score=True)\n\n#4.使用sort_values对按照集 MLA Test Accuracy Mean这一列倒序排序，指定inplace为True，覆盖原本数据\nMLA_compare.sort_values(by=['MLA Test Accuracy Mean'],ascending=True,inplace=True)\nMLA_compare\nMLA_compare.index[-20:-1]\n\n#5.删除MLA_compare后20位数据\nMLA_compare.drop(axis=0,index=MLA_compare.index[-20:-1])"
    }
]
    results = []
    for entry in json_data:
        if key.lower() in entry['content'].lower():
            results.append({'title': entry['title'], 'content': entry['content']})
    df = pd.DataFrame(results)
    df.style.set_properties(**{'white-space': 'pre-wrap', 'text-align': 'left'})
def d_all():
    json_data=[
    {
        "title": "关联规则1fruit",
        "content": "df = pd.read_csv('./data/fruit.csv',sep=';')\n#1.以“;”为分割符读取数据中的ID和fruit列，将fruit列通过lambda函数，以逗号为分割符进行切分并去除数据集中的空格，生成矩阵 df_mtix\ndf_mtix = df['fruit'].map(lambda x:x.replace(\" \",'').split(','))\n#2.使用TransactionEncoder函数对矩阵进行拟合df_mtix,生成数据df_te。将数据df_te转换成DataFrame,使用df_te.columns为列名\nfrom mlxtend.preprocessing import TransactionEncoder\nte = TransactionEncoder()\ndf_te = te.fit_transform(df_mtix)\ndf_te = pd.DataFrame(data=df_te,columns=te.columns_)\n#3.使用fpgrowth算法建模发现数据中的关联规则，指定min_support=0.5,use_colname=True,使用mlxtend库中的assocication_rules 函数来找出关联规则。指定metric为'confidence',min_threshold为0.3\nfrom mlxtend.frequent_patterns import fpgrowth\nfrom mlxtend.frequent_patterns import association_rules\nmodel = fpgrowth(df_te,min_support=0.5,use_colnames=True)\nrules = association_rules(model,metric='confidence',min_threshold=0.3)\n#4.输出关联规则rules中，前验置信度>0.6,置信度大于0.5的项集\nrules[(rules['antecedent support']>0.6) & (rules['support']>0.5)]\n#5.输出关联规则rules中，antecedents为{苹果,香蕉}的内容\nrules[rules['antecedents'] == {'apple','banana'}]"
    },
    {
        "title": "关联规则2transactions",
        "content": "df = pd.read_csv('./data/transactions.csv')\n#1.对df中的items列应用lambda函数，将每行中的值按照逗号分割的字符串拆分成列表，并去掉空格，将结果存储在transactions列表中\ntransactions = []\ntransactions = df['items'].map(lambda x:x.replace(\" \",'').split(','))\n#2.使用TransctionEncoder将数transactions转换为合适的Apriori算法的格式数据te_data,使用Pandas的DataFrame()函数将te_data转换为DataFrame对象df\nfrom mlxtend.preprocessing import TransactionEncoder\nte = TransactionEncoder()\nte_data = te.fit_transform(transactions)\ndf = pd.DataFrame(te_data,columns=te.columns_)\nfrom mlxtend.frequent_patterns import fpgrowth\nfrequent_items = fpgrowth(df,min_support=0.3,use_colnames=True)\n#3在频繁项集frequent_items中构建关联规则rules,度量指标为confidence,最小阀值为0.5\nfrom mlxtend.frequent_patterns import association_rules\nrules = association_rules(frequent_items,metric='confidence',min_threshold=0.5)\n#4.打印同时满足先验支持度>=0.6,支持度>0.3,提升度>0.8的rules数据\nrules[(rules['antecedent support']>=0.6) & (rules['support']>=0.3) & (rules['lift']>=0.8)]\n#5.筛选规则rules,将规则中同时满足antecedents_sale为Banana,consequents_sale为Apple规则选出来，并存储在final_sale中，使得rules.loc[fina_sale]输出\nantecedents_sale = rules['antecedents'] == frozenset({'Banana'})  #返回true和false的集合\nconsequents_sale = rules['consequents'] == frozenset({'Apple'})\nfinal_sale = (antecedents_sale&consequents_sale)\n#考试的时候要注意这里，考题有可能是删除，也有可能是筛选\n# as_rule.loc[~final_sale]  #加了波浪符号或者负号表示筛选出相反的数据，也就是选择不同时含有上述条件的选项 ，即删除以上条件的数据\nrules.loc[final_sale]  #不加了波浪符号或者负号表示筛选出以上条件的数据   "
    },
    {
        "title": "关联规则3GoodsOrder_eng.csv",
        "content": "pip install mlxtend\nimport numpy as np \nimport pandas as pd\nfrom mlxtend.frequent_patterns import apriori,fpgrowth,association_rules\nimport warnings \nwarnings.filterwarnings('ignore')\norder_data = pd.read_csv('./data/GoodsOrder_eng.csv',sep=',',header=0,encoding='gbk')\n#转换数据格式\norder_data['Goods'] = order_data['Goods'].apply(lambda x : '' + x)\n#将order_data按id分组求和，并重置索引\ndata = order_data.groupby(by='id',as_index=False)['Goods'].sum().reset_index()\ndata['Goods'] = data['Goods'].apply(lambda x:[x[1:]])\ndata_list = list(data['Goods'])\n#2.分割商品名为每一个元素。使得dataset最终输出如以下格式：\ndata_trans = []\nfor i in data_list:\n    p = i[0].split(',')\n    data_trans.append(p)\ndata_set = data_trans\ncolumn_list = []\nfor var in data_set:\n    column_list = set(column_list)|set(var)\n#3.遍历data_set中的每一个商品，并将data中对应位置的值加1，即购买一次则在相应物品上加1，使得data输出为以下形式：\ndata = pd.DataFrame(np.zeros((len(data_set),6)),columns=list(column_list))\nfor i in range(len(data_set)):\n    for j in data_set[i]:\n        data[j][i]+=1\ndata = data.applymap(lambda x:1 if x > 0 else 0)\n#4.使用Apriori算法从数据中计算频繁项集，并将最小支持度设置为0.02.然后根据支持度倒排序，最后返回频繁项集frequent_itemts,使得frequent_items部分输出为：\nfrom mlxtend.frequent_patterns import apriori\nfrom mlxtend.frequent_patterns import fpgrowth\nfrequent_itemts = apriori(data,min_support=0.02,use_colnames=True)\nfrequent_itemts = frequent_itemts.sort_values(by='support',ascending=False)\nfrequent_itemts\n#5.使用association_rules从频繁项集frequent_items中构建关联规则，metric为'confidence',min_threshold=0.35。对关联规则按confidence值进行降序排列，并将排序结果存储在association_rule中，使得df_association_rule部分输出为:\nfrom mlxtend.preprocessing import TransactionEncoder\nfrom mlxtend.frequent_patterns import association_rules\nassociation_rule = association_rules(frequent_itemts,metric='confidence',min_threshold=0.35)\nassociation_rule.sort_values(by='confidence',ascending=False,inplace=True)\nassociation_rule\n"
    },
    {
        "title": "pyspark2,creditcard",
        "content": "from pyspark.context import SparkContext\nfrom pyspark.sql import SparkSession\nsc = SparkContext('local','test')\nfrom pyspark.sql.functions import explode\nfrom pyspark.sql.functions import split\nspark = SparkSession(sc)\nsc\ndata_O = spark.read.load('./data/creditcard.csv',format='csv',header='true',inferSchema='true')\ndata_O.show(1)\n#1.data_O按Class列分组统计每个类别的个数，输出为classFreq,打印classFreq输出为\nclassFreq = data_O.groupBy('Class').count()\nclassFreq.show()\ndata = data_O.toPandas()\ndata = data.sample(frac=1)\n#欺诈样本492条\nfraud_df = data.loc[data['Class']==1]\nfraud_df   #492*31\nnon_fraud_df = data.loc[data['Class']==0][:492]\nnormal_distributed_df = pd.concat([fraud_df,non_fraud_df])\nnormal_distributed_df\nnew_df = normal_distributed_df.sample(frac=1,random_state=42)\nnew_df.shape\nimport seaborn as sns\nimport matplotlib.pyplot as plt\ncolors = ['#BF9C5','#f9c5b3']\n#2.以Class为X轴，V10为Y轴，绘制出箱线图，用于查看new_df数据集中V10变量在不同班级之间的分布情况。输出如下：\nV10_sns = sns.boxplot(x='Class',y='V10',data=new_df)\nV10_sns.set_title('V10 vs Class Negative Correlation')\nplt.show()\ndfff = spark.createDataFrame(new_df)\nfrom pyspark.sql.functions import *\nfrom pyspark.sql.window import Window\n#3.运用withColumn将dfff添加一列idx,该列是按照窗口win中的Time字段排序后的行号row_number.\nwin = Window().orderBy('Time')\ndfff = dfff.withColumn('idx',row_number().over(win))\ndfff.show(1)\nfrom pyspark.ml import Pipeline\nfrom pyspark.ml.classification import GBTClassifier\nfrom pyspark.ml.feature import VectorIndexer,VectorAssembler\nfrom pyspark.ml.evaluation import BinaryClassificationEvaluator\nfrom pyspark.ml.linalg import DenseVector\n#4.将dfff中的rdd映射为元组，元组中只有一个元素DenseVector,DenseVector的第一个值由dfff的前30列组成，第二个值为dfff的第31列，第三个值为dfff的第32列\ntraining_df = dfff.rdd.map(lambda x:(DenseVector(x[0:29]),x[30],x[31]))\ntraining_df = spark.createDataFrame(training_df,[\"features\",\"label\",\"index\"])\ntraining_df.show(1)\ntraining_df = training_df.select('index','features','label')\ntraining_df.show(1)\ntrain_data,test_data = training_df.randomSplit([.8,.2],seed=1234)\ngbt = GBTClassifier(featuresCol='features',maxIter=100,maxDepth=8)\nmodel = gbt.fit(train_data)\npredictions = model.transform(test_data)\nevaluator = BinaryClassificationEvaluator()\nevaluator.evaluate(predictions)\n混淆矩阵： P(正例/正元组)：感兴趣的类别 N(负例/负元组)：不感兴趣的类别 TP真正例：原本是正例也被预测为正例 TN真负例：原本为负例也被预测为负例 FP假正例：原本为负例被预测为正例 FN假负例：原本为正例被预测为负例 分类的偏差考察角度： 准确率(精度)：(TP+TN)/P+N 错误率(汉明损失)：(FN+FP)/P+N 查准率：TP/(TP+FP),主要关注对负例的预测是否精准 查全率：TP/(TP+FN),主要关注对正例的预测是否精准，也叫召回率 F1值(常用)：F1值是查准率和查全率的一个调和平均值。其综合考虑了查准率和查全率\ntp = predictions[(predictions.label == 1) & (predictions.prediction == 1)].count()\ntn = predictions[(predictions.label == 0) & (predictions.prediction == 0)].count()\nfp = predictions[(predictions.label == 0) & (predictions.prediction == 1)].count()\nfn = predictions[(predictions.label == 1) & (predictions.prediction == 0)].count()\n#5.打印召回率和精确率\nprint(\"Recall:\",tp/(tp+fn))\nprint(\"Precision:\",tp/(tp+fp))"
    },
    {
        "title": "pyspark3,houses_data",
        "content": "from pyspark.context import SparkContext\nfrom pyspark.sql import SparkSession\nsc = SparkContext('local','test')\nspark = SparkSession(sc)\nsc\nfrom pyspark.sql.types import *\nfrom pyspark.sql import Row\nhouses_data = spark.read.format('org.apache.spark.sql.execution.datasources.csv.CSVFileFormat')\\\n.option('header','true')\\\n.option('inferSchema','true')\\\n.load('./data/houses_data.csv')\nrdd = sc.textFile('./data/houses_data.csv')\nhouses_data.show(5)\nprint(rdd.take(5))\n#1.对数据集的每一行用逗号进行分割\nrdd = rdd.map(lambda x:x.split(','))\nrdd.take(2)\nheader = rdd.first()\nheader\n#2.使用‘filter’删除包含标题的行\nrdd = rdd.filter(lambda x:x!=header)\nrdd.take(2)\ndf = rdd.map(lambda line:Row(street=line[0],city=line[1],zip=line[2],beds=line[4],baths=line[5],sqft=line[6],price=line[9])).toDF()\ndf.show(5)\ndf.toPandas().head()\n#3.df按照'beds'字段分组，并计算每个分组中记录的数量，并显示结果\ndf.groupBy('beds').count().show()\ndf.describe(['baths','beds','price','sqft']).show()\nimport pyspark.mllib\nimport pyspark.mllib.regression\nfrom pyspark.mllib.regression import LabeledPoint\nfrom pyspark.sql.functions import  *\ndf = df.select('price','baths','beds','sqft')\ndf = df[df.baths>0]\ndf = df[df.beds>0]\ndf = df[df.sqft>0]\ndf.show(5)\ndf.describe(['baths','beds','price','sqft']).show()\ntemp = df.rdd.map(lambda line:LabeledPoint(line[0],[line[1:]]))\ntemp.take(5)\nfrom pyspark.mllib.util import MLUtils\nfrom pyspark.mllib.linalg import Vectors\nfrom pyspark.mllib.feature import StandardScaler\nfeatures = df.rdd.map(lambda row:row[1:])\nfeatures.take(5)\nstandardizer = StandardScaler()\nmodel = standardizer.fit(features)\nfeature_transform = model.transform(features)\nfeature_transform.take(5)\nlab = df.rdd.map(lambda row:row[0])\nlab.take(5)\n#4.将标签lab和特征feature_transform进行zip操作\ntransformedData = lab.zip(feature_transform)  \ntransformedData.take(5)\n#5.将transformedData转换为LabeledPoint类型，其中row[0]作为标签，row[1]作为特征向量\ntransformedData = transformedData.map(lambda row:LabeledPoint(row[0],row[1]))\ntransformedData.take(5)\ntrainingData,testingData = transformedData.randomSplit([.8,.2],seed=1234)\nfrom pyspark.mllib.regression import LinearRegressionWithSGD\nlinearModel = LinearRegressionWithSGD.train(trainingData,1000,.2)\nlinearModel.weights\ntestingData.take(10)\nlinearModel.predict([1.49297445326,3.52055958053,1.73535287287])"
    },
    {
        "title": "pyspark1,iris",
        "content": "import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom sklearn.datasets import load_iris\nfrom pyspark.conf import SparkConf\nfrom pyspark.ml.feature import VectorAssembler,StandardScaler,PCA\nfrom pyspark.context import SparkContext\nfrom pyspark.sql import SparkSession\nsc = SparkContext('local','test')\nspark = SparkSession(sc)\nsc\niris = load_iris()\nX = iris['data']\ny = iris['target']\ndata = pd.DataFrame(X,columns=iris.feature_names)\n#1.将Pandas数据框转换为Spark数据框dataset,并将其列名设置为iris特征名称，使得dataset输出为：\ndataset = spark.createDataFrame(data)\ndataset.show(6)\n#2.使用VectorAssembler将dataset多列数据转化为单列的向量‘features’,使得df输出为：\ncol = ['sepal length (cm)','sepal width (cm)','petal length (cm)','petal width (cm)']\nmodel_ve = VectorAssembler(inputCols=col, outputCol=\"features\")\ndf = model_ve.transform(dataset).select('features')\ndf.show(6)\n#3.创建一个StandarScaler对象，命名为scaler,用来对特征列‘features’进行标准化，将标准化后的结果输出到新的特征列'scaledFeatures',withMean和withStd分别为True,最后在df上进行fit操作\nscaler = StandardScaler(inputCol='features',outputCol='scaledFeatures',withMean=True,withStd=True).fit(df)\ndf_scaled = scaler.transform(df)\nn_components = 3\npca = PCA(k=n_components,inputCol='scaledFeatures',outputCol='pcaFeatures').fit(df_scaled)\ndf_pca = pca.transform(df_scaled)\nprint('Expained Variance Ratio',pca.explainedVariance.toArray())\ndf_pca.show(6)\ndf_pca.rdd.collect()[0:2]\nlist1 = df_pca.rdd.map(lambda x:(x[2][0],x[2][1],x[2][2])).collect()\ntype(list1[1][1])\ndf_origin = spark.createDataFrame([(float(tup[0]),float(tup[1]),float(tup[2])) for tup in list1],['x','y','z'])\ndf_origin.show(5)\n#4.将Spark数据框df_origin转换为Pandas数据框df_Pandas\ndf_Pandas = df_origin.toPandas()\ndf_Pandas.head()\ndf_Pandas['id'] = 'row' + df_Pandas.index.astype(str)\ncols = list(df_Pandas)\ncols\ncols.insert(0,cols.pop(cols.index('id')))\ndf_Pandas = df_Pandas.loc[:,cols]\ndf_Pandas.head(5)\npath = './data/input.csv'\n#5.将df_Pandas的数据保存到指定的path下，不保存index\ndf_Pandas.to_csv(path,index=False)\ndf = spark.read.csv(path,header=True)\ndf.show(5)"
    },
    {
        "title": "推荐1，BX-Book-Ratings",
        "content": "user_artists = pd.read_csv('./data/book/BX-Book-Ratings.csv',sep=';',encoding='latin-1',usecols=['User-ID','ISBN','Book-Rating'])\n#1.构建评分矩阵rating_matric,将缺失值填充为0\nrating_matric = user_artists.pivot_table(index='User-ID',columns='ISBN',values='Book-Rating')\nrating_matric.fillna(0,inplace=True)\nrating_matric.head(5)\n#2.使用cosine_similarity函数计算项目相似度矩阵\n#3.用户评分过的物品中获取与item相似的物品\nfrom sklearn.metrics.pairwise import cosine_similarity\nitem_similarity_matric = cosine_similarity(rating_matric.T)\n#将rating_matric 的索引转换为字符串\nrating_matric.index = rating_matric.index.map(str)\n#定义预测用户对书籍的评分函数\ndef predict_rating(user,item):\n    # 获取item的索引\n    item_index = rating_matric.columns.get_loc(item)\n    #获取item的相似度分数\n    similarity_scores = item_similarity_matric[item_index]\n    #获取用户的评分\n    user_ratings = rating_matric.loc[user]\n    #获取用户评分过的的物品\n    rated_items = user_ratings[user_ratings > 0].index\n    #在用户评分过的物品中获取与item相似的物品   ,由考生填写 .这里有点难理解\n    similar_items = [item for item in rated_items if item_similarity_matric[item_index][rating_matric.columns.get_loc(item)]>0]\n    similar_items_index = []\n    for i in range(len(similar_items)):\n        similar_items_index.append(rating_matric.columns.get_loc(similar_items[i]))\n        #如果没有相似的物品，则返回0\n    if len(similar_items) == 0:\n        return 0\n    #否则，计算用户对item的预测评分\n    else:\n        #a = np.array([1,2]),b = np.array([3,4])要求元素个数相同，相当于求内积，对应元素相乘再相加，“1*3 + 2*4 = 11”\n        return user_ratings[similar_items].dot(similarity_scores[similar_items_index])/similarity_scores[similar_items_index].sum()\n        #上面其实是求均值，也可以这么写\n        # return user_ratings[similar_items].mean()\n# 测试，计算用户276729对书籍0064401367的预测评分\nuser = '276729'\nitem = '0064401367'\npredict_rating = predict_rating(user,item)\nprint('预测用户{}对书籍{}的评分为:{}'.format(user,item,predict_rating))"
    },
    {
        "title": "推荐2，artists.dat",
        "content": "user_artists = pd.read_csv('./data/music/user_artists.dat',sep='\\t') #优用'\\t'分隔符读取文件\nartists = pd.read_csv('./data/music/artists.dat',sep='\\t',usecols=['id','name']) #仅读取id,name列\n#1.将user_artists和artist两个dataFrame进行合并，以user_artists中的artistID和artists中的id为键值进行连接\ndata = pd.merge(user_artists,artists,left_on='artistID',right_on='id')\ndata = data.drop(['id','artistID'],axis=1)    \ndata\n#2.去除评分次数少于50次的音乐和用户\ncounts = data['userID'].value_counts()   #计算每个用户的评分次数\ndata = data[data['userID'].isin(counts[counts >=50].index)]  #去除评分次数少于50次的用户\ndata\ncounts = data['name'].value_counts()   #计算每个音乐的评分次数\ndata = data[data['name'].isin(counts[counts >=50].index)]   #去除评分次数少于50次的音乐\ndata\n#将音乐和用户ID转换为数字\nname_to_index = {} #创建name_to_index字典\nindex_to_name = {} #创建index_to_name字典\nfor i,name in enumerate(data['name'].unique()):  #遍历data中的name列\n    name_to_index[name] = i   #将name映射到数字\n    index_to_name[i] = name   #将数字映射到name\n\nuser_id_to_index = {} #创建user_id_to_index字典\nindex_to_user_id = {} #创建index_to_user_id字典\nfor i,user_id in enumerate(data['userID'].unique()):  #遍历data中的name列\n    user_id_to_index[user_id] = i   #将user_id映射到数字\n    index_to_user_id[i] = user_id   #将数字映射到user_id\ndata['name_index'] = data['name'].apply(lambda x:name_to_index[x]) #将data中的name映射到数字\ndata['user_index'] = data['userID'].apply(lambda x:user_id_to_index[x]) #将data中的user_id映射到数字\n\n# 构建用户-音乐评分矩阵\nn_users = len(user_id_to_index)  #获取用户数量\nn_artists = len(name_to_index)   #获取音乐数量\nratings_matrix = np.zeros((n_users,n_artists))  #创建用户-音乐评分矩阵\n#3.将data中的评分填入用户-音乐评分矩阵，使得ratingsmatrix的输出为：\nfor row in data.itertuples(): #遍历data\n    ratings_matrix[row[5],row[4]] = row[1]  #将data中的评分填入用户-音乐评分矩阵\n#计算用户之间的相似度\nfrom sklearn.metrics.pairwise import cosine_similarity   #导入sklearn中的余弦相似度函数\nuser_simlarity = cosine_similarity(ratings_matrix)  #计算用户之间的相似度\nuser_simlarity\nuser_index = 0 #设置用户索引\n#4.获取与用户最相似的用户索引similar_users(不包括自己),使得similar_users的输出为：\n#获取与用户索引为0的用户最相似的用户的索引\n#index_u = np.argsort(-user_similarity[user_index])   #np.argsort：排序，但是返回的是数据的下标\n# 方式1\n# similar_users = np.argsort(-user_simlarity[user_index])[np.argsort(-user_simlarity[user_index]) != 0]\n# 方式2\nsimilar_users = np.argsort(-user_simlarity[user_index])[1:] #从1开始就是把0去掉，0是自己\n\nsimilar_users\nrecommended_artists = []   #创建推荐音乐列表\n#5.填补以下程序，获取相似用户评分的音乐索引artists_rated\nfor i in similar_users: #遍历相似用户索引\n#     np.where(rating_matrix[i]>0)返回的是一个元组，第一个值为下标索引的列表，第二个无值\n    artists_rated = np.where(ratings_matrix[i] > 0)[0] #获取相似用户评分的音乐,要选出相似用户听过的，也就是评分>0\n    for j in artists_rated:\n        if ratings_matrix[user_index][j] == 0: #用户未评分\n            recommended_artists.append((j,ratings_matrix[i][j])) #将音乐加入推荐音乐列表  (j,ratings_matrix[i][j]):(音乐,评分)\nrecommended_artists[0:10]\nrecommended_artists = sorted(recommended_artists,key=lambda x:x[1],reverse=True)[:10] #筛选出推荐音乐中的前10首\nartists['name'].values\nfor artist in recommended_artists: #遍历推荐音乐\n#     以下两种方式都行\n    print(artists[artists['name'] == index_to_name[artist[0]]]['name'].values[0])  #打印推荐音乐 values的结果为一个数组，但数组里面只有一个值\n    print(index_to_name[artist[0]])"
    },
    {
        "title": "分类1，pima-indians-diabetes.data",
        "content": "df = pd.read_csv('./data/pima-indians-diabetes.data',sep=',')\ndf['Outcome'].value_counts() #将数据集按照输入特征和目标特征进行划分，分别命名为X,y，且保证其类型为numpy.ndarry\nx_cols = [col for col in df.columns if col!='Outcome']\ny_col = 'Outcome'\nX = df[x_cols].values #将dataframe转化为ndarry，才能进入下面的标准化\ny = df[y_col].values\n#1.划分数据集X,y,分别命名为X_train,X_test,y_train,y_test,测试集比例为10%，固定随机数种子为42，打乱顺序，并以df[y_col]做分层抽样\n#横向拆分数据\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import  StandardScaler\nfrom collections import Counter\nx_train, x_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=42)\n#查看划分后的训练集和测试集中两类目标值的数量\nprint('Distribution of y train {}'.format(Counter(y_train)))\nprint('Distribution of y test {}'.format(Counter(y_test)))\n#标准化\nstd_scaler = StandardScaler().fit(X)\nx_train = std_scaler.fit_transform(x_train)\nx_test = std_scaler.fit_transform(x_test)\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.svm import  SVC\nfrom sklearn.linear_model import  LogisticRegression\nfrom sklearn.tree import  DecisionTreeClassifier\nfrom sklearn.naive_bayes import  GaussianNB\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.ensemble import GradientBoostingClassifier\nmodels = []\nmodels.append(('KNN',KNeighborsClassifier()))\nmodels.append(('SVC',SVC()))\nmodels.append(('LR',LogisticRegression()))\nmodels.append(('DT',DecisionTreeClassifier()))\nmodels.append(('GNB',GaussianNB()))\nmodels.append(('RF',RandomForestClassifier()))\nmodels.append(('GB',GradientBoostingClassifier()))\nnames = []\nscores = []\n#2.遍历models中的每个模型，分别使用训练集(xtrain,ytrain)进行训练，然后用测试集xtest进行预测，计算准确率accuracy score,并将模型名称name和准确率accuracy score分别存储在列表names和scores中，最后将列表names和scores转换成DataFrame格式，DataFrame命名为tr split,tr split打印输出结果如下：\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import make_scorer\nfrom sklearn.model_selection import cross_val_score\nfor name,model in models:\n    model.fit(x_train,y_train)\n    score = accuracy_score(y_test,model.predict(x_test))\n    names.append(name)\n    scores.append(score)\n# 方式1:\n# tr_split = pd.DataFrame(data={'name':names,'score':scores})\n# 方式2:\ntr_split = pd.DataFrame(data=list(zip(names,scores)),columns=['name','score'])\ntr_split\nfrom sklearn.model_selection import GridSearchCV\nc_values = list(np.arange(1,10))\n#3.使用GridSearchCV对GradientBoostingClassifier进行网格搜索，参数为param_prid,使用5折交叉验证，评分标准为'accuracy'.使用训练集(xtrain,ytrain)训练网格搜索模型，打印出最佳参数和最佳模型estimator.并采样最佳estimator创建模型new_model\nrom sklearn.ensemble import GradientBoostingClassifier\nGradientBoostingClassifier()\nparam_grid = {'n_estimators':[5,50,250,500],'max_depth':[1,3,5,7,9],'learning_rate':[0.01,0.1,1,10,100]}\ngrid = GridSearchCV(estimator=GradientBoostingClassifier(),param_grid=param_grid)  #默认为5折\ngrid.fit(x_train,y_train)\nprint(grid.best_params_) #打印最佳参数\nprint(grid.best_estimator_) #打印最佳estimator\nnew_model = grid.best_estimator_\nnew_model.fit(x_train,y_train)\nprint(accuracy_score(y_test,new_model.predict(x_test)))\n#4.初始化StackingCVClassfier,要求至少定义3个基础分类器，LogisticRegression作为元分类器，使用3折交叉验证，要求StackingCVClassfier的得分accuracy score至少大于0.79。accuracy score打印输出如下：\nfrom mlxtend.classifier import StackingCVClassifier\nsclf = StackingCVClassifier(classifiers=[LogisticRegression(),DecisionTreeClassifier(),SVC()],meta_classifier=LogisticRegression(),cv=3,use_features_in_secondary=True)\nsclf.fit(x_train,y_train)\nprint('accuracy score: {}'.format(accuracy_score(y_test,sclf.predict(x_test))))"
    },
    {
        "title": "分类2，Chicago_Crimes.csv",
        "content": "%matplotlib inline\n#读取数据\ndf = pd.read_csv('./第一套题数据/data/Chicago_Crimes.csv',error_bad_lines=False)\n#随机抽样，抽取1%\ndf_sample = df.sample(frac=0.01)\ndel df_sample['IUCR']\ndel df_sample['Case Number']\ndel df_sample['ID']\ndel df_sample['FBI Code']\ndel df_sample['Updated On']\ndel df_sample['Arrest']\ndel df_sample['Domestic']\ndel df_sample['Unnamed: 0']\ndel df_sample['Latitude']\ndf_sample.info()\ndf_sample.isnull().sum()\ndf_sample.describe(include='O')\ndf_na = pd.DataFrame(data=df_sample.isnull().sum()/df.shape[0],columns=['miss_rate']).sort_values(by='miss_rate',ascending=False)\ndf_sample.dropna(inplace=True)\ndf_sample.isnull().sum()\n#1.先将字段Date转为datetime类型，再扩展字段，提取年、月、周、日、小时信息。同时删除Date字段\ndf_sample['Date'] = df_sample['Date'].astype(np.datetime64)\ndf_sample['year'] = df_sample['Date'].dt.r\ndf_sample['month'] = df_sample['Date'].dt.month\ndf_sample['week'] = df_sample['Date'].dt.weekday   \ndf_sample['day'] = df_sample['Date'].dt.day\ndf_sample['hour'] = df_sample['Date'].dt.hour\n\n#删除Date列\ndel df_sample['Date']\n#2.字符串类型字段'Block','Primary Type','Description','Location Description','Location',在进行数据分析之前需要数值化，提高运行效率。factorize 函数可以将字符串类型数据映射为一组数字，相同的字符串类型映射为想通的数字。\nlist_col = ['Block','Description','Location Description','Location']\nfor col in list_col:\n    df_sample[col]= pd.factorize(df_sample[col])[0]\n#由考生填写\n#如果考题中要求把'Primary Type列名改成'Primary_Type,那么可以拿出来单独处理\ndf_sample['Primary_Type'] = pd.factorize(df_sample['Primary Type'])[0]\ndel df_sample['Primary Type']\nfrom sklearn.preprocessing import MinMaxScaler\ndf_sample['X Coordinate'] = df_sample['X Coordinate'].astype(float)\ndf_sample['Y Coordinate'] = df_sample['Y Coordinate'].astype(float)\ndf_sample['X Coordinate'] = MinMaxScaler().fit_transform(df_sample['X Coordinate'].values.reshape(-1,1))\ndf_sample['Y Coordinate'] = MinMaxScaler().fit_transform(df_sample['Y Coordinate'].values.reshape(-1,1))\n\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(df_sample.loc[:,df_sample.columns!='Primary_Type'], df_sample['Primary_Type'], test_size=0.3, random_state=42)\n#3.使用GradientBoostingClassifier分类器进行训练模型model_gbdt\nfrom sklearn.ensemble import GradientBoostingClassifier\nfrom sklearn.metrics import f1_score\n#由考生填写\nmodel_gbdt = GradientBoostingClassifier(n_estimators=8)  #设置参数， 如果题目没指定就用默认的，不用填\nmodel_gbdt.fit(X=X_train,y=y_train)\ny_prel = model_gbdt.predict(X_test)\nf1_score1 = f1_score(y_test,y_prel,average='micro')\nprint('f1_score为{}'.format(f1_score1))\n#4.使用网格搜索交叉验证对模型model_gbdt进行优化，调整参数learn_rate建议值为[0.1,0.2,0.3,0.4,0.5],cv采用5折进行模型训练，得到最优模型、最优参数和最优评分\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.metrics import make_scorer\n\nparam_grids = {'learning_rate':[0.1,0.2,0.3,0.4,0.5]}\nmodel_gs = GridSearchCV(estimator=model_gbdt,param_grid=param_grids,cv=5,scoring=make_scorer(f1_score))\nmodel_gs.fit(X=X_train,y=y_train)\n# 最优模型\nmodel_gs.best_estimator_\n# #最优参数\nmodel_gs.best_params_\n# # 最优评分\nmodel_gs.best_score_\n#5.使用VotingClassifier聚合了多个基础模型的预测结果。通过硬投票，软投票和自定义权重的软投票三种方式进行比较，确定最后的结果\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.ensemble import VotingClassifier\nfrom sklearn.ensemble import AdaBoostClassifier\nfrom xgboost import XGBClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nimport warnings\nwarnings.filterwarnings('ignore')\n\nclf1 = XGBClassifier(learning_rate=0.1,n_estimators=150,max_depth=3,min_child_weight=2,subsample=0.7,colsample_bytree=0.6,objective='binary:logistic')\nclf2 = RandomForestClassifier(n_estimators=50,max_depth=1,min_samples_split=4,min_samples_leaf=63,oob_score=True)\nclf3 = SVC(C=0.1,probability=True)  #软投票的时候,probability必须指定且为True\n\nclf = VotingClassifier(estimators=[('xgb',clf1),('rf',clf2),('svc',clf3)],voting='hard')\nclf\n#硬投票\neclf = VotingClassifier(estimators=[('xgb',clf1),('rf',clf2),('svc',clf3)],voting='hard')\nX_train.info()\nindex = 0\nfor clf,label in zip([clf1,clf2,clf3,eclf],['XGBBoosting','Random Forest','SVM','Voting']):\n        scores = cross_val_score(clf,X_train,y_train,cv=5,scoring='accuracy')\n        print(\"Accuracy:%0.2f(+/- %0.2f) [%s]\"%(scores.mean(),scores.std(),label))\n#软投票只需要设置voting='soft'即可\neclf = VotingClassifier(estimators=[('xgb',clf1),('rf',clf2),('svc',clf3)],voting='soft')\nX_train.info()\nfor clf,label in zip([clf1,clf2,clf3,eclf],['XGBBoosting','Random Forest','SVM','Voting']):\n    scores = cross_val_score(clf,X_train,y_train,cv=5,scoring='accuracy')\n    print(\"Accuracy:%0.2f(+/- %0.2f) [%s]\" %(scores.mean(),scores.std(),label))\n#软投票自定义权重\neclf = VotingClassifier(estimators=[('xgb',clf1),('rf',clf2),('svc',clf3)],voting='soft',weights=[0,1,9])\nX_train.info()\nfor clf,label in zip([clf1,clf2,clf3,eclf],['XGBBoosting','Random Forest','SVM','Voting']):\n    scores = cross_val_score(clf,X_train,y_train,cv=5,scoring='accuracy')\n    print(\"Accuracy:%0.2f(+/- %0.2f) [%s]\" %(scores.mean(),scores.std(),label))\n"
    },
    {
        "title": "聚类1，data_cluster.csv,MinMaxScaler",
        "content": "df = pd.read_csv('./data/data_cluster.csv',encoding='gbk',header=0)\n#1.使用MinMaxScaler函数对数据进行归一化处理，将数据存储为df_norm.\nfrom sklearn.preprocessing import MinMaxScaler\nmodel = MinMaxScaler()\ndf_norm = model.fit_transform(df)\ndf_norm = pd.DataFrame(df_norm,columns=df.columns)\n#2.使用SpecteralClustering算法选择K值2-12对数据进行聚类分析，使用轮廓系数进行评分，将轮廓系数和K值存储在列表k和silhouette_s 中，并绘制折线图\nfrom sklearn.cluster import SpectralClustering\nfrom sklearn.metrics import silhouette_score \nk,silhouette_s = [],[]\nfor i in range(2,13):\n    sc_model = SpectralClustering(n_clusters=i)    #这里本地运行很慢，在云环境就好很多\n    sc_model.fit(X=df_norm)\n    k.append(i)\n    print(i)\n    s = silhouette_score(X=df_norm,labels=sc_model.labels_)\n    silhouette_s.append(s)\nimport matplotlib.pyplot as plt\nplt.figure()\nplt.rc('font',family='YouYuan')   #这个记得加上\nplt.title('SpectralClustering')\nplt.xlabel(\"聚类的簇数\")\nplt.ylabel('silhouette_score')\nplt.plot(k,silhouette_s)\nplt.show()\n"
    },
    {
        "title": "聚类2，data_cluster.csv,sample()",
        "content": "import seaborn as sns\nfrom sklearn import metrics\nsns.set()\npd.options.display.max_columns = None\nimport warnings \nwarnings.filterwarnings('ignore')\norigin_data = pd.read_csv('./data/data_cluster.csv')\n#1.使用sample()方法从origin_data中随机抽取10000个样本，并允许重复抽样。重置索引，使得输出data为以下格式\ndata = origin_data.sample(n=10000)\ndata.reset_index()  #重置索引\ndf_cluster = data.copy()\n#2.从df_cluster中选择所有np.number类型的列，并将其保存在df_X中\ndf_X = df_cluster.select_dtypes(include=np.number)\ndf_X\ncols = df_X.columns\ncols\nfrom sklearn.preprocessing import MinMaxScaler\nscaler = MinMaxScaler()\nsclaed = pd.DataFrame(scaler.fit_transform(df_X),columns=cols)\nsclaed.head()\nfrom sklearn.cluster import KMeans\nk,silhouette,sse = [],[],[]\n#3.使用K-means模型，设置聚类数为i,初始算法为k-means++,最大迭代次数为500，初始化次数为10，随机种子为0，并将模型拟合到scaled数据上，计算使用欧几里得距离度量标准的轮廓系数s1和模型的质心间距离的总和s2。\nfrom sklearn.metrics import silhouette_score\nfor i in range(2,20):\n    kmeans_model = KMeans(n_clusters=i,init='k-means++',max_iter=500,n_init=10,random_state=0)\n    kmeans_model.fit(sclaed)\n    result = kmeans_model.labels_\n    k.append(i)\n    s1 = silhouette_score(sclaed,result)\n    ss = kmeans_model.inertia_ \n    sse.append(ss)\n    silhouette.append(s1)\n    print('K=',i,'Silhouette=',s1,'SSE=',ss)\nprint(len(silhouette),len(range(2,20)))\nplt.figure()\nplt.title('K-means silhouette')\n# plt.plot(range(2,20),silhouette,'*')  ##第三个参数，'o'代表折线的每一处拐角处用什么符号标记，o代表用原点，*代表用星号好表\nplt.plot(k,silhouette,'*')\n# plt.plot(range(2,20),silhouette,'-',alpha=0.5)\nplt.plot(k,silhouette,'-',alpha=0.5)\nplt.xlabel('Number of Cluster')\nplt.ylabel('silhouette')\nplt.show()\n\nplt.figure()\nplt.title('K-means SSE')\nplt.plot(k,sse)\nplt.plot(k,sse,'o')  #第三个参数，'o'代表折线的每一处拐角处用什么符号标记，o代表用原点，*代表用星号好表\nplt.xlabel('Number of cluster')\nplt.ylabel('SSE')\nplt.show()\n"
    },
    {
        "title": "回归student-info.csv",
        "content": "import matplotlib.pyplot as plt\nimport seaborn as sns\ndf = pd.read_csv('./data/student-info.csv')\n#1.查看年龄和性别的分布柱状图，输出如下：\nsns.countplot(x='age',hue='sex',data=df)\nplt.xlabel('age')\nplt.ylabel('sex')\nplt.title('Distribution bar chart of age and gender')\nplt.show()\n#2.查看年龄和成绩分布箱线图，输出如下：\nsns.boxplot(x='age',y='G3',data=df)\nplt.xlabel('age')\nplt.ylabel('G3')\nplt.title('Box plot of age and grade distribution')\nplt.show()\n#3.使用swarmplot函数查看年龄和成绩分布图：输出如下\nsns.swarmplot(x='age',y='G3',data=df)\nplt.xlabel('age')\nplt.ylabel('G3')\nplt.title('Age and grade distribution chart')\nplt.show()\n\ndel df['G1']\ndel df['G2']\ndf_oh = pd.get_dummies(df)\n#4.分析数据相关性系数，并取出和G3相关性最高的9个属性\ncro = df_oh.corr()\na = cro['G3'].sort_values(ascending=True)[:9]\nfrom sklearn.model_selection import train_test_split\nX = df_oh.loc[:,df_oh.columns != 'G3']\ny = df_oh['G3']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)\n#5.使用lightBGM算法进行建模，并使用网格搜索对模型进行cv使用5，输出模型评分的mae,rmse,abs\nfrom lightgbm import LGBMRegressor\n# 配置最优模型参数的模型\nmodel = LGBMRegressor()\nmodel.fit(X=X_train,y=y_train)\n# 使用偏差进行预测\nfrom sklearn.metrics import mean_absolute_error,mean_squared_log_error,mean_squared_error\nfrom sklearn.metrics import make_scorer\nprint(mean_absolute_error(y_test,model.predict(X_test)))\nprint(mean_squared_log_error(y_test,model.predict(X_test)))\ny_pred = model.predict(X_test)\nprint(np.median(np.abs(y_test - y_pred)/y_pred))\n\nfrom sklearn.model_selection import GridSearchCV\nparam_grid = {'learning_rate':[0.1,0.2,0.3,0.4,0.5]}\nmodel_gs = GridSearchCV(estimator=model,param_grid=param_grid,cv=5,scoring=make_scorer(mean_absolute_error))\nmodel_gs.fit(X_train,y_train)\nmodel_gs.best_score_\n#6.使用Voting算法聚合RandomForestRegressor,GradientBoostingRegressor,LinearRegression模型，预测成绩G3\nfrom sklearn.ensemble import VotingRegressor\nfrom sklearn.ensemble import RandomForestRegressor,GradientBoostingRegressor\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.metrics import r2_score\nreg1 = RandomForestRegressor(random_state=1)\nreg1.fit(X_train,y_train)\nprint('rf R2:',r2_score(y_test,reg1.predict(X_test)))\nreg2 = GradientBoostingRegressor(random_state=1)\nreg1.fit(X_train,y_train)\nprint('gbdt R2:',r2_score(y_test,reg1.predict(X_test)))\nreg3 = LinearRegression()\nreg3.fit(X_train,y_train)\nprint('lr R2:',r2_score(y_test,reg1.predict(X_test)))\nfrom sklearn.ensemble import VotingClassifier\nfrom mlxtend.classifier import StackingCVClassifier\nVotingClassifier()\nereg = VotingRegressor([('gb',reg1),('rf',reg2),('lr',reg3)],weights=[1,5,10])\nereg.fit(X_train,y_train)\nr2_score(y_test,ereg.predict(X_test))\npre_ereg = ereg.predict(X_test)\nprint(\"VotingRegressor R2:\",r2_score(y_test,pre_ereg))\nprint(pre_ereg)\n\n"
    },
    {
        "title": "特征train_forest_covertype.csv",
        "content": "import warnings\nwarnings.filterwarnings('ignore')\nimport numpy as np\nimport pandas as pd\nimport time\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom subprocess import check_output \nfrom lightgbm import LGBMClassifier\nimport lightgbm as lgb\nfrom tqdm import tqdm_notebook as tqdm\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import confusion_matrix,log_loss\n#!pip install xgboost\nfrom xgboost import XGBClassifier\nfrom lightgbm import LGBMClassifier\nfrom sklearn.pipeline import Pipeline\n#读取数据\ndf_forest = pd.read_csv('./data/train_forest_covertype.csv')\n\n#随机采样15120条数据\ndf_forest_sample = df_forest.sample(n=15120)\n\n# 删除Soil_Type7和Soil_Type15\ndf_forest_sample.drop(['Soil_Type7','Soil_Type15'],axis=1,inplace=True)\n\nXGB = XGBClassifier()\nlgbm = LGBMClassifier()\nfirst_models = [XGB,lgbm]\nfirst_model_names = ['XGB','lgbm']\nseed = 42\nskf = 5\n\n#1.ShuffleSplit函数进行切分，指定参数plitting iterations为skf,test_size为0.3，random_state为seed\nfrom sklearn.model_selection import ShuffleSplit\nfrom sklearn.preprocessing import StandardScaler\nn_folds = 5\nShuffleSplit(n_splits=skf, random_state=seed, test_size=0.3, train_size=0.6)  #切分数据\nstd_sca = StandardScaler()\nX = df_forest_sample.drop(columns=['Cover_Type'])\ny = pd.factorize(df_forest_sample['Cover_Type'])[0]\n\nMLA_columns = ['MLA Name','MLA Parameters','MLA Train Accuracy Mean','MLA Test Accuracy Mean','MLA Time']\nMLA_compare = pd.DataFrame(columns=MLA_columns)\n#create table to compare MLA predictions\nMLA_predict = df_forest_sample[['Id']]\ntrain_size = X.shape[0]\nn_models = len(first_models)\noof_pred = np.zeros((train_size,n_models))\nscores = []\nrow_index = 0\n\n#2.使用Pipline进行模型训练set中指定标准为('Scaler',std_sca),模型为('Estimator',model)3.使用cross_validate函数对模型进行评分，模型使用model,数据集使用X，y,指定return_train_score为True\nfrom sklearn.model_selection import cross_validate\ncross_validate()\nfrom sklearn.model_selection import cross_validate\nfor n,model in enumerate(first_models):\n    model_pipeline = Pipeline(steps=[('Scaler', StandardScaler()), ('Estimator', model)])\n    MLA_name = model.__class__.__name__\n    MLA_compare.loc[row_index,'MLA_name'] = MLA_name\n    MLA_compare.loc[row_index,'MLA Parameters'] = str(model_pipeline.get_params())\n    cv_result = cross_validate(estimator=model,X=X,y=y,cv=skf,return_train_score=True)\n    MLA_compare.loc[row_index,'MLA Time'] = cv_result['fit_time'].mean()\n    MLA_compare.loc[row_index,'MLA Train Accuracy Mean'] = cv_result['train_score'].mean()\n    MLA_compare.loc[row_index,'MLA Test Accuracy Mean'] = cv_result['test_score'].mean()\n    model_pipeline.fit(X,y)\n    MLA_predict[MLA_name] = model_pipeline.predict(X)\n    row_index+=1\n\nMLA_compare\n\ncv_result = cross_validate(estimator=first_models[0],X=X,y=y,cv=skf,return_train_score=True)\n\n#4.使用sort_values对按照集 MLA Test Accuracy Mean这一列倒序排序，指定inplace为True，覆盖原本数据\nMLA_compare.sort_values(by=['MLA Test Accuracy Mean'],ascending=True,inplace=True)\nMLA_compare\nMLA_compare.index[-20:-1]\n\n#5.删除MLA_compare后20位数据\nMLA_compare.drop(axis=0,index=MLA_compare.index[-20:-1])"
    }
]
    results = []
    for dict_item in json_data:
        results.append({'title': dict_item['title']})
    df = pd.DataFrame(results)
    df.style.set_properties(**{'white-space': 'pre-wrap', 'text-align': 'left'})