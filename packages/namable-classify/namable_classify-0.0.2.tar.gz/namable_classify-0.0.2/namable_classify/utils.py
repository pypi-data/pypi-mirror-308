"""通用工具函数和路径设置"""

# AUTOGENERATED! DO NOT EDIT! File to edit: ../nbs/00_utils.ipynb.

# %% auto 0
__all__ = ['lib_init_path', 'lib_directory_path', 'lib_repo_path', 'runs_path', 'runs_figs_path', 'data_path', 'rich_console',
           'original_print', 'print', 'MuteWarnings', 'ensure_array', 'default_on_exception', 'append_dict_list']

# %% ../nbs/00_utils.ipynb 6
from pathlib import Path
import inspect
import namable_classify
lib_init_path = Path(inspect.getfile(namable_classify))
lib_directory_path = lib_init_path.parent
lib_repo_path = lib_directory_path.parent
runs_path = lib_repo_path/'runs'
runs_path.mkdir(exist_ok=True, parents=True)
runs_figs_path = runs_path/'figs'
runs_figs_path.mkdir(exist_ok=True, parents=True)
data_path = lib_repo_path/'data'
data_path.mkdir(exist_ok=True, parents=True)

# %% ../nbs/00_utils.ipynb 7
with open(lib_repo_path/"README.md") as readme:
    namable_classify.__doc__ = readme.read()

# %% ../nbs/00_utils.ipynb 10
# How to set logger level in loguru?
# https://github.com/Delgan/loguru/issues/138
# Make faster? picologging
# import 
# def set_logger_level(level):
#     os
# How to add file handler to loguru logger?
# try:
import richuru
from rich.console import Console
from rich.theme import Theme
import logging
from rich.markdown import Markdown
import rich

# 如果在python console里面调用，就可以看到好看的东西。
from rich import pretty
pretty.install()

rich_console = Console(
    theme=richuru.Theme(  # required, otherwise the color will be incorrect
        {
            'logging.level.success': 'green',
            'logging.level.trace': 'bright_black',
        }
    ), 
    markup=True
)
richuru.install(rich_console=rich_console, 
                time_format="%a %Y-%m-%d %H:%M:%S.%f", 
                level = logging.INFO
)
# except ImportError:
#     pass


# %% ../nbs/00_utils.ipynb 13
from loguru import logger
original_print = print
# print = lambda *args, **kwargs: logger.info(*args, **kwargs)
def print(*args, **kwargs):
    if len(args)==0 and len(kwargs) == 0:
        logger.info('')
    logger.info(*args, **kwargs)

# %% ../nbs/00_utils.ipynb 19
from fastcore.basics import patch
import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim
from rich.table import Table

@patch
def inspect_model_parameters(model:nn.Module):
    trainable_params = 0
    all_param = 0
    trainable_bytes = 0
    all_bytes = 0
    for _, param in model.named_parameters():
        param_bytes = param.numel() * param.element_size()
        all_param += param.numel()
        all_bytes += param_bytes
        if param.requires_grad:
            trainable_params += param.numel()
            trainable_bytes += param_bytes
    return trainable_params, all_param, trainable_bytes, all_bytes

@patch
def num_of_total_parameters(model:nn.Module):
    return (model).inspect_model_parameters()[1]

@patch
def num_of_trainable_parameters(model:nn.Module):
    return (model).inspect_model_parameters()[0]

@patch
def print_trainable_parameters(model:nn.Module):
    """
    Prints the number of trainable parameters in the model.
    """
    trainable_params, all_param, trainable_bytes, all_bytes = model.inspect_model_parameters()
    # print(
    table = Table(title=f"Model {model.__class__.__name__}'s Trainable Parameters Inspection")
    table.add_column("Number of Trainable Parameters", justify="right", style="cyan", no_wrap=True)
    table.add_column("Number of Total Parameters", style="magenta")
    table.add_column("Trainable Ratio (0-1)", justify="right", style="green")
    table.add_row(f"{trainable_params:.3e} ({trainable_bytes:.3e} bytes)", f"{all_param:.3e} ({all_bytes:.3e} bytes)", f"{trainable_params / all_param:.3e}")
    
    logger.info(
        f"trainable params: {trainable_params} || all params: {all_param} || trainable%: {100 * trainable_params / all_param:.2f}", 
        rich=table
    )

# %% ../nbs/00_utils.ipynb 23
from bigmodelvis import Visualization
@patch
def model_rich_tree(self:nn.Module):
    module_tree = Visualization(self).structure_graph(printTree=False)
    return module_tree

from rich.panel import Panel
@patch
def print_model_pretty(self:nn.Module):
    module_tree = self.model_rich_tree()
    panel = Panel(module_tree, title=f"Model Tree for {self.__class__.__name__}")
    logger.info(str(self), rich=panel)
    # return module_tree

# %% ../nbs/00_utils.ipynb 26
import warnings
class MuteWarnings:
    def __enter__(self):
        # self.warnings_show = warnings.showwarning
        # warnings.showwarning = lambda *args, **kwargs: None
        self.mute()
        
    def __exit__(self, exc_type, exc_val, exc_tb):
        # warnings.showwarning = self.warnings_show
        self.close()        
        
    def mute(self):
        warnings.filterwarnings("ignore", append=True)
        
    def resume(self):
        warnings.filters.pop(0)
        

# %% ../nbs/00_utils.ipynb 27
import torch
import numpy as np
def ensure_array(x: torch.TensorType | np.ndarray | list):
    if isinstance(x, torch.Tensor):
        return x.detach().cpu().numpy()
    elif isinstance(x, np.ndarray):
        return x
    else: # list
        return np.array(x)

# %% ../nbs/00_utils.ipynb 29
# from scipy.special import softmax
from decorator import decorator
# def default_on_exception(default_value=None):
#     def decorator(func):
#         def wrapper(*args, **kwargs):
#             try:
#                 result = func(*args, **kwargs)
#                 return result
#             except Exception as e:
#                 logger.warning(f"An exception occurred: {e}")
#                 return default_value
#         return wrapper
#     return decorator

@decorator
def default_on_exception(func, default_value=None, verbose=False, *args, **kwargs):
    try:
        result = func(*args, **kwargs)
        return result
    except Exception as e:
        # logger.warning(f"An exception occurred: {e}")
        if verbose:
            logger.exception(e)
        return default_value

# %% ../nbs/00_utils.ipynb 31
def append_dict_list(dict, name, value):
    dict[name] = dict.get(name, []) + [value]
