"""automatically research on the relationship between the performance and meta parameters (a.k.a. hyperparameters or config) via searching (a.k.a. sweeping) experiments."""

# AUTOGENERATED! DO NOT EDIT! File to edit: ../../../nbs/02_auto_exp_fullfinetune.ipynb.

# %% auto 0
__all__ = ['start', 'end', 'learning_rates', 'seed', 'run_names', 'learning_rate_exec']

# %% ../../../nbs/02_auto_exp_fullfinetune.ipynb 5
# 先直接跑两个, 来不及写了

import numpy as np
# 设置采样的起始值和结束值
start = np.log(1e-5)
end = np.log(1e-1)
learning_rates = np.logspace(start, end, num=30, base=np.e)
np.random.shuffle(learning_rates)
learning_rates = learning_rates.tolist()

# %% ../../../nbs/02_auto_exp_fullfinetune.ipynb 7
# seed = 0
seed = 2
# seed = 1
def learning_rate_exec(learning_rate):
    parameters = fixed_meta_parameters.copy()
    parameters.yuequ = 'full_finetune'
    parameters.experiment_index = seed
    parameters.learning_rate = learning_rate
    return run_with_config(parameters)

# %% ../../../nbs/02_auto_exp_fullfinetune.ipynb 8
run_names = [f"{lr:.2e}" for lr in learning_rates]

# %% ../../../nbs/02_auto_exp_fullfinetune.ipynb 10
#| eval: false
from ..run import auto_run
auto_run(learning_rate_exec, learning_rates, run_names, f"sweep_lr_full_finetune-{seed}")
