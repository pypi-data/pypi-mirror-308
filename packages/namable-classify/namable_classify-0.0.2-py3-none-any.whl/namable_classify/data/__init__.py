"""处理数据加载和预处理的模块"""

# AUTOGENERATED! DO NOT EDIT! File to edit: ../../nbs/01_data.ipynb.

# %% auto 0
__all__ = ['cls_names_of', 'ClassificationDataConfig', 'ClassificationDataModule', 'sksplit_for_torch', 'TorchVisionDataModule',
           'CIFAR100DataModule', 'MNISTDataModule', 'CIFAR10DataModule', 'FashionMNISTDataModule', 'QMNISTDataModule',
           'KMNISTTDataModule']

# %% ../../nbs/01_data.ipynb 5
from ..utils import data_path, Path

# %% ../../nbs/01_data.ipynb 6
from torchvision.datasets import MNIST, FashionMNIST, QMNIST, KMNIST
from torchvision.transforms import v2 as transforms
import torch
from torchvision.datasets import CIFAR100, CIFAR10

from torchvision.transforms.v2 import (
    AutoAugment,
    RandAugment,
    AugMix,
    TrivialAugmentWide,
)
from pydantic import BaseModel, Field

# %% ../../nbs/01_data.ipynb 8
def cls_names_of(cls_list:list|type):
    if isinstance(cls_list, list):
        return [cls.__name__ for cls in cls_list]
    return cls_list.__name__

# %% ../../nbs/01_data.ipynb 9
class ClassificationDataConfig(BaseModel):
    # protocol: str = 'torch'
    # dataset_name: str = 'cifar100'
    dataset_root: Path = data_path
    dataset_name: str = Field(default=CIFAR100.__name__, 
                              description='name of dataset', 
                              choices=cls_names_of([CIFAR100, CIFAR10, MNIST, FashionMNIST, QMNIST, KMNIST]))
    batch_size:int=1
    choice_of_auto_augment: str = Field(default=TrivialAugmentWide.__name__, 
                                          description='choice of auto augmentation policy', 
                                          choices = cls_names_of([TrivialAugmentWide, AutoAugment, RandAugment, AugMix]))
# TODO 支持多个来源的数据集自动加载
# from torchvision.datasets import __all__, CIFAR100
# __all__

# %% ../../nbs/01_data.ipynb 10
from typing import Callable
from torch.utils.data import random_split, DataLoader
import lightning as L
from torchvision import transforms
from lightning.pytorch.utilities.types import EVAL_DATALOADERS, TRAIN_DATALOADERS

import psutil

class ClassificationDataModule(L.LightningDataModule):
    num_of_classes = None
    classes = None
    
    @classmethod
    def from_config(cls, config:ClassificationDataConfig) -> 'ClassificationDataModule':
        return cls(**config.model_dump())
    def __init__(self, **config:ClassificationDataConfig) -> None:
        super().__init__()
        self.save_hyperparameters()
        self.save_hyperparameters(dict(num_of_classes=self.num_of_classes))
        # 根据CPU自动设置
        # self.workers = psutil.cpu_count(logical=True) 
        self.workers = 4
        # self.workers = psutil.cpu_count(logical=False) 
        # self.workers = psutil.cpu_count(logical=False) 
        # self.workers = 31 
    #     self.__sub_init__()
    #     # self.config = config
    # def __sub_init__(self, a=1, b=2) -> None:
    #     print("Hello")
    #     self.save_hyperparameters(dict(a=a, b=b))
    # @property
    # def num_classes(self) -> int:
    #     return self.hparams.num_classes
    # @num_classes.setter
    # def num_classes(self, value:int) -> None:
    #     self.hparams.num_classes = value
        
    # @property
    # def transform(self) -> Callable: #TODO 类型标注不知道怎么写
    #     return self.hparams.transform
    
    # @transform.setter
    # def transform(self, value:Callable) -> None:
    #     self.hparams.transform = value
    
    
        
    def train_dataloader(self)->TRAIN_DATALOADERS:
        return DataLoader(self.train_ds, batch_size=self.hparams.batch_size, 
                          num_workers=self.workers, shuffle=True, pin_memory=True)

    def val_dataloader(self)->EVAL_DATALOADERS:
        return DataLoader(self.val_ds, batch_size=self.hparams.batch_size, num_workers=self.workers, pin_memory=True)

    def test_dataloader(self)->EVAL_DATALOADERS:
        return DataLoader(self.test_ds, batch_size=self.hparams.batch_size, num_workers=self.workers, pin_memory=True)

    def predict_dataloader(self)->EVAL_DATALOADERS:
        return DataLoader(self.predict_ds, batch_size=self.hparams.batch_size, num_workers=self.workers, pin_memory=True)

# %% ../../nbs/01_data.ipynb 12
from torch.utils.data import random_split
from sklearn.model_selection import train_test_split
import torch
from torchvision.datasets import CIFAR100

def sksplit_for_torch(ds_full:torch.utils.data.Dataset, test_size:float=0.2, stratify_targets=None, random_state=None):
    indexes = list(range(len(ds_full)))
    stratify_targets = stratify_targets or (ds_full.targets if hasattr(ds_full, 'targets') else [int(ds_full[i][1]) for i in indexes])
    train_indexes, val_indexes = train_test_split(indexes, test_size=test_size,
                                                    stratify=stratify_targets, random_state=random_state)
    return torch.utils.data.Subset(ds_full, train_indexes), torch.utils.data.Subset(ds_full, val_indexes)
    # return random_split(
                #     ds_full, [, ], 
                #     generator=torch.Generator().manual_seed(L.seed_everything()), 
                # )

# %% ../../nbs/01_data.ipynb 13
CIFAR100.url = "https://cloud.tsinghua.edu.cn/f/ab461b61f3564ccdb933/?dl=1" # Tsinghua mirrorURL
class TorchVisionDataModule(ClassificationDataModule):
    # 目前支持的情况是，train test两个split，没有val，需要自己分val的情况
    torchvision_cls = CIFAR100
    num_of_classes = 100
    def __init__(self, 
                 train_transform=None, # 需要后续设置
                 test_transform=None, # 需要后续设置
                 train_val_split=0.9, # 训练集和验证集的比例
                 **config:ClassificationDataConfig) -> None:
        super().__init__(**config)
        self.save_hyperparameters()
    
    @property
    def dataset_name(self):
        return self.torchvision_cls.__name__

    def prepare_data(self):
        # download
        train_ds = self.torchvision_cls(self.hparams.dataset_root, train=True, download=True)
        test_ds = self.torchvision_cls(self.hparams.dataset_root, train=False, download=True)
        self.classes = train_ds.classes
        assert len(self.classes)==self.num_of_classes, f"Number of classes in dataset is {len(self.classes)}, but {self.num_of_classes} is expected."

    def setup(self, stage: str):
        # Assign train/val datasets for use in dataloaders
        match (stage):
            case ("fit") | ("validate"):
                # 还有个 validate 但是fit的时候我们就设置好了，所以直接跳过
                if not hasattr(self, "val_ds"):
                    ds_full = self.torchvision_cls(self.hparams.dataset_root, train=True, transform=self.hparams.train_transform)
                    self.train_ds, self.val_ds = sksplit_for_torch(ds_full, 1-self.hparams.train_val_split, random_state=0) # 不和Lighning seed everything一起
                else:
                    print("Validation loader has been setup before. ")
                pass
            case ("test"):
                self.test_ds = self.torchvision_cls(self.hparams.dataset_root, train=False, transform=self.hparams.test_transform)
            case ("predict"):
                self.predict_ds = self.torchvision_cls(self.hparams.dataset_root, train=False, transform=self.hparams.test_transform)

class CIFAR100DataModule(TorchVisionDataModule):
    torchvision_cls = CIFAR100
    num_of_classes = 100
    
class MNISTDataModule(TorchVisionDataModule):
    torchvision_cls = MNIST
    num_of_classes = 10
    
class CIFAR10DataModule(TorchVisionDataModule):
    torchvision_cls = CIFAR10
    num_of_classes = 10
    
class FashionMNISTDataModule(TorchVisionDataModule):
    torchvision_cls = FashionMNIST
    num_of_classes = 10
    
class QMNISTDataModule(TorchVisionDataModule):
    torchvision_cls = QMNIST
    num_of_classes = 10
    
class KMNISTTDataModule(TorchVisionDataModule):
    torchvision_cls = KMNIST
    num_of_classes = 10

# ImageNet有所不同  
# ？论文中 ImageNet test到底用不用
# class ImageNetDataModule(TorchVisionDataModule):
#     torchvision_cls = ImageNet
#     num_of_classes = 1000
# , EMNIST

# %% ../../nbs/01_data.ipynb 21
# https://neptune.ai/blog/data-augmentation-in-python
# https://pytorch.org/vision/main/auto_examples/transforms/plot_transforms_illustrations.html#sphx-glr-auto-examples-transforms-plot-transforms-illustrations-py
# https://sebastianraschka.com/blog/2023/data-augmentation-pytorch.html # 结论是Trivial最好
from transformers import AutoImageProcessor, BitImageProcessor, ViTImageProcessor
from torchvision.transforms import (
    CenterCrop,
    Compose,
    Normalize,
    RandomHorizontalFlip,
    RandomResizedCrop,
    Resize,
    ToTensor,
    RandomRotation,
    RandomGrayscale,
    Grayscale,
    AutoAugment,
    RandAugment,
    AugMix,
    TrivialAugmentWide,
)
from .transforms import CutoutPIL
from fastcore.basics import patch
@patch
def set_transform_from_hf_image_preprocessor(self:ClassificationDataModule, hf_image_preprocessor:AutoImageProcessor, model_image_size=None):
    if model_image_size is None:
        if isinstance(hf_image_preprocessor, ViTImageProcessor):
            model_image_size:tuple[int, int] = (hf_image_preprocessor.size['height'], hf_image_preprocessor.size['width'])
        elif isinstance(hf_image_preprocessor, BitImageProcessor):
            model_image_size:tuple[int, int] = (hf_image_preprocessor.crop_size['height'], hf_image_preprocessor.crop_size['width'])
    normalize = Normalize(mean=hf_image_preprocessor.image_mean, std=hf_image_preprocessor.image_std)
    
    # choice_of_auto_augment = 
    
    self.hparams.train_transform = Compose(
        [
            # # RandomResizedCrop(image_processor.size["height"]),
            # RandomResizedCrop(image_processor.crop_size["height"]),
            # RandomHorizontalFlip(),
            # # RandomRotation((-30, 30)),
            # # RandomGrayscale(),
            # # AddPepperNoise(0.5, p=0.1),
            # Grayscale(num_output_channels=3),

            Resize(model_image_size),
            # CutoutPIL(cutout_factor=1/4), # cifar 32x32  随机把中间8x8正方形变成空白 
            # CutoutPIL(cutout_factor=0.5),
            # RandAugment(),
            TrivialAugmentWide(),
            
            # resize
            # center_crop
            
            # rescale
            # normalize
            
            ToTensor(),
            normalize,
        ]
    )

    self.hparams.test_transform = Compose(
        [
            # Resize(image_processor.size["height"]),
            # Resize(image_processor.crop_size["height"]),
            # # CenterCrop(image_processor.size["height"]),
            # CenterCrop(image_processor.crop_size["height"]),
            # Grayscale(num_output_channels=3),
            
            Resize(model_image_size),
            ToTensor(),
            normalize,
        ]
    )
    return model_image_size

# %% ../../nbs/01_data.ipynb 23
from fastcore.basics import patch
@patch
def get_lightning_data_module(self:ClassificationDataConfig):
    if self.dataset_name == 'MNIST':
        return MNISTDataModule.from_config(self)
    elif self.dataset_name == 'CIFAR100':
        return CIFAR100DataModule.from_config(self)
    else:
        raise ValueError(f"Unsupported dataset: {self.dataset_name}")
