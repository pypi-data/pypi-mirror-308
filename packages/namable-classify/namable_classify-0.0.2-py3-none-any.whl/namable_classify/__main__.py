"""主训练脚本入口，调用各模块进行模型训练"""

# AUTOGENERATED! DO NOT EDIT! File to edit: ../nbs/00_main.ipynb.

# %% auto 0
__all__ = ['config', 'cls_task', 'trainer', 'tuner', 'lr_finder', 'fig', 'new_lr']

# %% ../nbs/00_main.ipynb 5
from .core import ClassificationTask, ClassificationTaskConfig
config = ClassificationTaskConfig()
# config.learning_rate = 1e-1
# config.learning_rate = 1
# config.learning_rate = 1e-3
# config.learning_rate = 1e-5
config.learning_rate = 3e-4
config.experiment_index = 1
# config.experiment_index = 0
# config.learning_rate = 1e-6
config.dataset_config.batch_size = 64
config.yuequ = "LORA"
cls_task = ClassificationTask(config)
cls_task.print_model_pretty()
import torch
# cls_task.cls_model = torch.compile(cls_task.cls_model, mode='reduce-overhead')
#  fullgraph=True

# %% ../nbs/00_main.ipynb 6
from boguan_yuequ.auto import AutoYueQuAlgorithm
AutoYueQuAlgorithm(cls_task, config.yuequ)

# %% ../nbs/00_main.ipynb 8
import lightning as L
from .utils import runs_path
from lightning.pytorch.callbacks.early_stopping import EarlyStopping
from lightning.pytorch.callbacks import ModelSummary, StochasticWeightAveraging, DeviceStatsMonitor, LearningRateMonitor, LearningRateFinder
from lightning.pytorch.loggers import TensorBoardLogger, CSVLogger, WandbLogger

trainer = L.Trainer(default_root_dir=runs_path, enable_checkpointing=True, 
                    enable_model_summary=True, 
                    num_sanity_val_steps=2, # 防止 val 在训了好久train才发现崩溃
                    callbacks=[
                        # EarlyStopping(monitor="val_loss", mode="min")
                        EarlyStopping(monitor="val_acc1", mode="max", check_finite=True, 
                                      patience=5, 
                                    #   patience=6, 
                                      check_on_train_epoch_end=False,  # check on validation end
                                      verbose=True),
                        ModelSummary(max_depth=3),
                        # https://pytorch.org/blog/pytorch-1.6-now-includes-stochastic-weight-averaging/
                        # StochasticWeightAveraging(swa_lrs=1e-2), 
                        # DeviceStatsMonitor(cpu_stats=True)
                        LearningRateMonitor(), 
                        # LearningRateFinder() # 有奇怪的bug
                               ]
                    , max_epochs=15
                    # , gradient_clip_val=1.0, gradient_clip_algorithm="value"
                    , logger=[
                        # TensorBoardLogger(save_dir=runs_path/"tensorboard"),
                        TensorBoardLogger(save_dir=runs_path),
                              CSVLogger(save_dir=runs_path), 
                              WandbLogger(project="namable_classify", name="test")
                              ]
                    # , profiler="simple"
                    # , fast_dev_run=True
                    # limit_train_batches=10, limit_val_batches=5
                    # strategy="ddp", accelerator="gpu", devices=4
                    )

# %% ../nbs/00_main.ipynb 9
#| eval: false
from lightning.pytorch.tuner import Tuner
tuner = Tuner(trainer)

lr_finder = tuner.lr_find(cls_task, datamodule=cls_task.lit_data, 
                        #   max_lr=1e-2
                        method = "fit",
                        min_lr = 1e-8,
                        # min_lr = 1e-4,
    max_lr = 1,
    # num_training = 10,
    num_training = 100,
    mode = "exponential"
    # mode = "linear"
                        
                          )
print(lr_finder.results)

fig = lr_finder.plot(suggest=True)
from matplotlib import pyplot as plt
from .utils import runs_figs_path
plt.savefig(runs_figs_path/'lr_finder.png')
# fig.show()
new_lr = lr_finder.suggestion()
# new_lr, cls_task.hparams.learning_rate
print("New learning rate: ", new_lr)

cls_task.hparams.learning_rate = new_lr/10

# %% ../nbs/00_main.ipynb 11
#| eval: false
trainer.fit(cls_task, datamodule=cls_task.lit_data)

# %% ../nbs/00_main.ipynb 12
#| eval: false
trainer.test(cls_task, datamodule=cls_task.lit_data)
