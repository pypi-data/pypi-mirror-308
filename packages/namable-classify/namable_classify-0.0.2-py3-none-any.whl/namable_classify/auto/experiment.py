"""automatically research on the relationship between the performance and meta parameters (a.k.a. hyperparameters or config) via searching (a.k.a. sweeping) experiments."""

# AUTOGENERATED! DO NOT EDIT! File to edit: ../../nbs/02_auto_experiment.ipynb.

# %% auto 0
__all__ = ['auto_exp_runs_path', 'fixed_meta_parameters', 'study_results', 'backbone_name2pe', 'peft_to_try', 'delta_to_try',
           'yuequ_to_try', 'username', 'password', 'host', 'port', 'database_name', 'postgres_url', 'study',
           'run_with_config', 'objective']

# %% ../../nbs/02_auto_experiment.ipynb 4
import os
os.environ['HF_ENDPOINT'] = "https://hf-mirror.com"

# %% ../../nbs/02_auto_experiment.ipynb 5
from ..core import ClassificationTask, ClassificationTaskConfig
from boguan_yuequ.auto.nucleus import AutoYueQuAlgorithm
import lightning as L
from ..utils import runs_path
from lightning.pytorch.callbacks.early_stopping import EarlyStopping
from lightning.pytorch.callbacks import ModelSummary, StochasticWeightAveraging, DeviceStatsMonitor, LearningRateMonitor, LearningRateFinder, BatchSizeFinder
from lightning.pytorch.loggers import TensorBoardLogger, CSVLogger, WandbLogger

# %% ../../nbs/02_auto_experiment.ipynb 8
# from clearml import Task
import optuna
from optuna.integration import PyTorchLightningPruningCallback
from lightning.pytorch.loggers import MLFlowLogger
from ..utils import logger
import torch

auto_exp_runs_path = runs_path / "auto_experiment"
auto_exp_runs_path.mkdir(exist_ok=True, parents=True)


def run_with_config(
    config: ClassificationTaskConfig,
    trial: optuna.Trial = None,
    tuning_metric="val_acc1",  # Seriously, 为了学术诚信规范，我们AI科研者不能用 "test_acc1" 来调参。
    tuning_mode="max",
):
    logger.info(f"running with config: {config}")
    L.seed_everything(config.experiment_index)
    cls_task = ClassificationTask(config)
    cls_task.print_model_pretty()
    AutoYueQuAlgorithm(cls_task, config.yuequ, config.yuequ_pe)
    # AutoYueQuAlgorithm(cls_task.cls_model, config.yuequ, config.yuequ_pe)
    # Task.init(project_name=config.experiment_project, task_name=config.experiment_task)
    # https://clear.ml/docs/latest/docs/guides/frameworks/pytorch_lightning/pytorch_lightning_example/

    callbacks = [
        # EarlyStopping(monitor="val_loss", mode="min")
        EarlyStopping(
            monitor=tuning_metric,
            mode=tuning_mode,
            check_finite=True,
            #   patience=5,
            patience=10,
            #   patience=6,
            check_on_train_epoch_end=False,  # check on validation end
            verbose=True,
        ),
        ModelSummary(max_depth=3),
        # https://pytorch.org/blog/pytorch-1.6-now-includes-stochastic-weight-averaging/
        # StochasticWeightAveraging(swa_lrs=1e-2),
        # DeviceStatsMonitor(cpu_stats=True)
        # LearningRateMonitor(),
        # LearningRateFinder() # 有奇怪的bug
        # BatchSizeFinder(init_val=32) # 用 "power" 减少调参不确定性; 
    ]
    if trial is not None:
        callbacks.append(PyTorchLightningPruningCallback(trial, monitor=tuning_metric))

    lightning_loggers = [
        TensorBoardLogger(save_dir=auto_exp_runs_path),
        CSVLogger(save_dir=auto_exp_runs_path),
        MLFlowLogger(experiment_name=f"{config.experiment_project}/{config.experiment_task}", 
                    #  .replace("_", "-").replace(" ", "-"), 
                     tracking_uri="http://10.103.10.55:5000")
        # WandbLogger(project=config.experiment_project, name=config.experiment_task),
    ]
    
    torch.set_float32_matmul_precision("high") # 尝试用 TF32
    trainer = L.Trainer(
        default_root_dir=auto_exp_runs_path,
        enable_checkpointing=True,
        enable_model_summary=True,
        num_sanity_val_steps=2,  # 防止 val 在训了好久train才发现崩溃
        callbacks=callbacks
        , max_epochs=30 
        # , gradient_clip_val=1.0, gradient_clip_algorithm="value"
        ,
        logger=lightning_loggers,
        # , profiler="simple"
        # , fast_dev_run=True
        # limit_train_batches=10, limit_val_batches=5
        # strategy="ddp", accelerator="gpu", devices=4
        accelerator="gpu", devices=1, # 实验并行优于数据并行，尽量单卡训练
        # accelerator="gpu", devices=8, # 实验并行优于数据并行，尽量单卡训练
        # precision=16 
        # strategy="ddp"
    )
    # batch size 和 learning rate
    from lightning.pytorch.tuner import Tuner
    tuner = Tuner(trainer)
    found_batch_size = tuner.scale_batch_size(cls_task, datamodule=cls_task.lit_data, 
                                        #   mode='binsearch', 
                                          mode='power', 
                                          init_val=64)
    cls_task.hparams.batch_size = found_batch_size = 64 * 8
    linear_lr_scale = found_batch_size / 64
    # linear_lr_scale *= 8 # 多卡训练
    cls_task.hparams.learning_rate = config.learning_rate * linear_lr_scale
    logger.info(f"original learning rate: {config.learning_rate}, linear lr scale: {linear_lr_scale}, learning rate: {cls_task.hparams.learning_rate}")

    logger.info(f"actual hyperparameters: {cls_task.hparams}")
    trainer.fit(cls_task, datamodule=cls_task.lit_data)
    val_result = trainer.validate(cls_task, datamodule=cls_task.lit_data)
    test_result = trainer.test(cls_task, datamodule=cls_task.lit_data)
    # val_acc1 = val_result[0]["val_acc1"]
    # test_acc1 = test_result[0]["test_acc1"]
    # return val_acc1, test_acc1
    return val_result, test_result

# %% ../../nbs/02_auto_experiment.ipynb 9
from ..core import ClassificationModelConfig, ClassificationTaskConfig, ClassificationDataConfig
fixed_meta_parameters = ClassificationTaskConfig(
    experiment_project = "Homogeneous dwarf model is all you need for tuning pretrained giant model.", 
    # experiment_name = "Auto experiment", 
    experiment_task = "Auto experiment Stage 1 (single run, short epoches)", 
    label_smoothing=0.1,  # 未必固定。
    cls_model_config=ClassificationModelConfig(
        # checkpoint = "google/vit-base-patch16-224-in21k"
    ), 
    dataset_config = ClassificationDataConfig(
        # batch_size=64, # 经过前期经验, 这个方便站在61服务器跑, 大概10G显存。 固定基于这个调参
        batch_size=16,
    )
)


# %% ../../nbs/02_auto_experiment.ipynb 11
study_results = [] # 准备装入 dict

# %% ../../nbs/02_auto_experiment.ipynb 12
# 需要跑哪些backbone 和 对应的 pe呢？
from boguan_yuequ.benchmarking import pe_list_tiny_for_all_size, backbone_names
# from transformers import AutoModel
# # tiny+tiny vs tiny full vs tiny full_lora 也是有意义的对比，所以不做截断。
# for config, pe in zip(configs, pe_list_tiny_for_all_size):
#     model = AutoModel.from_pretrained(config)
#     yuequ = AutoYueQuAlgorithm(model, 'lora', pe)
#     model = yuequ.adapted_model
#     pe = yuequ.pe
backbone_name2pe = {backbone_name:pe for pe, backbone_name in zip(pe_list_tiny_for_all_size, backbone_names)}

# %% ../../nbs/02_auto_experiment.ipynb 14
# 需要跑哪些算法呢？
# from boguan_yuequ.auto import huggingface_peft_budget_config_key, thu_nlp_opendelta_budget_config_key
from boguan_yuequ.auto.integrations.peft import huggingface_peft_budget_config_key
from boguan_yuequ.auto.integrations.opendelta import thunlp_opendelta_budget_config_key

peft_to_try = [k.name for k in huggingface_peft_budget_config_key.keys()]
delta_to_try = [k for k in thunlp_opendelta_budget_config_key.keys() if k.upper() not in peft_to_try]
yuequ_to_try = peft_to_try + delta_to_try

# %% ../../nbs/02_auto_experiment.ipynb 18
# full_finetune 和 新方法单列
# 这里只跑baseline
from boguan_yuequ.benchmarking import pe_list_tiny_for_all_size, backbone_names
import optuna
from ..utils import logger
from rich.prompt import Prompt

import optuna.exceptions

def objective(trial, num_of_repeated_experiments = 5):
    # TODO 对每一个目标超参数 分开去做？ grid search
    # for yuequ in yuequ_tried_algs:
    
    meta_parameters = fixed_meta_parameters.copy()
    # 修改超参
    
    # 目标超参被建议
    meta_parameters.yuequ = trial.suggest_categorical("yuequ", yuequ_to_try)
    # choice = trial.suggest_categorical("pe_and_backbone_choice", list(range(len(backbone_names))))
    backbone_name = trial.suggest_categorical("backbone", backbone_names)
    meta_parameters.cls_model_config.checkpoint = backbone_name
    meta_parameters.yuequ_pe = backbone_name2pe[backbone_name]
    trial.set_user_attr("parameter_efficiency", meta_parameters.yuequ_pe) # 用于后续分析实验结果
    
    # 接下来是无关变量
# f"{yuequ}-learning_rate"

    # meta_parameters.learning_rate = trial.suggest_float(f"learning_rate", 1e-5, 1e-1, log=True)
    # meta_parameters.learning_rate = trial.suggest_float(f"learning_rate", 1e-5, 1e-2, log=True) # 正则化，建议的是Batch Size=64的学习率
    meta_parameters.learning_rate = trial.suggest_float(f"learning_rate", 1e-4, 4e-2, log=True) # 正则化，建议的是Batch Size=64的学习率
    
    result_dict = dict()

    metric_names = set()
    # 重复试验
    for experiment_index in range(num_of_repeated_experiments):
        meta_parameters.experiment_index = experiment_index
        # 当我们选定 experiment_index 之后，就不要随机建议参数了，现在我们元参数保持一样，重复5次随机实验。
        try:
            val_result, test_result = run_with_config(
                meta_parameters, trial, "val_acc1", "max"
            )
        except Exception as e:
            logger.exception(e)
            logger.error(f"Error in experiment {experiment_index}, May be you can\n"
                         "1. Stop the optuna study and you debug and fix the buggy code. "
                         "2. Searched optuna trial is invalid as an input, so just prune this trial and continue. "
                         )
            choice = Prompt.ask("What should we do now?",
                                choices=["1", "2"], default="1")
            if choice == "1":
                raise e
            else:
                raise optuna.exceptions.TrialPruned()
            
        # 注意不要用 test_acc1 调参。
        # 我们的原则是每一个目标超参验证集到最优, 然后再用最优的超参得到的模型(其实应该重新训练一遍)在测试集上测试。
        # 在论文研究的第一阶段，应该调参。时间不够的话
        
        single_run_result = val_result[0] | test_result[0]
        metric_names.update(single_run_result.keys())
        single_run_result = {f"{k}-run{experiment_index}":v for k, v in single_run_result.items()}
        for k, v in single_run_result.items():
            trial.set_user_attr(k, v)
        result_dict|=single_run_result
        
        trial.report(single_run_result[f"val_acc1-run{experiment_index}"], experiment_index)
        # if trial.should_prune():
        #     # Return the current predicted value instead of raising `TrialPruned`.
        #     # This is a workaround to tell the Optuna about the evaluation
        #     # results in pruned trials.
        #     for metric_name in metric_names:
        #         all_runs_results = [result_dict[f"{metric_name}-run{i}"] for i in range(num_of_repeated_experiments)]
        #         result_dict[f"{metric_name}-mean"] = sum(all_runs_results) / len(all_runs_results)
        #         trial.set_user_attr(f"{metric_name}-mean", result_dict[f"{metric_name}-mean"])
        #     trial.set_user_attr(f"num_of_repeated", experiment_index+1)
        #     return result_dict["val_acc1-mean"]
    # 计算一下平均数
    for metric_name in metric_names:
        all_runs_results = [result_dict[f"{metric_name}-run{i}"] for i in range(num_of_repeated_experiments)]
        result_dict[f"{metric_name}-mean"] = sum(all_runs_results) / len(all_runs_results)
        trial.set_user_attr(f"{metric_name}-mean", result_dict[f"{metric_name}-mean"])
    trial.set_user_attr(f"num_of_repeated", num_of_repeated_experiments)
    return result_dict["val_acc1-mean"]
    

# %% ../../nbs/02_auto_experiment.ipynb 22
from ..utils import runs_path
from optuna.samplers import *
from optuna.pruners import *
# study_path = auto_exp_runs_path / "optuna_studies.db"
# sqlite_url = f"sqlite:///{study_path}"
# sqlite_url = f"sqlite://{study_path}"
# TODO 

username = 'ycm'
password = 'password'
# host = 'localhost'
host = '10.103.10.53'
port = 5432
database_name = 'namable_classify'
postgres_url = f'postgresql+psycopg2://{username}:{password}@{host}:{port}/{database_name}'
# pip install psycopg2-binary 
# postgres_url = 'postgresql://myuser:mypassword@localhost/mydatabase'
# TODO safety and privacy
study = optuna.create_study(
    # study_name="peft baselines benchmark",  # old version
    # study_name="peft baselines benchmark 11.3", 
    study_name="peft baselines benchmark 11.7", 
    # storage=sqlite_url, 
    storage=postgres_url, 
    load_if_exists=True, 
    direction="maximize", 
    # https://pub.aimind.so/a-deep-dive-in-optunas-advance-features-2e495e71435c
    # sampler=GPSampler(seed=42), 
    # sampler=TPESampler(seed=42), 
    # sampler=TPESampler(), 
    # https://github.com/optuna/optuna/issues/1647
    sampler=CmaEsSampler(consider_pruned_trials = True), 
    pruner=HyperbandPruner()
    # pruner=WilcoxonPruner()
    # CmaEsSampler(seed=42),  我们实验数量应该小于1000
    # WilcoxonPruner(min_n_trials=10) # 不适合这个，这个 immediate 是fold的情况
)
study.set_user_attr("contributors", ["Ye Canming"])
study.set_user_attr("fixed_meta_parameters", fixed_meta_parameters.json())
# 晚点再看
# https://optuna-integration.readthedocs.io/en/stable/reference/generated/optuna_integration.MLflowCallback.html

# %% ../../nbs/02_auto_experiment.ipynb 23
# study.optimize(objective, n_trials=100)
study.optimize(lambda trial: objective(trial, num_of_repeated_experiments=1), 
               n_trials=100)
