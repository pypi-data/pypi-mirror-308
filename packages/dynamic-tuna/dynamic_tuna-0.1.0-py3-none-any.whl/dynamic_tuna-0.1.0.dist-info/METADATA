Metadata-Version: 2.1
Name: dynamic_tuna
Version: 0.1.0
Summary: A description of what dynamic_tuna does
Home-page: https://github.com/yourusername/dynamic_tuna
Author: Your Name
Author-email: your.email@example.com
Classifier: Programming Language :: Python :: 3
Classifier: License :: OSI Approved :: MIT License
Classifier: Operating System :: OS Independent
Requires-Python: >=3.6
Description-Content-Type: text/markdown
License-File: LICENSE.txt

# Dynamic Tuna: Flexible Bayesian Optimization Library

**Dynamic Tuna** is a Bayesian optimization library built on Optuna and scikit-optimize that supports customizable exploration-exploitation control. The supported samplers (surrogate models) are Gaussian Process, Random Forest, Gradient Boosted Decision Trees, and Tree-structured Parzen Estimator -- each with customizable exploration-exploitation controls.

![Dynamic Tuna](https://img.shields.io/badge/bayesian-optimization-blue.svg) ![License](https://img.shields.io/badge/license-MIT-green)

---

## ğŸŒŸ Features

- **Dynamic Sampling**: Control the exploration-exploitation tradeoff with flexible `n_ei_function` parameters.

- **Diverse Samplers**: Use Gaussian Process, Random Forest, or Gradient Boosted Trees as the surrogate model for efficient sampling.
- **Customizable Hyperparameters**: Define custom `n_ei_function` parameters for precise control over the optimization process.

---

## ğŸ“¥ Installation

Clone the repository and install the required packages from `requirements.txt`:

```bash
git clone https://github.com/yourusername/dynamic_tuna.git
cd dynamic_tuna
pip install -r requirements.txt


	Note: This library requires Python 3.8+.

ğŸš€ Getting Started

Hereâ€™s a quick example to get you started with Dynamic Tuna.


import optuna
from dynamic_tuna import GBTSampler

# Define search space
search_space = {
    "param1": optuna.distributions.UniformDistribution(0, 1),
    "param2": optuna.distributions.IntUniformDistribution(1, 100)}

# Initialize sampler with dynamic EI candidates
sampler = GBTSampler(search_space, n_ei_function=lambda n, a=1, b=2: a * n + b)

# Create and run study
study = optuna.create_study(sampler=sampler)
study.optimize(objective_function, n_trials=50)
print("Best Parameters:", study.best_params)
```
## âš™ï¸ Usage

	1.	Initialize a Sampler:
Choose from GBTSampler, RandomForestSampler, or GPSampler and define your n_ei_function.
	2.	Define a Search Space:
Use Optunaâ€™s UniformDistribution, IntUniformDistribution, and more.
	3.	Run Optimization:
Run your study with optuna.create_study() and check the results.

## ğŸ”§ Contributing

Contributions are welcome! To contribute:

	1.	Fork the repository.
	2.	Create a new branch with your feature or bugfix.
	3.	Submit a pull request with a description of your changes.

Please ensure that your code is well-documented and tested.

## ğŸ“œ License

This project is licensed under the MIT License. See the LICENSE file for details.

## ğŸ“¬ Contact

Have questions or feedback? Reach out to me at dsilverberg95@gmail.com or create an issue in the repository.

Happy Optimizing! ğŸ‰

